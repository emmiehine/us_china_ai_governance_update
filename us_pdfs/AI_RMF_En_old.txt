NIST AI 100-1
Artificial Intelligence Risk Management
Framework (AI RMF 1.0)NIST AI 100-1
Artificial Intelligence Risk Management
Framework (AI RMF 1.0)
This publication is available free of charge from:
https://doi.org/10.6028/NIST.AI.100-1
January 2023
U.S. Department of Commerce
Gina M. Raimondo, Secretary
National Institute of Standards and Technology
Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology 
Certain commercial entities, equipment, or materials may be identified in this document in order to describe an experimental procedure or concept adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that
the entities, materials, or equipment are necessarily the best available for the purpose.
This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1
Update Schedule and Versions
The Artificial Intelligence Risk Management Framework (AIRMF) is intended to be a living document.
NIST will review the content and usefulness of the Framework regularly to determine if an update is appropriate; a review with formal input from the AI community is expected to take place no later than 2028. The Framework will employ a two-number versioning system to track and identify major and minor changes.The first number will represent the generation of the AIRMF and its companion documents (e.g., 1.0) and will
change only with major revisions. Minor revisions will be tracked using “.n” after the generation number
(e.g.,1.1). AllchangeswillbetrackedusingaVersionControlTablewhichidentifiesthehistory,including
version number, date of change, and description of change. NIST plans to update the AI RMF Playbook
frequently.CommentsontheAIRMFPlaybookmaybesentviaemailtoAIframework@nist.govatanytime
andwillbereviewedandintegratedonasemi-annualbasis.Table of Contents
ExecutiveSummary 1
Part1: FoundationalInformation 4
1 FramingRisk 4
1.1 UnderstandingandAddressingRisks,Impacts,andHarms 4
1.2 ChallengesforAIRiskManagement 5
1.2.1 RiskMeasurement 5
1.2.2 RiskTolerance 7
1.2.3 RiskPrioritization 7
1.2.4 OrganizationalIntegrationandManagementofRisk 8
2 Audience 9
3 AIRisksandTrustworthiness 12
3.1 ValidandReliable 13
3.2 Safe 14
3.3 SecureandResilient 15
3.4 AccountableandTransparent 15
3.5 ExplainableandInterpretable 16
3.6 Privacy-Enhanced 17
3.7 Fair–withHarmfulBiasManaged 17
4 EffectivenessoftheAIRMF 19
Part2: CoreandProfiles 20
5 AIRMFCore 20
5.1 Govern 21
5.2 Map 24
5.3 Measure 28
5.4 Manage 31
6 AIRMFProfiles 33
AppendixA:DescriptionsofAIActorTasksfromFigures2and3 35
AppendixB:HowAIRisksDifferfromTraditionalSoftwareRisks 38
AppendixC:AIRiskManagementandHuman-AIInteraction 40
AppendixD:AttributesoftheAIRMF 42
List of Tables
Table1 Categoriesandsubcategoriesforthe GOVERN function. 22
Table2 Categoriesandsubcategoriesforthe MAP function. 26
Table3 Categoriesandsubcategoriesforthe MEASURE function. 29
Table4 Categoriesandsubcategoriesforthe MANAGE function. 32
iNISTAI100-1 AIRMF1.0
List of Figures
Fig.1 ExamplesofpotentialharmsrelatedtoAIsystems. TrustworthyAIsystems
and their responsible use can mitigate negative risks and contribute to bene-
fitsforpeople,organizations,andecosystems. 5
Fig.2 Lifecycle and Key Dimensions of an AI System. Modified from OECD
(2022) OECD Framework for the Classification of AI systems — OECD
Digital Economy Papers. The two inner circles show AI systems’ key di-
mensions and the outer circle shows AI lifecycle stages. Ideally, risk man-
agement efforts start with the Plan and Design function in the application
context and are performed throughout the AI system lifecycle. See Figure 3
forrepresentativeAIactors. 10
Fig.3 AI actors across AI lifecycle stages. See Appendix A for detailed descrip-
tions of AI actor tasks, including details about testing, evaluation, verifica-
tion, and validation tasks. Note that AI actors in the AI Model dimension
(Figure2)areseparatedasabestpractice,withthosebuildingandusingthe
modelsseparatedfromthoseverifyingandvalidatingthemodels. 11
Fig.4 Characteristics of trustworthy AI systems. Valid & Reliable is a necessary
condition of trustworthiness and is shown as the base for other trustworthi-
ness characteristics. Accountable & Transparent is shown as a vertical box
becauseitrelatestoallothercharacteristics. 12
Fig.5 Functions organize AI risk management activities at their highest level to
govern, map, measure, and manage AI risks. Governance is designed to be
a cross-cutting function to inform and be infused throughout the other three
functions. 20
PageiiNISTAI100-1 AIRMF1.0
Executive Summary
Artificial intelligence (AI) technologies have significant potential to transform society and
people’s lives – from commerce and health to transportation and cybersecurity to the envi-
ronment and our planet. AI technologies can drive inclusive economic growth and support
scientific advancements that improve the conditions of our world. AI technologies, how-
ever,alsoposerisksthatcannegativelyimpactindividuals,groups,organizations,commu-
nities,society,theenvironment,andtheplanet. Likerisksforothertypesoftechnology,AI
riskscanemergeinavarietyofwaysandcanbecharacterizedaslong-orshort-term,high-
orlow-probability,systemicorlocalized,andhigh-orlow-impact.
The AI RMF refers to an AI system as an engineered or machine-based system that
can,foragivensetofobjectives,generateoutputssuchaspredictions,recommenda-
tions,ordecisionsinfluencingrealorvirtualenvironments. AIsystemsaredesigned
tooperatewithvaryinglevelsofautonomy(Adaptedfrom: OECDRecommendation
onAI:2019; ISO/IEC 22989:2022).
Whiletherearemyriadstandardsandbestpracticestohelporganizationsmitigatetherisks
of traditional software or information-based systems, the risks posed by AI systems are in
manywaysunique(SeeAppendixB).AIsystems,forexample,maybetrainedondatathat
canchangeovertime,sometimessignificantlyandunexpectedly,affectingsystemfunction-
ality and trustworthiness in ways that are hard to understand. AI systems and the contexts
inwhichtheyaredeployedarefrequentlycomplex,makingitdifficulttodetectandrespond
to failures when they occur. AI systems are inherently socio-technical in nature, meaning
they are influenced by societal dynamics and human behavior. AI risks – and benefits –
can emerge from the interplay of technical aspects combined with societal factors related
to how a system is used, its interactions with other AI systems, who operates it, and the
socialcontextinwhichitisdeployed.
TheserisksmakeAIauniquelychallengingtechnologytodeployandutilizebothfororga-
nizations and within society. Without proper controls, AI systems can amplify, perpetuate,
or exacerbate inequitable or undesirable outcomes for individuals and communities. With
propercontrols,AIsystemscanmitigateandmanageinequitableoutcomes.
AI risk management is a key component of responsible development and use of AI sys-
tems. Responsible AI practices can help align the decisions about AI system design, de-
velopment, and uses with intended aim and values. Core concepts in responsible AI em-
phasizehumancentricity,socialresponsibility,andsustainability. AIriskmanagementcan
drive responsible uses and practices by prompting organizations and their internal teams
who design, develop, and deploy AI to think more critically about context and potential
or unexpected negative and positive impacts. Understanding and managing the risks of AI
systemswillhelptoenhancetrustworthiness,andinturn,cultivatepublictrust.
Page1NISTAI100-1 AIRMF1.0
Social responsibility can refer to the organization’s responsibility “for the impacts
of its decisions and activities on society and the environment through transparent
and ethical behavior” (ISO 26000:2010). Sustainability refers to the “state of the
global system, including environmental, social, and economic aspects, in which the
needs of the present are met without compromising the ability of future generations
to meet their own needs” (ISO/IEC TR 24368:2022). Responsible AI is meant to
result in technology that is also equitable and accountable. The expectation is that
organizational practices are carried out in accord with “professional responsibility,”
defined by ISO as an approach that “aims to ensure that professionals who design,
develop, or deploy AI systems and applications or AI-based products or systems,
recognize their unique position to exert influence on people, society, and the future
ofAI”(ISO/IEC TR 24368:2022).
As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283),
the goal of the AI RMF is to offer a resource to the organizations designing, developing,
deploying,orusingAIsystemstohelpmanagethemanyrisksofAIandpromotetrustwor-
thy and responsible development and use of AI systems. The Framework is intended to be
voluntary,rights-preserving,non-sector-specific,anduse-caseagnostic,providingflexibil-
ity to organizations of all sizes and in all sectors and throughout society to implement the
approachesintheFramework.
The Framework is designed to equip organizations and individuals – referred to here as
AI actors – with approaches that increase the trustworthiness of AI systems, and to help
foster the responsible design, development, deployment, and use of AI systems over time.
AI actors are defined by the Organisation for Economic Co-operation and Development
(OECD) as “those who play an active role in the AI system lifecycle, including organiza-
tions and individuals that deploy or operate AI” [OECD (2019) Artificial Intelligence in
Society—OECDiLibrary](SeeAppendixA).
The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies
continue to develop, and to be operationalized by organizations in varying degrees and
capacities so society can benefit from AI while also being protected from its potential
harms.
The Framework and supporting resources will be updated, expanded, and improved based
on evolving technology, the standards landscape around the world, and AI community ex-
perienceandfeedback. NISTwillcontinuetoaligntheAIRMFandrelatedguidancewith
applicable international standards, guidelines, and practices. As the AI RMF is put into
use,additionallessonswillbelearnedtoinformfutureupdatesandadditionalresources.
The Framework is divided into two parts. Part 1 discusses how organizations can frame
therisksrelatedtoAIanddescribestheintendedaudience. Next,AIrisksandtrustworthi-
ness are analyzed, outlining the characteristics of trustworthy AI systems, which include
Page2NISTAI100-1 AIRMF1.0
valid and reliable, safe, secure and resilient, accountable and transparent, explainable and
interpretable,privacyenhanced,andfairwiththeirharmfulbiasesmanaged.
Part 2 comprises the “Core” of the Framework. It describes four specific functions to help
organizations address the risks of AI systems in practice. These functions – GOVERN,
MAP, MEASURE, and MANAGE – are broken down further into categories and subcate-
gories. While GOVERN applies to all stages of organizations’ AI risk management pro-
cesses and procedures, the MAP, MEASURE, and MANAGE functions can be applied in AI
system-specificcontextsandatspecificstagesoftheAIlifecycle.
Additional resources related to the Framework are included in the AI RMF Playbook,
whichisavailableviatheNISTAIRMFwebsite:
https://www.nist.gov/itl/ai-risk-management-framework.
Development of the AI RMF by NIST in collaboration with the private and public sec-
tors is directed and consistent with its broader AI efforts called for by the National AI
Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom-
mendations, and the Plan for Federal Engagement in Developing Technical Standards and
Related Tools. Engagement with the AI community during this Framework’s development
– via responses to a formal Request for Information, three widely attended workshops,
publiccommentsonaconceptpaperandtwodraftsoftheFramework,discussionsatmul-
tiplepublicforums,andmanysmallgroupmeetings–hasinformeddevelopmentoftheAI
RMF 1.0 as well as AI research and development and evaluation conducted by NIST and
others. Priority research and additional guidance that will enhance this Framework will be
captured in an associated AI Risk Management Framework Roadmap to which NIST and
thebroadercommunitycancontribute.
Page3NISTAI100-1 AIRMF1.0
Part 1: Foundational Information
1. Framing Risk
AI risk management offers a path to minimize potential negative impacts of AI systems,
such as threats to civil liberties and rights, while also providing opportunities to maximize
positive impacts. Addressing, documenting, and managing AI risks and potential negative
impactseffectivelycanleadtomoretrustworthyAIsystems.
1.1 UnderstandingandAddressingRisks,Impacts,andHarms
InthecontextoftheAIRMF,riskreferstothecompositemeasureofanevent’sprobability
of occurring and the magnitude or degree of the consequences of the corresponding event.
The impacts, or consequences, of AI systems can be positive, negative, or both and can
result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the
negative impact of a potential event, risk is a function of 1) the negative impact, or magni-
tude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of
occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be
experienced by individuals, groups, communities, organizations, society, the environment,
andtheplanet.
“Riskmanagementreferstocoordinatedactivitiestodirectandcontrolanorganiza-
tionwithregardtorisk”(Source: ISO 31000:2018).
While risk management processes generally address negative impacts, this Framework of-
fers approaches to minimize anticipated negative impacts of AI systems and identify op-
portunities to maximize positive impacts. Effectively managing the risk of potential harms
couldleadtomoretrustworthyAIsystemsandunleashpotentialbenefitstopeople(individ-
uals,communities,andsociety),organizations,andsystems/ecosystems. Riskmanagement
canenableAIdevelopersanduserstounderstandimpactsandaccountfortheinherentlim-
itations and uncertainties in their models and systems, which in turn can improve overall
system performance and trustworthiness and the likelihood that AI technologies will be
usedinwaysthatarebeneficial.
TheAIRMFisdesignedtoaddressnewrisksastheyemerge. Thisflexibilityisparticularly
important where impacts are not easily foreseeable and applications are evolving. While
someAIrisksandbenefitsarewell-known,itcanbechallengingtoassessnegativeimpacts
andthedegreeofharms. Figure1providesexamplesofpotentialharmsthatcanberelated
toAIsystems.
AIriskmanagementeffortsshouldconsiderthathumansmayassumethatAIsystemswork
– and work well – in all settings. For example, whether correct or not, AI systems are
often perceived as being more objective than humans or as offering greater capabilities
thangeneralsoftware.
Page4NISTAI100-1 AIRMF1.0
Fig.1. ExamplesofpotentialharmsrelatedtoAIsystems. TrustworthyAIsystemsandtheir
responsibleusecanmitigatenegativerisksandcontributetobenefitsforpeople,organizations,and
ecosystems.
1.2 ChallengesforAIRiskManagement
Severalchallengesaredescribedbelow. Theyshouldbetakenintoaccountwhenmanaging
risksinpursuitofAItrustworthiness.
1.2.1 RiskMeasurement
AI risks or failures that are not well-defined or adequately understood are difficult to mea-
surequantitativelyorqualitatively. TheinabilitytoappropriatelymeasureAIrisksdoesnot
implythatanAIsystemnecessarilyposeseitherahighorlowrisk. Someriskmeasurement
challengesinclude:
Risksrelatedtothird-partysoftware,hardware,anddata: Third-partydataorsystems
can accelerate research and development and facilitate technology transition. They also
maycomplicateriskmeasurement. Riskcanemergebothfromthird-partydata,softwareor
hardwareitselfandhowitisused. Riskmetricsormethodologiesusedbytheorganization
developing the AI system may not align with the risk metrics or methodologies uses by
the organization deploying or operating the system. Also, the organization developing
theAIsystemmaynotbetransparentabouttheriskmetricsormethodologiesitused. Risk
measurementandmanagementcanbecomplicatedbyhowcustomersuseorintegratethird-
party data or systems into AI products or services, particularly without sufficient internal
governancestructuresandtechnicalsafeguards. Regardless,allpartiesandAIactorsshould
manage risk in the AI systems they develop, deploy, or use as standalone or integrated
components.
Tracking emergent risks: Organizations’ risk management efforts will be enhanced by
identifying and tracking emergent risks and considering techniques for measuring them.
Page5NISTAI100-1 AIRMF1.0
AI system impact assessment approaches can help AI actors understand potential impacts
orharmswithinspecificcontexts.
Availability of reliable metrics: The current lack of consensus on robust and verifiable
measurement methods for risk and trustworthiness, and applicability to different AI use
cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure
negative risk or harms include the reality that development of metrics is often an institu-
tionalendeavorandmayinadvertentlyreflectfactorsunrelatedtotheunderlyingimpact. In
addition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-
come relied upon in unexpected ways, or fail to account for differences in affected groups
andcontexts.
Approachesformeasuringimpactsonapopulationworkbestiftheyrecognizethatcontexts
matter,thatharmsmayaffectvariedgroupsorsub-groupsdifferently,andthatcommunities
orothersub-groupswhomaybeharmedarenotalwaysdirectusersofasystem.
Risk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI
lifecycle may yield different results than measuring risk at a later stage; some risks may
be latent at a given point in time and may increase as AI systems adapt and evolve. Fur-
thermore, different AI actors across the AI lifecycle can have different risk perspectives.
For example, an AI developer who makes AI software available, such as pre-trained mod-
els, can have a different risk perspective than an AI actor who is responsible for deploying
that pre-trained model in a specific use case. Such deployers may not recognize that their
particularusescouldentailriskswhichdifferfromthoseperceivedbytheinitialdeveloper.
All involved AI actors share responsibilities for designing, developing, and deploying a
trustworthyAIsystemthatisfitforpurpose.
Risk in real-world settings: While measuring AI risks in a laboratory or a controlled
environmentmayyieldimportantinsightspre-deployment,thesemeasurementsmaydiffer
fromrisksthatemergeinoperational,real-worldsettings.
Inscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability
can be a result of the opaque nature of AI systems (limited explainability or interpretabil-
ity), lack of transparency or documentation in AI system development or deployment, or
inherentuncertaintiesinAIsystems.
Humanbaseline: RiskmanagementofAIsystemsthatareintendedtoaugmentorreplace
human activity, for example decision making, requires some form of baseline metrics for
comparison. ThisisdifficulttosystematizesinceAIsystemscarryoutdifferenttasks–and
performtasksdifferently–thanhumans.
Page6NISTAI100-1 AIRMF1.0
1.2.2 RiskTolerance
While the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk
tolerance refers to the organization’s or AI actor’s (see Appendix A) readiness to bear the
risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-
tory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that
isacceptabletoorganizationsorsocietyarehighlycontextualandapplicationanduse-case
specific. Risk tolerances can be influenced by policies and norms established by AI sys-
tem owners, organizations, industries, communities, or policy makers. Risk tolerances are
likely to change over time as AI systems, policies, and norms evolve. Different organiza-
tions may have varied risk tolerances due to their particular organizational priorities and
resourceconsiderations.
Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-
tinuetobedevelopedanddebatedbybusinesses,governments,academia,andcivilsociety.
TotheextentthatchallengesforspecifyingAIrisktolerancesremainunresolved,theremay
becontextswhereariskmanagementframeworkisnotyetreadilyapplicableformitigating
negativeAIrisks.
The Framework is intended to be flexible and to augment existing risk practices
which should align with applicable laws, regulations, and norms. Organizations
should follow existing regulations and guidelines for risk criteria, tolerance, and
response established by organizational, domain, discipline, sector, or professional
requirements. Somesectorsorindustriesmayhaveestablisheddefinitionsofharmor
established documentation, reporting, and disclosure requirements. Within sectors,
risk management may depend on existing guidelines for specific applications and
use case settings. Where established guidelines do not exist, organizations should
definereasonablerisktolerance. Oncetoleranceisdefined,thisAIRMFcanbeused
tomanagerisksandtodocumentriskmanagementprocesses.
1.2.3 RiskPrioritization
Attemptingtoeliminatenegativeriskentirelycanbecounterproductiveinpracticebecause
not all incidents and failures can be eliminated. Unrealistic expectations about risk may
lead organizations to allocate resources in a manner that makes risk triage inefficient or
impractical or wastes scarce resources. A risk management culture can help organizations
recognize that not all AI risks are the same, and resources can be allocated purposefully.
Actionable risk management efforts lay out clear guidelines for assessing trustworthiness
of each AI system an organization develops or deploys. Policies and resources should be
prioritizedbasedontheassessedrisklevelandpotentialimpactofanAIsystem. Theextent
to which an AI system may be customized or tailored to the specific context of use by the
AIdeployercanbeacontributingfactor.
Page7NISTAI100-1 AIRMF1.0
When applying the AI RMF, risks which the organization determines to be highest for the
AI systems within a given context of use call for the most urgent prioritization and most
thorough risk management process. In cases where an AI system presents unacceptable
negativerisklevels–suchaswheresignificantnegativeimpactsareimminent,severeharms
are actually occurring, or catastrophic risks are present – development and deployment
should cease in a safe manner until risks can be sufficiently managed. If an AI system’s
development,deployment,andusecasesarefoundtobelow-riskinaspecificcontext,that
maysuggestpotentiallylowerprioritization.
RiskprioritizationmaydifferbetweenAIsystemsthataredesignedordeployedtodirectly
interact with humans as compared to AI systems that are not. Higher initial prioritization
maybecalledforinsettingswheretheAIsystemistrainedonlargedatasetscomprisedof
sensitiveorprotecteddatasuchaspersonallyidentifiableinformation,orwheretheoutputs
oftheAIsystemshavedirectorindirectimpactonhumans. AIsystemsdesignedtointeract
only with computational systems and trained on non-sensitive datasets (for example, data
collectedfromthephysicalenvironment)maycallforlowerinitialprioritization. Nonethe-
less, regularly assessing and prioritizing risk based on context remains important because
non-human-facingAIsystemscanhavedownstreamsafetyorsocialimplications.
Residual risk – defined as risk remaining after risk treatment (Source: ISO GUIDE 73) –
directlyimpactsendusersoraffectedindividualsandcommunities. Documentingresidual
riskswillcallforthesystemprovidertofullyconsidertherisksofdeployingtheAIproduct
andwillinformendusersaboutpotentialnegativeimpactsofinteractingwiththesystem.
1.2.4 OrganizationalIntegrationandManagementofRisk
AI risks should not be considered in isolation. Different AI actors have different responsi-
bilitiesandawarenessdependingontheirrolesinthelifecycle. Forexample,organizations
developing an AI system often will not have information about how the system may be
used. AI risk management should be integrated and incorporated into broader enterprise
riskmanagementstrategiesandprocesses. TreatingAIrisksalongwithothercriticalrisks,
suchascybersecurityandprivacy,willyieldamoreintegratedoutcomeandorganizational
efficiencies.
The AI RMF may be utilized along with related guidance and frameworks for managing
AI system risks or broader enterprise risks. Some risks related to AI systems are common
acrossothertypesofsoftwaredevelopmentanddeployment. Examplesofoverlappingrisks
include: privacy concerns related to the use of underlying data to train AI systems; the en-
ergy and environmental implications associated with resource-heavy computing demands;
securityconcernsrelatedtotheconfidentiality,integrity,andavailabilityofthesystemand
its training and output data; and general security of the underlying software and hardware
forAIsystems.
Page8NISTAI100-1 AIRMF1.0
Organizations need to establish and maintain the appropriate accountability mechanisms,
roles and responsibilities, culture, and incentive structures for risk management to be ef-
fective. UseoftheAIRMFalonewillnotleadtothesechangesorprovidetheappropriate
incentives. Effective risk management is realized through organizational commitment at
senior levels and may require cultural change within an organization or industry. In addi-
tion,smalltomedium-sizedorganizationsmanagingAIrisksorimplementingtheAIRMF
may face different challenges than large organizations, depending on their capabilities and
resources.
2. Audience
Identifying and managing AI risks and potential impacts – both positive and negative – re-
quires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will
represent a diversity of experience, expertise, and backgrounds and comprise demograph-
ically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors
acrosstheAIlifecycleanddimensions.
The OECD has developed a framework for classifying AI lifecycle activities according to
fivekeysocio-technicaldimensions,eachwithpropertiesrelevantforAIpolicyandgover-
nance,includingriskmanagement[OECD(2022)OECDFrameworkfortheClassification
of AI systems — OECD Digital Economy Papers]. Figure 2 shows these dimensions,
slightly modified by NIST for purposes of this framework. The NIST modification high-
lights the importance of test, evaluation, verification, and validation (TEVV) processes
throughoutanAIlifecycleandgeneralizestheoperationalcontextofanAIsystem.
AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI
Model, and Task and Output. AI actors involved in these dimensions who perform or
managethedesign,development,deployment,evaluation,anduseofAIsystemsanddrive
AIriskmanagementeffortsaretheprimaryAIRMFaudience.
RepresentativeAIactorsacrossthelifecycledimensionsarelistedinFigure3anddescribed
in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks
and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific
expertiseareintegratedthroughouttheAIlifecycleandareespeciallylikelytobenefitfrom
theFramework. Performedregularly,TEVVtaskscanprovideinsightsrelativetotechnical,
societal,legal,andethicalstandardsornorms,andcanassistwithanticipatingimpactsand
assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV
allowsforbothmid-courseremediationandpost-hocriskmanagement.
The People & Planet dimension at the center of Figure 2 represents human rights and the
broader well-being of society and the planet. The AI actors in this dimension comprise
a separate AI RMF audience who informs the primary audience. These AI actors may in-
cludetradeassociations,standardsdevelopingorganizations,researchers,advocacygroups,
Page9NISTAI100-1 AIRMF1.0
Fig.2. LifecycleandKeyDimensionsofanAISystem. ModifiedfromOECD(2022)OECD
FrameworkfortheClassificationofAIsystems—OECDDigitalEconomyPapers. Thetwoinner
circlesshowAIsystems’keydimensionsandtheoutercircleshowsAIlifecyclestages. Ideally,
riskmanagementeffortsstartwiththePlanandDesignfunctionintheapplicationcontextandare
performedthroughouttheAIsystemlifecycle. SeeFigure3forrepresentativeAIactors.
environmental groups, civil society organizations, end users, and potentially impacted in-
dividualsandcommunities. Theseactorscan:
• assistinprovidingcontextandunderstandingpotentialandactualimpacts;
• beasourceofformalorquasi-formalnormsandguidanceforAIriskmanagement;
• designateboundariesforAIoperation(technical,societal,legal,andethical);and
• promote discussion of the tradeoffs needed to balance societal values and priorities
related to civil liberties and rights, equity, the environment and the planet, and the
economy.
Successful risk management depends upon a sense of collective responsibility among AI
actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse
perspectives, disciplines, professions, and experiences. Diverse teams contribute to more
open sharing of ideas and assumptions about the purposes and functions of technology –
making these implicit aspects more explicit. This broader collective perspective creates
opportunitiesforsurfacingproblemsandidentifyingexistingandemergentrisks.
Page10NISTAI100-1 AIRMF1.0
htiw,ecitcarptsebasadetarapesera)2erugiF(noisnemidledoMIAehtnisrotcaIAtahtetoN
,gnitsettuobasliatedgnidulcni,sksatrotcaIAfosnoitpircseddeliatedrofAxidneppAeeS
.sledomehtgnitadilavdnagniyfirevesohtmorfdetarapessledomehtgnisudnagnidliubesoht
.sksatnoitadilavdna,noitacfiirev,noitaulave
.segatselcycefilIAssorcasrotcaIA
.3.giF
Page11NISTAI100-1 AIRMF1.0
3. AI Risks and Trustworthiness
For AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-
teria that are of value to interested parties. Approaches which enhance AI trustworthiness
can reduce negative AI risks. This Framework articulates the following characteristics of
trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI
systems include: valid and reliable, safe, secure and resilient, accountable and trans-
parent, explainable and interpretable, privacy-enhanced, and fair with harmful bias
managed. Creating trustworthy AI requires balancing each of these characteristics based
on the AI system’s context of use. While all characteristics are socio-technical system at-
tributes, accountability and transparency also relate to the processes and activities internal
to an AI system and its external setting. Neglecting these characteristics can increase the
probabilityandmagnitudeofnegativeconsequences.
Fig.4. CharacteristicsoftrustworthyAIsystems. Valid&Reliableisanecessaryconditionof
trustworthinessandisshownasthebaseforothertrustworthinesscharacteristics. Accountable&
Transparentisshownasaverticalboxbecauseitrelatestoallothercharacteristics.
Trustworthinesscharacteristics(showninFigure4)areinextricablytiedtosocialandorga-
nizationalbehavior,thedatasetsusedbyAIsystems,selectionofAImodelsandalgorithms
andthedecisionsmadebythosewhobuildthem,andtheinteractionswiththehumanswho
provideinsightfromandoversightofsuchsystems. Humanjudgmentshouldbeemployed
when deciding on the specific metrics related to AI trustworthiness characteristics and the
precisethresholdvaluesforthosemetrics.
Addressing AItrustworthiness characteristics individuallywill not ensure AIsystem trust-
worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-
ting, and some will be more or less important in any given situation. Ultimately, trustwor-
thinessisasocialconceptthatrangesacrossaspectrumandisonlyasstrongasitsweakest
characteristics.
WhenmanagingAIrisks,organizationscanfacedifficultdecisionsinbalancingthesechar-
acteristics. Forexample,incertainscenariostradeoffsmayemergebetweenoptimizingfor
interpretability and achieving privacy. In other cases, organizations might face a tradeoff
between predictive accuracy and interpretability. Or, under certain conditions such as data
sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions
Page12NISTAI100-1 AIRMF1.0
about fairness and other values in certain domains. Dealing with tradeoffs requires tak-
ing into account the decision-making context. These analyses can highlight the existence
andextentoftradeoffsbetweendifferentmeasures,buttheydonotanswerquestionsabout
howtonavigatethetradeoff. Thosedependonthevaluesatplayintherelevantcontextand
shouldberesolvedinamannerthatisbothtransparentandappropriatelyjustifiable.
There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For
example, subject matter experts can assist in the evaluation of TEVV findings and work
with product and deployment teams to align TEVV parameters to requirements and de-
ployment conditions. When properly resourced, increasing the breadth and diversity of
input from interested parties and relevant AI actors throughout the AI lifecycle can en-
hance opportunities for informing contextually sensitive evaluations, and for identifying
AI system benefits and positive impacts. These practices can increase the likelihood that
risksarisinginsocialcontextsaremanagedappropriately.
Understanding and treatment of trustworthiness characteristics depends on an AI actor’s
particularrolewithintheAIlifecycle. ForanygivenAIsystem,anAIdesignerordeveloper
mayhaveadifferentperceptionofthecharacteristicsthanthedeployer.
Trustworthiness characteristics explained in this document influence each other.
Highly secure but unfair systems, accurate but opaque and uninterpretable systems,
and inaccurate but secure, privacy-enhanced, and transparent systems are all unde-
sirable. Acomprehensiveapproachtoriskmanagementcallsforbalancingtradeoffs
among the trustworthiness characteristics. It is the joint responsibility of all AI ac-
tors to determine whether AI technology is an appropriate or necessary tool for a
givencontextorpurpose,andhowtouseitresponsibly. Thedecisiontocommission
or deploy an AI system should be based on a contextual assessment of trustworthi-
ness characteristics and the relative risks, impacts, costs, and benefits, and informed
byabroadsetofinterestedparties.
3.1 ValidandReliable
Validation is the “confirmation, through the provision of objective evidence, that the re-
quirements for a specific intended use or application have been fulfilled” (Source: ISO
9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-
alizedtodataandsettingsbeyondtheirtrainingcreatesandincreasesnegativeAIrisksand
reducestrustworthiness.
Reliabilityisdefinedinthesamestandardasthe“abilityofanitemtoperformasrequired,
without failure, for a given time interval, under given conditions” (Source: ISO/IEC TS
5723:2022). Reliability is a goal for overall correctness of AI system operation under the
conditions of expected use and over a given period of time, including the entire lifetime of
thesystem.
Page13NISTAI100-1 AIRMF1.0
Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and
canbeintensionwithoneanotherinAIsystems.
Accuracy is defined by ISO/IEC TS 5723:2022 as “closeness of results of observations,
computations, or estimates to the true values or the values accepted as being true.” Mea-
sures of accuracy should consider computational-centric measures (e.g., false positive and
false negative rates), human-AI teaming, and demonstrate external validity (generalizable
beyond the training conditions). Accuracy measurements should always be paired with
clearlydefinedandrealistictestsets–thatarerepresentativeofconditionsofexpecteduse
– and details about test methodology; these should be included in associated documen-
tation. Accuracy measurements may include disaggregation of results for different data
segments.
Robustness or generalizability is defined as the “ability of a system to maintain its level
of performance under a variety of circumstances” (Source: ISO/IEC TS 5723:2022). Ro-
bustness is a goal for appropriate system functionality in a broad set of conditions and
circumstances, including uses of AI systems not initially anticipated. Robustness requires
not only that the system perform exactly as it does under expected uses, but also that it
should perform in ways that minimize potential harms to people if it is operating in an
unexpectedsetting.
Validity and reliability for deployed AI systems are often assessed by ongoing testing or
monitoring that confirms a system is performing as intended. Measurement of validity,
accuracy,robustness,andreliabilitycontributetotrustworthinessandshouldtakeintocon-
siderationthatcertaintypesoffailurescancausegreaterharm. AIriskmanagementefforts
should prioritize the minimization of potential negative impacts, and may need to include
humaninterventionincaseswheretheAIsystemcannotdetectorcorrecterrors.
3.2 Safe
AI systems should “not under defined conditions, lead to a state in which human life,
health,property,ortheenvironmentisendangered”(Source: ISO/IEC TS 5723:2022). Safe
operationofAIsystemsisimprovedthrough:
• responsibledesign,development,anddeploymentpractices;
• clearinformationtodeployersonresponsibleuseofthesystem;
• responsibledecision-makingbydeployersandendusers;and
• explanationsanddocumentationofrisksbasedonempiricalevidenceofincidents.
Different types of safety risks may require tailored AI risk management approaches based
on context and the severity of potential risks presented. Safety risks that pose a potential
riskofseriousinjuryordeathcallforthemosturgentprioritizationandmostthoroughrisk
managementprocess.
Page14NISTAI100-1 AIRMF1.0
Employing safety considerations during the lifecycle and starting as early as possible with
planninganddesigncanpreventfailuresorconditionsthatcanrenderasystemdangerous.
Other practical approaches for AI safety often relate to rigorous simulation and in-domain
testing, real-time monitoring, and the ability to shut down, modify, or have human inter-
ventionintosystemsthatdeviatefromintendedorexpectedfunctionality.
AI safety risk management approaches should take cues from efforts and guidelines for
safety in fields such as transportation and healthcare, and align with existing sector- or
application-specificguidelinesorstandards.
3.3 SecureandResilient
AI systems, as well as the ecosystems in which they are deployed, may be said to be re-
silientiftheycanwithstandunexpectedadverseeventsorunexpectedchangesintheirenvi-
ronmentoruse–oriftheycanmaintaintheirfunctionsandstructureinthefaceofinternal
and external change and degrade safely and gracefully when this is necessary (Adapted
from: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples,
data poisoning, and the exfiltration of models, training data, or other intellectual property
through AI system endpoints. AI systems that can maintain confidentiality, integrity, and
availability through protection mechanisms that prevent unauthorized access and use may
be said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Manage-
mentFrameworkareamongthosewhichareapplicablehere.
Security and resilience are related but distinct characteristics. While resilience is the abil-
ity to return to normal function after an unexpected adverse event, security includes re-
silience but also encompasses protocols to avoid, protect against, respond to, or recover
from attacks. Resilience relates to robustness and goes beyond the provenance of the data
toencompassunexpectedoradversarialuse(orabuseormisuse)ofthemodelordata.
3.4 AccountableandTransparent
Trustworthy AI depends upon accountability. Accountability presupposes transparency.
TransparencyreflectstheextenttowhichinformationaboutanAIsystemanditsoutputsis
availabletoindividualsinteractingwithsuchasystem–regardlessofwhethertheyareeven
awarethattheyaredoingso. Meaningfultransparencyprovidesaccesstoappropriatelevels
of information based on the stage of the AI lifecycle and tailored to the role or knowledge
of AI actors or individuals interacting with or using the AI system. By promoting higher
levelsofunderstanding,transparencyincreasesconfidenceintheAIsystem.
This characteristic’s scope spans from design decisions and training data to model train-
ing, the structure of the model, its intended use cases, and how and when deployment,
post-deployment, or end user decisions were made and by whom. Transparency is often
necessaryforactionableredressrelatedtoAIsystemoutputsthatareincorrectorotherwise
lead to negative impacts. Transparency should consider human-AI interaction: for exam-
Page15NISTAI100-1 AIRMF1.0
ple, how a human operator or user is notified when a potential or actual adverse outcome
caused by an AI system is detected. A transparent system is not necessarily an accurate,
privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an
opaque system possesses such characteristics, and to do so over time as complex systems
evolve.
TheroleofAIactorsshouldbeconsideredwhenseekingaccountabilityfortheoutcomesof
AIsystems. TherelationshipbetweenriskandaccountabilityassociatedwithAIandtech-
nologicalsystemsmorebroadlydiffersacrosscultural,legal,sectoral,andsocietalcontexts.
When consequences are severe, such as when life and liberty are at stake, AI developers
and deployers should consider proportionally and proactively adjusting their transparency
andaccountabilitypractices. Maintainingorganizationalpracticesandgoverningstructures
forharmreduction,likeriskmanagement,canhelpleadtomoreaccountablesystems.
Measures to enhance transparency and accountability should also consider the impact of
theseeffortsontheimplementingentity,includingthelevelofnecessaryresourcesandthe
needtosafeguardproprietaryinformation.
Maintaining the provenance of training data and supporting attribution of the AI system’s
decisions to subsets of training data can assist with both transparency and accountability.
Training data may also be subject to copyright and should follow applicable intellectual
propertyrightslaws.
AstransparencytoolsforAIsystemsandrelateddocumentationcontinuetoevolve,devel-
opers of AI systems are encouraged to test different types of transparency tools in cooper-
ationwithAIdeployerstoensurethatAIsystemsareusedasintended.
3.5 ExplainableandInterpretable
Explainability refers to a representation of the mechanisms underlying AI systems’ oper-
ation, whereas interpretability refers to the meaning of AI systems’ output in the context
of their designed functional purposes. Together, explainability and interpretability assist
those operating or overseeing an AI system, as well as users of an AI system, to gain
deeper insights into the functionality and trustworthiness of the system, including its out-
puts. The underlying assumption is that perceptions of negative risk stem from a lack of
ability to make sense of, or contextualize, system output appropriately. Explainable and
interpretableAIsystemsofferinformationthatwillhelpendusersunderstandthepurposes
andpotentialimpactofanAIsystem.
Risk from lack of explainability may be managed by describing how AI systems function,
with descriptions tailored to individual differences such as the user’s role, knowledge, and
skilllevel. Explainablesystemscanbedebuggedandmonitoredmoreeasily,andtheylend
themselvestomorethoroughdocumentation,audit,andgovernance.
Page16NISTAI100-1 AIRMF1.0
Risks to interpretability often can be addressed by communicating a description of why
an AI system made a particular prediction or recommendation. (See “Four Principles of
Explainable Artificial Intelligence” and “Psychological Foundations of Explainability and
InterpretabilityinArtificialIntelligence”foundhere.)
Transparency, explainability, and interpretability are distinct characteristics that support
each other. Transparency can answer the question of “what happened” in the system. Ex-
plainability can answer the question of “how” a decision was made in the system. Inter-
pretability can answer the question of “why” a decision was made by the system and its
meaningorcontexttotheuser.
3.6 Privacy-Enhanced
Privacyrefersgenerallytothenormsandpracticesthathelptosafeguardhumanautonomy,
identity, and dignity. These norms and practices typically address freedom from intrusion,
limiting observation, or individuals’ agency to consent to disclosure or control of facets of
their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool
forImprovingPrivacythroughEnterpriseRiskManagement.)
Privacyvaluessuchasanonymity,confidentiality,andcontrolgenerallyshouldguidechoices
for AI system design, development, and deployment. Privacy-related risks may influence
security, bias, and transparency and come with tradeoffs with these other characteristics.
Likesafetyandsecurity,specifictechnicalfeaturesofanAIsystemmaypromoteorreduce
privacy. AI systems can also present new risks to privacy by allowing inference to identify
individualsorpreviouslyprivateinformationaboutindividuals.
Privacy-enhancingtechnologies(“PETs”)forAI,aswellasdataminimizingmethodssuch
as de-identification and aggregation for certain model outputs, can support design for
privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-
enhancing techniques can result in a loss in accuracy, affecting decisions about fairness
andothervaluesincertaindomains.
3.7 Fair–withHarmfulBiasManaged
FairnessinAIincludesconcernsforequalityandequitybyaddressingissuessuchasharm-
fulbiasanddiscrimination. Standardsoffairnesscanbecomplexanddifficulttodefinebe-
causeperceptionsoffairnessdifferamongculturesandmayshiftdependingonapplication.
Organizations’ risk management efforts will be enhanced by recognizing and considering
these differences. Systems in which harmful biases are mitigated are not necessarily fair.
For example, systems in which predictions are somewhat balanced across demographic
groups may still be inaccessible to individuals with disabilities or affected by the digital
divideormayexacerbateexistingdisparitiesorsystemicbiases.
Page17NISTAI100-1 AIRMF1.0
Biasisbroaderthandemographicbalanceanddatarepresentativeness. NISThasidentified
three major categories of AI bias to be considered and managed: systemic, computational
and statistical, and human-cognitive. Each of these can occur in the absence of prejudice,
partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga-
nizational norms, practices, and processes across the AI lifecycle, and the broader society
that uses AI systems. Computational and statistical biases can be present in AI datasets
andalgorithmicprocesses,andoftenstemfromsystematicerrorsduetonon-representative
samples. Human-cognitive biases relate to how an individual or group perceives AI sys-
tem information to make a decision or fill in missing information, or how humans think
about purposes and functions of an AI system. Human-cognitive biases are omnipresent
indecision-makingprocessesacrosstheAIlifecycleandsystemuse,includingthedesign,
implementation,operation,andmaintenanceofAI.
Bias exists in many forms and can become ingrained in the automated systems that help
make decisions about our lives. While bias is not always a negative phenomenon, AI sys-
tems can potentially increase the speed and scale of biases and perpetuate and amplify
harmstoindividuals,groups,communities,organizations,andsociety. Biasistightlyasso-
ciated with the concepts of transparency as well as fairness in society. (For more informa-
tionaboutbias,includingthethreecategories,seeNISTSpecialPublication1270,Towards
aStandardforIdentifyingandManagingBiasinArtificialIntelligence.)
Page18NISTAI100-1 AIRMF1.0
4. Effectiveness of the AI RMF
Evaluations of AI RMF effectiveness – including ways to measure bottom-line improve-
ments in the trustworthiness of AI systems – will be part of future NIST activities, in
conjunctionwiththeAIcommunity.
Organizations and other users of the Framework are encouraged to periodically evaluate
whether the AI RMF has improved their ability to manage AI risks, including but not lim-
itedtotheirpolicies,processes,practices,implementationplans,indicators,measurements,
and expected outcomes. NIST intends to work collaboratively with others to develop met-
rics, methodologies, and goals for evaluating the AI RMF’s effectiveness, and to broadly
shareresultsandsupportinginformation. Frameworkusersareexpectedtobenefitfrom:
• enhanced processes for governing, mapping, measuring, and managing AI risk, and
clearlydocumentingoutcomes;
• improved awareness of the relationships and tradeoffs among trustworthiness char-
acteristics,socio-technicalapproaches,andAIrisks;
• explicitprocessesformakinggo/no-gosystemcommissioninganddeploymentdeci-
sions;
• established policies, processes, practices, and procedures for improving organiza-
tionalaccountabilityeffortsrelatedtoAIsystemrisks;
• enhancedorganizationalculturewhichprioritizestheidentificationandmanagement
of AI system risks and potential impacts to individuals, communities, organizations,
andsociety;
• better information sharing within and across organizations about risks, decision-
makingprocesses,responsibilities,commonpitfalls,TEVVpractices,andapproaches
forcontinuousimprovement;
• greatercontextualknowledgeforincreasedawarenessofdownstreamrisks;
• strengthenedengagementwithinterestedpartiesandrelevantAIactors;and
• augmentedcapacityforTEVVofAIsystemsandassociatedrisks.
Page19NISTAI100-1 AIRMF1.0
Part 2: Core and Profiles
5. AI RMF Core
TheAIRMFCoreprovidesoutcomesandactionsthatenabledialogue,understanding,and
activities to manage AI risks and responsibly develop trustworthy AI systems. As illus-
trated in Figure 5, the Core is composed of four functions: GOVERN, MAP, MEASURE,
and MANAGE. Each of these high-level functions is broken down into categories and sub-
categories. Categoriesandsubcategoriesaresubdividedintospecificactionsandoutcomes.
Actionsdonotconstituteachecklist,noraretheynecessarilyanorderedsetofsteps.
Fig.5. FunctionsorganizeAIriskmanagementactivitiesattheirhighestleveltogovern,map,
measure,andmanageAIrisks. Governanceisdesignedtobeacross-cuttingfunctiontoinform
andbeinfusedthroughouttheotherthreefunctions.
Risk management should be continuous, timely, and performed throughout the AI system
lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects
diverseandmultidisciplinaryperspectives,potentiallyincludingtheviewsofAIactorsout-
sidetheorganization. Havingadiverseteamcontributestomoreopensharingofideasand
assumptions about purposes and functions of the technology being designed, developed,
Page20NISTAI100-1 AIRMF1.0
deployed, or evaluated – which can create opportunities to surface problems and identify
existingandemergentrisks.
An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available
to help organizations navigate the AI RMF and achieve its outcomes through suggested
tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook
is voluntary and organizations can utilize the suggestions according to their needs and
interests. Playbook users can create tailored guidance selected from suggested material
fortheirownuseandcontributetheirsuggestionsforsharingwiththebroadercommunity.
AlongwiththeAIRMF,thePlaybookispartoftheNISTTrustworthyandResponsibleAI
ResourceCenter.
Framework users may apply these functions as best suits their needs for managing
AI risks based on their resources and capabilities. Some organizations may choose
to select from among the categories and subcategories; others may choose and have
thecapacitytoapplyallcategoriesandsubcategories. Assumingagovernancestruc-
ture is in place, functions may be performed in any order across the AI lifecycle as
deemed to add value by a user of the framework. After instituting the outcomes in
GOVERN, most users of the AI RMF would start with the MAP function and con-
tinue to MEASURE or MANAGE. However users integrate the functions, the process
should be iterative, with cross-referencing between functions as necessary. Simi-
larly, there are categories and subcategories with elements that apply to multiple
functions,orthatlogicallyshouldtakeplacebeforecertainsubcategorydecisions.
5.1 Govern
The GOVERN function:
• cultivatesandimplementsacultureofriskmanagementwithinorganizationsdesign-
ing,developing,deploying,evaluating,oracquiringAIsystems;
• outlines processes, documents, and organizational schemes that anticipate, identify,
andmanagetherisksasystemcanpose,includingtousersandothersacrosssociety
–andprocedurestoachievethoseoutcomes;
• incorporatesprocessestoassesspotentialimpacts;
• provides a structure by which AI risk management functions can align with organi-
zationalprinciples,policies,andstrategicpriorities;
• connects technical aspects of AI system design and development to organizational
values and principles, and enables organizational practices and competencies for the
individuals involved in acquiring, training, deploying, and monitoring such systems;
and
• addresses full product lifecycle and associated processes, including legal and other
issuesconcerninguseofthird-partysoftwareorhardwaresystemsanddata.
Page21NISTAI100-1 AIRMF1.0
GOVERN is a cross-cutting function that is infused throughout AI risk management and
enablestheotherfunctionsoftheprocess. Aspectsof GOVERN,especiallythoserelatedto
compliance or evaluation, should be integrated into each of the other functions. Attention
to governance is a continual and intrinsic requirement for effective AI risk management
overanAIsystem’slifespanandtheorganization’shierarchy.
Strong governance can drive and enhance internal practices and norms to facilitate orga-
nizational risk culture. Governing authorities can determine the overarching policies that
direct an organization’s mission, goals, values, culture, and risk tolerance. Senior leader-
ship sets the tone for risk management within an organization, and with it, organizational
culture. Management aligns the technical aspects of AI risk management to policies and
operations. Documentation can enhance transparency, improve human review processes,
andbolsteraccountabilityinAIsystemteams.
After putting in place the structures, systems, processes, and teams described in the GOV-
ERN function, organizations should benefit from a purpose-driven culture focused on risk
understanding and management. It is incumbent on Framework users to continue to ex-
ecute the GOVERN function as knowledge, cultures, and needs or expectations from AI
actorsevolveovertime.
PracticesrelatedtogoverningAIrisksaredescribedintheNISTAIRMFPlaybook. Table
1liststhe GOVERN function’scategoriesandsubcategories.
Table1: Categoriesandsubcategoriesforthe GOVERN function.
Categories Subcategories
GOVERN 1: GOVERN 1.1: Legal and regulatory requirements involving AI
Policies,processes, areunderstood,managed,anddocumented.
procedures,and
GOVERN 1.2: The characteristics of trustworthy AI are inte-
practicesacrossthe
grated into organizational policies, processes, procedures, and
organizationrelated
practices.
tothemapping,
GOVERN 1.3: Processes, procedures, and practices are in place
measuring,and
todeterminetheneededlevelofriskmanagementactivitiesbased
managingofAI
ontheorganization’srisktolerance.
risksareinplace,
transparent,and GOVERN1.4: Theriskmanagementprocessanditsoutcomesare
implemented established through transparent policies, procedures, and other
effectively. controlsbasedonorganizationalriskpriorities.
Continuedonnextpage
Page22NISTAI100-1 AIRMF1.0
Table1: Categoriesandsubcategoriesforthe GOVERN function. (Continued)
Categories Subcategories
GOVERN 1.5: Ongoing monitoring and periodic review of the
risk management process and its outcomes are planned and or-
ganizational roles and responsibilities clearly defined, including
determiningthefrequencyofperiodicreview.
GOVERN 1.6: Mechanisms are in place to inventory AI systems
andareresourcedaccordingtoorganizationalriskpriorities.
GOVERN 1.7: Processes and procedures are in place for decom-
missioning and phasing out AI systems safely and in a man-
ner that does not increase risks or decrease the organization’s
trustworthiness.
GOVERN 2: GOVERN 2.1: Roles and responsibilities and lines of communi-
Accountability cationrelatedtomapping,measuring,andmanagingAIrisksare
structuresarein documented and are clear to individuals and teams throughout
placesothatthe theorganization.
appropriateteams
GOVERN 2.2: The organization’s personnel and partners receive
andindividualsare
AI riskmanagement training to enablethem to perform theirdu-
empowered,
ties and responsibilities consistent with related policies, proce-
responsible,and
dures,andagreements.
trainedformapping,
GOVERN 2.3: Executive leadership of the organization takes re-
measuring,and
sponsibility for decisions about risks associated with AI system
managingAIrisks.
developmentanddeployment.
GOVERN 3: GOVERN 3.1: Decision-making related to mapping, measuring,
Workforcediversity, and managing AI risks throughout the lifecycle is informed by a
equity,inclusion, diverse team (e.g., diversity of demographics, disciplines, expe-
andaccessibility rience,expertise,andbackgrounds).
processesare
GOVERN 3.2: Policies and procedures are in place to define and
prioritizedinthe
differentiate roles and responsibilities for human-AI configura-
mapping,
tionsandoversightofAIsystems.
measuring,and
managingofAI
risksthroughoutthe
lifecycle.
GOVERN 4: GOVERN 4.1: Organizational policies and practices are in place
Organizational tofosteracriticalthinkingandsafety-firstmindsetinthedesign,
teamsarecommitted development, deployment, and uses of AI systems to minimize
toaculture potentialnegativeimpacts.
Continuedonnextpage
Page23NISTAI100-1 AIRMF1.0
Table1: Categoriesandsubcategoriesforthe GOVERN function. (Continued)
Categories Subcategories
thatconsidersand GOVERN 4.2: Organizational teams document the risks and po-
communicatesAI tentialimpactsoftheAItechnologytheydesign,develop,deploy,
risk. evaluate,anduse,andtheycommunicateabouttheimpactsmore
broadly.
GOVERN 4.3: Organizational practices are in place to enable AI
testing,identificationofincidents,andinformationsharing.
GOVERN 5: GOVERN 5.1: Organizational policies and practices are in place
Processesarein to collect, consider, prioritize, and integrate feedback from those
placeforrobust external to the team that developed or deployed the AI system
engagementwith regarding the potential individual and societal impacts related to
relevantAIactors. AIrisks.
GOVERN 5.2: Mechanisms are established to enable the team
that developed or deployed AI systems to regularly incorporate
adjudicated feedback from relevant AI actors into system design
andimplementation.
GOVERN 6: Policies GOVERN 6.1: Policies and procedures are in place that address
andproceduresare AIrisksassociatedwiththird-partyentities,includingrisksofin-
inplacetoaddress fringementofathird-party’sintellectualpropertyorotherrights.
AIrisksandbenefits
GOVERN 6.2: Contingency processes are in place to handle
arisingfrom
failures or incidents in third-party data or AI systems deemed to
third-partysoftware
behigh-risk.
anddataandother
supplychainissues.
5.2 Map
The MAP function establishes the context to frame risks related to an AI system. The AI
lifecycle consists of many interdependent activities involving a diverse set of actors (See
Figure 3). In practice, AI actors in charge of one part of the process often do not have full
visibility or control over other parts and their associated contexts. The interdependencies
between these activities, and among the relevant AI actors, can make it difficult to reliably
anticipateimpactsofAIsystems. Forexample,earlydecisionsinidentifyingpurposesand
objectives of an AI system can alter its behavior and capabilities, and the dynamics of de-
ployment setting (such as end users or impacted individuals) can shape the impacts of AI
system decisions. As a result, the best intentions within one dimension of the AI lifecycle
can be undermined via interactions with decisions and conditions in other, later activities.
Page24NISTAI100-1 AIRMF1.0
This complexity and varying levels of visibility can introduce uncertainty into risk man-
agement practices. Anticipating, assessing, and otherwise addressing potential sources of
negativeriskcanmitigatethisuncertaintyandenhancetheintegrityofthedecisionprocess.
The information gathered while carrying out the MAP function enables negative risk pre-
vention and informs decisions for processes such as model management, as well as an
initial decision about appropriateness or the need for an AI solution. Outcomes in the
MAP function are the basis for the MEASURE and MANAGE functions. Without contex-
tual knowledge, and awareness of risks within the identified contexts, risk management is
difficult to perform. The MAP function is intended to enhance an organization’s ability to
identifyrisksandbroadercontributingfactors.
Implementation of this function is enhanced by incorporating perspectives from a diverse
internal team and engagement with those external to the team that developed or deployed
the AI system. Engagement with external collaborators, end users, potentially impacted
communities, and others may vary based on the risk level of a particular AI system, the
makeup of the internal team, and organizational policies. Gathering such broad perspec-
tives can help organizations proactively prevent negative risks and develop more trustwor-
thyAIsystemsby:
• improvingtheircapacityforunderstandingcontexts;
• checkingtheirassumptionsaboutcontextofuse;
• enabling recognition of when systems are not functional within or out of their in-
tendedcontext;
• identifyingpositiveandbeneficialusesoftheirexistingAIsystems;
• improvingunderstandingoflimitationsinAIandMLprocesses;
• identifyingconstraintsinreal-worldapplicationsthatmayleadtonegativeimpacts;
• identifying known and foreseeable negative impacts related to intended use of AI
systems;and
• anticipatingrisksoftheuseofAIsystemsbeyondintendeduse.
After completing the MAP function, Framework users should have sufficient contextual
knowledge about AI system impacts to inform an initial go/no-go decision about whether
todesign,develop,ordeployanAIsystem. Ifadecisionismadetoproceed,organizations
should utilize the MEASURE and MANAGE functions along with policies and procedures
putintoplaceinthe GOVERN functiontoassistinAIriskmanagementefforts. Itisincum-
bent on Framework users to continue applying the MAP function to AI systems as context,
capabilities,risks,benefits,andpotentialimpactsevolveovertime.
Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table
2liststhe MAP function’scategoriesandsubcategories.
Page25NISTAI100-1 AIRMF1.0
Table2: Categoriesandsubcategoriesforthe MAP function.
Categories Subcategories
MAP 1: Contextis MAP1.1: Intendedpurposes,potentiallybeneficialuses,context-
establishedand specificlaws,normsandexpectations,andprospectivesettingsin
understood. which the AI system will be deployed are understood and docu-
mented. Considerationsinclude: thespecificsetortypesofusers
alongwiththeirexpectations;potentialpositiveandnegativeim-
pacts of system uses to individuals, communities, organizations,
society,andtheplanet;assumptionsandrelatedlimitationsabout
AI system purposes, uses, and risks across the development or
productAIlifecycle;andrelatedTEVVandsystemmetrics.
MAP 1.2: Interdisciplinary AI actors, competencies, skills, and
capacities for establishing context reflect demographic diversity
and broad domain and user experience expertise, and their par-
ticipationisdocumented. Opportunitiesforinterdisciplinarycol-
laborationareprioritized.
MAP 1.3: The organization’s mission and relevant goals for AI
technologyareunderstoodanddocumented.
MAP 1.4: Thebusinessvalueorcontextofbusinessusehasbeen
clearly defined or – in the case of assessing existing AI systems
–re-evaluated.
MAP 1.5: Organizational risk tolerances are determined and
documented.
MAP 1.6: System requirements (e.g., “the system shall respect
theprivacyofitsusers”)areelicitedfromandunderstoodbyrel-
evant AI actors. Design decisions take socio-technical implica-
tionsintoaccounttoaddressAIrisks.
MAP 2: MAP 2.1: The specific tasks and methods used to implement the
Categorizationof tasksthattheAIsystemwillsupportaredefined(e.g.,classifiers,
theAIsystemis generativemodels,recommenders).
performed.
MAP 2.2: Information about the AI system’s knowledge limits
and how system output may be utilized and overseen by humans
is documented. Documentation provides sufficient information
to assist relevant AI actors when making decisions and taking
subsequentactions.
Continuedonnextpage
Page26NISTAI100-1 AIRMF1.0
Table2: Categoriesandsubcategoriesforthe MAP function. (Continued)
Categories Subcategories
MAP 2.3: ScientificintegrityandTEVVconsiderationsareiden-
tified and documented, including those related to experimental
design, data collection and selection (e.g., availability, repre-
sentativeness, suitability), system trustworthiness, and construct
validation.
MAP 3: AI MAP 3.1: Potential benefits of intended AI system functionality
capabilities,targeted andperformanceareexaminedanddocumented.
usage,goals,and
MAP 3.2: Potential costs, including non-monetary costs, which
expectedbenefits
resultfromexpectedorrealizedAIerrorsorsystemfunctionality
andcostscompared
and trustworthiness – as connected to organizational risk toler-
withappropriate
ance–areexaminedanddocumented.
benchmarksare
MAP 3.3: Targeted application scope is specified and docu-
understood.
mentedbasedonthesystem’scapability,establishedcontext,and
AIsystemcategorization.
MAP 3.4: Processes for operator and practitioner proficiency
with AI system performance and trustworthiness – and relevant
technicalstandardsandcertifications–aredefined,assessed,and
documented.
MAP 3.5: Processes for human oversight are defined, assessed,
anddocumentedinaccordancewithorganizationalpoliciesfrom
the GOVERN function.
MAP 4: Risksand MAP4.1: ApproachesformappingAItechnologyandlegalrisks
benefitsaremapped of its components – including the use of third-party data or soft-
forallcomponents ware–areinplace,followed,anddocumented,asarerisksofin-
oftheAIsystem fringementofathirdparty’sintellectualpropertyorotherrights.
includingthird-party
MAP 4.2: Internal risk controls for components of the AI sys-
softwareanddata.
tem, including third-party AI technologies, are identified and
documented.
MAP 5: Impactsto MAP 5.1: Likelihood and magnitude of each identified impact
individuals,groups, (both potentially beneficial and harmful) based on expected use,
communities, past uses of AI systems in similar contexts, public incident re-
organizations,and ports, feedback from those external to the team that developed
societyare or deployed the AI system, or other data are identified and
characterized. documented.
Continuedonnextpage
Page27NISTAI100-1 AIRMF1.0
Table2: Categoriesandsubcategoriesforthe MAP function. (Continued)
Categories Subcategories
MAP 5.2: Practices and personnel for supporting regular en-
gagementwithrelevantAIactorsandintegratingfeedbackabout
positive, negative, and unanticipated impacts are in place and
documented.
5.3 Measure
The MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-
niques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related
impacts. ItusesknowledgerelevanttoAIrisksidentifiedinthe MAP functionandinforms
the MANAGE function. AI systems should be tested before their deployment and regu-
larly while in operation. AI risk measurements include documenting aspects of systems’
functionalityandtrustworthiness.
MeasuringAIrisksincludestrackingmetricsfortrustworthycharacteristics,socialimpact,
and human-AI configurations. Processes developed or adopted in the MEASURE function
should include rigorous software testing and performance assessment methodologies with
associated measures of uncertainty, comparisons to performance benchmarks, and formal-
izedreportinganddocumentationofresults. Processesforindependentreviewcanimprove
the effectiveness of testing and can mitigate internal biases and potential conflicts of inter-
est.
Wheretradeoffsamongthetrustworthycharacteristicsarise,measurementprovidesatrace-
able basis to inform management decisions. Options may include recalibration, impact
mitigation,orremovalofthesystemfromdesign,development,production,oruse,aswell
asarangeofcompensating,detective,deterrent,directive,andrecoverycontrols.
AftercompletingtheMEASUREfunction,objective,repeatable,orscalabletest,evaluation,
verification,andvalidation(TEVV)processesincludingmetrics,methods,andmethodolo-
gies are in place, followed, and documented. Metrics and measurement methodologies
shouldadheretoscientific,legal,andethicalnormsandbecarriedoutinanopenandtrans-
parent process. New types of measurement, qualitative and quantitative, may need to be
developed. The degree to which each measurement type provides unique and meaningful
information to the assessment of AI risks should be considered. Framework users will en-
hancetheircapacitytocomprehensivelyevaluatesystemtrustworthiness,identifyandtrack
existingandemergentrisks,andverifyefficacyofthemetrics. Measurementoutcomeswill
be utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-
cumbent on Framework users to continue applying the MEASURE function to AI systems
asknowledge,methodologies,risks,andimpactsevolveovertime.
Page28NISTAI100-1 AIRMF1.0
PracticesrelatedtomeasuringAIrisksaredescribedintheNISTAIRMFPlaybook. Table
3liststhe MEASURE function’scategoriesandsubcategories.
Table3: Categoriesandsubcategoriesforthe MEASURE function.
Categories Subcategories
MEASURE 1: MEASURE 1.1: Approaches and metrics for measurement of AI
Appropriate risksenumeratedduringtheMAPfunctionareselectedforimple-
methodsandmetrics mentation starting with the most significant AI risks. The risks
areidentifiedand or trustworthiness characteristics that will not – or cannot – be
applied. measuredareproperlydocumented.
MEASURE 1.2: Appropriateness of AI metrics and effectiveness
ofexistingcontrolsareregularlyassessedandupdated,including
reportsoferrorsandpotentialimpactsonaffectedcommunities.
MEASURE 1.3: Internal experts who did not serve as front-line
developers for the system and/or independent assessors are in-
volved in regular assessments and updates. Domain experts,
users, AI actors external to the team that developed or deployed
theAIsystem,andaffectedcommunitiesareconsultedinsupport
ofassessmentsasnecessaryperorganizationalrisktolerance.
MEASURE 2: AI MEASURE2.1: Testsets,metrics,anddetailsaboutthetoolsused
systemsare duringTEVVaredocumented.
evaluatedfor
MEASURE 2.2: Evaluations involving human subjects meet ap-
trustworthy
plicable requirements (including human subject protection) and
characteristics.
arerepresentativeoftherelevantpopulation.
MEASURE 2.3: AI system performance or assurance criteria
are measured qualitatively or quantitatively and demonstrated
for conditions similar to deployment setting(s). Measures are
documented.
MEASURE 2.4: The functionality and behavior of the AI sys-
temanditscomponents–asidentifiedinthe MAP function–are
monitoredwheninproduction.
MEASURE 2.5: The AI system to be deployed is demonstrated
to be valid and reliable. Limitations of the generalizability be-
yond the conditions under which the technology was developed
aredocumented.
Continuedonnextpage
Page29NISTAI100-1 AIRMF1.0
Table3: Categoriesandsubcategoriesforthe MEASURE function. (Continued)
Categories Subcategories
MEASURE 2.6: The AI system is evaluated regularly for safety
risks–asidentifiedintheMAPfunction. TheAIsystemtobede-
ployed is demonstrated to be safe, its residual negative risk does
notexceedtherisktolerance,anditcanfailsafely,particularlyif
made to operate beyond its knowledge limits. Safety metrics re-
flect system reliability and robustness, real-time monitoring, and
responsetimesforAIsystemfailures.
MEASURE 2.7: AI system security and resilience – as identified
inthe MAP function–areevaluatedanddocumented.
MEASURE 2.8: Risks associatedwith transparency andaccount-
ability – as identified in the MAP function – are examined and
documented.
MEASURE 2.9: The AI model is explained, validated, and docu-
mented, and AI system output is interpreted within its context –
asidentifiedinthe MAP function–toinformresponsibleuseand
governance.
MEASURE 2.10: Privacy risk of the AI system – as identified in
the MAP function–isexaminedanddocumented.
MEASURE 2.11: Fairness and bias – as identified in the MAP
function–areevaluatedandresultsaredocumented.
MEASURE 2.12: Environmental impact and sustainability of AI
model training and management activities – as identified in the
MAP function–areassessedanddocumented.
MEASURE 2.13: Effectiveness of the employed TEVV met-
rics and processes in the MEASURE function are evaluated and
documented.
MEASURE 3: MEASURE 3.1: Approaches, personnel, and documentation are
Mechanismsfor in place to regularly identify and track existing, unanticipated,
trackingidentified and emergent AI risks based on factors such as intended and ac-
AIrisksovertime tualperformanceindeployedcontexts.
areinplace.
MEASURE 3.2: Risk tracking approaches are considered for
settings where AI risks are difficult to assess using currently
available measurement techniques or where metrics are not yet
available.
Continuedonnextpage
Page30NISTAI100-1 AIRMF1.0
Table3: Categoriesandsubcategoriesforthe MEASURE function. (Continued)
Categories Subcategories
MEASURE 3.3: Feedback processes for end users and impacted
communitiestoreportproblemsandappealsystemoutcomesare
establishedandintegratedintoAIsystemevaluationmetrics.
MEASURE 4: MEASURE4.1: MeasurementapproachesforidentifyingAIrisks
Feedbackabout are connected to deployment context(s) and informed through
efficacyof consultation with domain experts and other end users. Ap-
measurementis proachesaredocumented.
gatheredand
MEASURE 4.2: Measurement results regarding AI system trust-
assessed.
worthiness in deployment context(s) and across the AI lifecycle
are informed by input from domain experts and relevant AI ac-
tors to validate whether the system is performing consistently as
intended. Resultsaredocumented.
MEASURE 4.3: Measurable performance improvements or de-
clines based on consultations with relevant AI actors, in-
cluding affected communities, and field data about context-
relevant risks and trustworthiness characteristics are identified
anddocumented.
5.4 Manage
The MANAGE function entails allocating risk resources to mapped and measured risks on
aregularbasisandasdefinedbythe GOVERN function. Risktreatmentcomprisesplansto
respondto,recoverfrom,andcommunicateaboutincidentsorevents.
ContextualinformationgleanedfromexpertconsultationandinputfromrelevantAIactors
– established in GOVERN and carried out in MAP – is utilized in this function to decrease
thelikelihoodofsystemfailuresandnegativeimpacts. Systematicdocumentationpractices
established in GOVERN and utilized in MAP and MEASURE bolster AI risk management
effortsandincreasetransparencyandaccountability. Processesforassessingemergentrisks
areinplace,alongwithmechanismsforcontinualimprovement.
After completing the MANAGE function, plans for prioritizing risk and regular monitoring
and improvement will be in place. Framework users will have enhanced capacity to man-
age the risks of deployed AI systems and to allocate risk management resources based on
assessed and prioritized risks. It is incumbent on Framework users to continue to apply
the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or
expectationsfromrelevantAIactorsevolveovertime.
Page31NISTAI100-1 AIRMF1.0
PracticesrelatedtomanagingAIrisksaredescribedintheNISTAIRMFPlaybook. Table
4liststhe MANAGE function’scategoriesandsubcategories.
Table4: Categoriesandsubcategoriesforthe MANAGE function.
Categories Subcategories
MANAGE 1: AI MANAGE 1.1: A determination is made as to whether the AI
risksbasedon system achieves its intended purposes and stated objectives and
assessmentsand whetheritsdevelopmentordeploymentshouldproceed.
otheranalytical
MANAGE 1.2: Treatment of documented AI risks is prioritized
outputfromthe
basedonimpact,likelihood,andavailableresourcesormethods.
MAP and MEASURE
MANAGE1.3: ResponsestotheAIrisksdeemedhighpriority,as
functionsare
identifiedbythe MAP function,aredeveloped,planned,anddoc-
prioritized,
umented. Riskresponseoptionscanincludemitigating,transfer-
respondedto,and
ring,avoiding,oraccepting.
managed.
MANAGE 1.4: Negative residual risks (defined as the sum of all
unmitigated risks) to both downstream acquirers of AI systems
andendusersaredocumented.
MANAGE 2: MANAGE 2.1: Resources required to manage AI risks are taken
Strategiesto into account – along with viable non-AI alternative systems, ap-
maximizeAI proaches, or methods – to reduce the magnitude or likelihood of
benefitsand potentialimpacts.
minimizenegative
MANAGE 2.2: Mechanisms are in place and applied to sustain
impactsareplanned,
thevalueofdeployedAIsystems.
prepared,
MANAGE2.3: Proceduresarefollowedtorespondtoandrecover
implemented,
fromapreviouslyunknownriskwhenitisidentified.
documented,and
informedbyinput MANAGE 2.4: Mechanismsareinplaceandapplied,andrespon-
fromrelevantAI sibilitiesareassignedandunderstood,tosupersede,disengage,or
actors. deactivateAIsystemsthatdemonstrateperformanceoroutcomes
inconsistentwithintendeduse.
MANAGE 3: AI MANAGE 3.1: AI risks and benefits from third-party resources
risksandbenefits are regularly monitored, and risk controls are applied and
fromthird-party documented.
entitiesare
MANAGE 3.2: Pre-trained models which are used for develop-
managed.
ment are monitored as part of AI system regular monitoring and
maintenance.
Continuedonnextpage
Page32NISTAI100-1 AIRMF1.0
Table4: Categoriesandsubcategoriesforthe MANAGE function. (Continued)
Categories Subcategories
MANAGE 4: Risk MANAGE 4.1: Post-deployment AI system monitoring plans
treatments, are implemented, including mechanisms for capturing and eval-
includingresponse uating input from users and other relevant AI actors, appeal
andrecovery,and andoverride,decommissioning,incidentresponse,recovery,and
communication changemanagement.
plansforthe
MANAGE 4.2: Measurableactivitiesforcontinualimprovements
identifiedand
areintegratedintoAIsystemupdatesandincluderegularengage-
measuredAIrisks
mentwithinterestedparties,includingrelevantAIactors.
aredocumentedand
MANAGE4.3: Incidentsanderrorsarecommunicatedtorelevant
monitoredregularly.
AI actors, including affected communities. Processes for track-
ing, responding to, and recovering from incidents and errors are
followedanddocumented.
6. AI RMF Profiles
AI RMF use-case profiles are implementations of the AI RMF functions, categories, and
subcategoriesforaspecificsettingorapplicationbasedontherequirements,risktolerance,
and resources of the Framework user: for example, an AI RMF hiring profile or an AI
RMF fair housing profile. Profiles may illustrate and offer insights into how risk can be
managed at various stages of the AI lifecycle or in specific sector, technology, or end-use
applications. AIRMFprofilesassistorganizationsindecidinghowtheymightbestmanage
AI risk that is well-aligned with their goals, considers legal/regulatory requirements and
bestpractices,andreflectsriskmanagementpriorities.
AI RMF temporal profiles are descriptions of either the current state or the desired, target
stateofspecificAIriskmanagementactivitieswithinagivensector,industry,organization,
or application context. An AI RMF Current Profile indicates how AI is currently being
managed and the related risks in terms of current outcomes. A Target Profile indicates the
outcomesneededtoachievethedesiredortargetAIriskmanagementgoals.
Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk
management objectives. Action plans can be developed to address these gaps to fulfill
outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by
the user’s needs and risk management processes. This risk-based approach also enables
Framework users to compare their approaches with other approaches and to gauge the
resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-
effective,prioritizedmanner.
Page33NISTAI100-1 AIRMF1.0
AIRMFcross-sectoralprofilescoverrisksofmodelsorapplicationsthatcanbeusedacross
use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure,
and manage risks for activities or business processes common across sectors such as the
useoflargelanguagemodels,cloud-basedservicesoracquisition.
ThisFrameworkdoesnotprescribeprofiletemplates,allowingforflexibilityinimplemen-
tation.
Page34NISTAI100-1 AIRMF1.0
Appendix A:
Descriptions of AI Actor Tasks from Figures 2 and 3
AI Design tasks are performed during the Application Context and Data and Input phases
of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI
systems and are responsible for the planning, design, and data collection and processing
tasksoftheAIsystemsothattheAIsystemislawfulandfit-for-purpose. Tasksincludear-
ticulating and documenting the system’s concept and objectives, underlying assumptions,
context, and requirements; gathering and cleaning data; and documenting the metadata
and characteristics of the dataset. AI actors in this category include data scientists, do-
main experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion,
and accessibility, members of impacted communities, human factors experts (e.g., UX/UI
design), governance experts, data engineers, data providers, system funders, product man-
agers,third-partyentities,evaluators,andlegalandprivacygovernance.
AIDevelopmenttasksareperformedduringtheAIModelphaseofthelifecycleinFigure
2. AIDevelopmentactorsprovidetheinitialinfrastructureofAIsystemsandareresponsi-
ble for model building and interpretation tasks, which involve the creation, selection, cali-
bration, training, and/or testing of models or algorithms. AI actors in this category include
machinelearning experts,data scientists, developers,third-party entities,legal andprivacy
governanceexperts,andexpertsinthesocio-culturalandcontextualfactorsassociatedwith
thedeploymentsetting.
AI Deployment tasks are performed during the Task and Output phase of the lifecycle in
Figure 2. AI Deployment actors are responsible for contextual decisions relating to how
the AI system is used to assure deployment of the system into production. Related tasks
include piloting the system, checking compatibility with legacy systems, ensuring regu-
latory compliance, managing organizational change, and evaluating user experience. AI
actors in this category include system integrators, software developers, end users, oper-
ators and practitioners, evaluators, and domain experts with expertise in human factors,
socio-culturalanalysis,andgovernance.
Operation and Monitoring tasks are performed in the Application Context/Operate and
MonitorphaseofthelifecycleinFigure2. ThesetasksarecarriedoutbyAIactorswhoare
responsibleforoperatingtheAIsystemandworkingwithotherstoregularlyassesssystem
outputandimpacts. AIactorsinthiscategoryincludesystemoperators,domainexperts,AI
designers,userswhointerpretorincorporatetheoutputofAIsystems,productdevelopers,
evaluators and auditors, compliance experts, organizational management, and members of
theresearchcommunity.
Test, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout
the AI lifecycle. They are carried out by AI actors who examine the AI system or its
components, or detect and remediate problems. Ideally, AI actors carrying out verification
Page35NISTAI100-1 AIRMF1.0
andvalidationtasksaredistinctfromthosewhoperformtestandevaluationactions. Tasks
can be incorporated into a phase as early as design, where tests are planned in accordance
withthedesignrequirement.
• TEVV tasks for design, planning, and data may center on internal and external vali-
dation of assumptions for system design, data collection, and measurements relative
totheintendedcontextofdeploymentorapplication.
• TEVV tasks for development (i.e., model building) include model validation and
assessment.
• TEVVtasksfordeploymentincludesystemvalidationandintegrationinproduction,
with testing, and recalibration for systems and process integration, user experience,
andcompliancewithexistinglegal,regulatory,andethicalspecifications.
• TEVVtasksforoperationsinvolveongoingmonitoringforperiodicupdates,testing,
and subject matter expert (SME) recalibration of models, the tracking of incidents
or errors reported and their management, the detection of emergent properties and
relatedimpacts,andprocessesforredressandresponse.
Human Factors tasks and activities are found throughout the dimensions of the AI life-
cycle. They include human-centered design practices and methodologies, promoting the
active involvement of end users and other interested parties and relevant AI actors, incor-
porating context-specific norms and values in system design, evaluating and adapting end
userexperiences,andbroadintegrationofhumansandhumandynamicsinallphasesofthe
AIlifecycle. Humanfactorsprofessionalsprovidemultidisciplinaryskillsandperspectives
to understand context of use, inform interdisciplinary and demographic diversity, engage
in consultative processes, design and evaluate user experience, perform human-centered
evaluationandtesting,andinformimpactassessments.
Domain Expert tasks involve input from multidisciplinary practitioners or scholars who
provide knowledge or expertise in – and about – an industry sector, economic sector, con-
text, or application area where an AI system is being used. AI actors who are domain
experts can provide essential guidance for AI system design and development, and inter-
pretoutputsinsupportofworkperformedbyTEVVandAIimpactassessmentteams.
AIImpactAssessmenttasksincludeassessingandevaluatingrequirementsforAIsystem
accountability, combating harmful bias, examining impacts of AI systems, product safety,
liability, and security, among others. AI actors such as impact assessors and evaluators
providetechnical,humanfactor,socio-cultural,andlegalexpertise.
ProcurementtasksareconductedbyAIactorswithfinancial,legal,orpolicymanagement
authority for acquisition of AI models, products, or services from a third-party developer,
vendor,orcontractor.
Governance and Oversight tasks are assumed by AI actors with management, fiduciary,
and legal authority and responsibility for the organization in which an AI system is de-
Page36NISTAI100-1 AIRMF1.0
signed, developed, and/or deployed. Key AI actors responsible for AI governance include
organizational management, senior leadership, and the Board of Directors. These actors
are parties that are concerned with the impact and sustainability of the organization as a
whole.
AdditionalAIActors
Third-party entities include providers, developers, vendors, and evaluators of data, al-
gorithms, models, and/or systems and related services for another organization or the or-
ganization’s customers or clients. Third-party entities are responsible for AI design and
developmenttasks,inwholeorinpart. Bydefinition,theyareexternaltothedesign,devel-
opment, or deployment team of the organization that acquires its technologies or services.
The technologies acquired from third-party entities may be complex or opaque, and risk
tolerancesmaynotalignwiththedeployingoroperatingorganization.
End users of an AI system are the individuals or groups that use the system for specific
purposes. TheseindividualsorgroupsinteractwithanAIsysteminaspecificcontext. End
userscanrangeincompetencyfromAIexpertstofirst-timetechnologyendusers.
Affected individuals/communities encompass all individuals, groups, communities, or
organizationsdirectlyorindirectlyaffectedbyAIsystemsordecisionsbasedontheoutput
of AI systems. These individuals do not necessarily interact with the deployed system or
application.
Other AI actors may provide formal or quasi-formal norms or guidance for specifying
and managing AI risks. They can include trade associations, standards developing or-
ganizations, advocacy groups, researchers, environmental groups, and civil society
organizations.
The general public is most likely to directly experience positive and negative impacts of
AItechnologies. TheymayprovidethemotivationforactionstakenbytheAIactors. This
group can include individuals, communities, and consumers associated with the context in
whichanAIsystemisdevelopedordeployed.
Page37NISTAI100-1 AIRMF1.0
Appendix B:
How AI Risks Differ from Traditional Software Risks
As with traditional software, risks from AI-based technology can be bigger than an en-
terprise, span organizations, and lead to societal impacts. AI systems also bring a set of
risks that are not comprehensively addressed by current risk frameworks and approaches.
SomeAIsystemfeaturesthatpresentrisksalsocanbebeneficial. Forexample,pre-trained
models and transfer learning can advance research and increase accuracy and resilience
whencomparedtoothermodelsandapproaches. Identifyingcontextualfactorsinthe MAP
function will assist AI actors in determining the level of risk and potential management
efforts.
Compared to traditional software, AI-specific risks that are new or increased include the
following:
• ThedatausedforbuildinganAIsystemmaynotbeatrueorappropriaterepresenta-
tion ofthe context or intendeduse of the AIsystem, and the ground truthmay either
notexist ornotbe available. Additionally,harmfulbias andotherdata qualityissues
canaffectAIsystemtrustworthiness,whichcouldleadtonegativeimpacts.
• AI system dependency and reliance on data for training tasks, combined with in-
creasedvolumeandcomplexitytypicallyassociatedwithsuchdata.
• IntentionalorunintentionalchangesduringtrainingmayfundamentallyalterAIsys-
temperformance.
• Datasets used to train AI systems may become detached from their original and in-
tendedcontextormaybecomestaleoroutdatedrelativetodeploymentcontext.
• AI system scale and complexity (many systems contain billions or even trillions of
decisionpoints)housedwithinmoretraditionalsoftwareapplications.
• Use of pre-trained models that can advance research and improve performance can
alsoincreaselevelsofstatisticaluncertaintyandcauseissueswithbiasmanagement,
scientificvalidity,andreproducibility.
• Higher degree of difficulty in predicting failure modes for emergent properties of
large-scalepre-trainedmodels.
• PrivacyriskduetoenhanceddataaggregationcapabilityforAIsystems.
• AI systems may require more frequent maintenance and triggers for conducting cor-
rectivemaintenanceduetodata,model,orconceptdrift.
• Increasedopacityandconcernsaboutreproducibility.
• UnderdevelopedsoftwaretestingstandardsandinabilitytodocumentAI-basedprac-
tices to the standard expected of traditionally engineered software for all but the
simplestofcases.
• Difficulty in performing regular AI-based software testing, or determining what to
test, since AI systems are not subject to the same controls as traditional code devel-
opment.
Page38NISTAI100-1 AIRMF1.0
• ComputationalcostsfordevelopingAIsystemsandtheirimpactontheenvironment
andplanet.
• Inability to predict or detect the side effects of AI-based systems beyond statistical
measures.
Privacy and cybersecurity risk management considerations and approaches are applicable
in the design, development, deployment, evaluation, and use of AI systems. Privacy and
cybersecurity risks are also considered as part of broader enterprise risk management con-
siderations,whichmayincorporateAIrisks. AspartoftheefforttoaddressAItrustworthi-
ness characteristics such as “Secure and Resilient” and “Privacy-Enhanced,” organizations
may consider leveraging available standards and guidance that provide broad guidance to
organizationstoreducesecurityandprivacyrisks,suchas,butnotlimitedto,theNISTCy-
bersecurityFramework,theNISTPrivacyFramework,theNISTRiskManagementFrame-
work, and the Secure Software Development Framework. These frameworks have some
features in common with the AI RMF. Like most risk management approaches, they are
outcome-based rather than prescriptive and are often structured around a Core set of func-
tions, categories, and subcategories. While there are significant differences between these
frameworks based on the domain addressed – and because AI risk management calls for
addressingmanyothertypesofrisks–frameworkslikethosementionedabovemayinform
securityandprivacyconsiderationsinthe MAP, MEASURE,and MANAGE functionsofthe
AIRMF.
At the same time, guidance available before publication of this AI RMF does not compre-
hensively address many AI system risks. For example, existing frameworks and guidance
areunableto:
• adequatelymanagetheproblemofharmfulbiasinAIsystems;
• confrontthechallengingrisksrelatedtogenerativeAI;
• comprehensivelyaddresssecurityconcernsrelatedtoevasion,modelextraction,mem-
bershipinference,availability,orothermachinelearningattacks;
• accountforthecomplexattacksurfaceofAIsystemsorothersecurityabusesenabled
byAIsystems;and
• considerrisksassociatedwiththird-partyAItechnologies,transferlearning,andoff-
labelusewhereAIsystemsmaybetrainedfordecision-makingoutsideanorganiza-
tion’ssecuritycontrolsortrainedinonedomainandthen“fine-tuned”foranother.
Both AI and traditional software technologies and systems are subject to rapid innovation.
Technology advances should be monitored and deployed to take advantage of those devel-
opmentsandworktowardsafutureofAIthatisbothtrustworthyandresponsible.
Page39NISTAI100-1 AIRMF1.0
Appendix C:
AI Risk Management and Human-AI Interaction
Organizations that design, develop, or deploy AI systems for use in operational settings
may enhance their AI risk management by understanding current limitations of human-
AI interaction. The AI RMF provides opportunities to clearly define and differentiate the
various human roles and responsibilities when using, interacting with, or managing AI
systems.
Manyofthedata-drivenapproachesthatAIsystemsrelyonattempttoconvertorrepresent
individual and social observational and decision-making practices into measurable quanti-
ties. Representing complex human phenomena with mathematical models can come at the
cost of removing necessary context. This loss of context may in turn make it difficult to
understandindividualandsocietalimpactsthatarekeytoAIriskmanagementefforts.
Issuesthatmeritfurtherconsiderationandresearchinclude:
1. HumanrolesandresponsibilitiesindecisionmakingandoverseeingAIsystems
need to be clearly defined and differentiated. Human-AI configurations can span
from fully autonomous to fully manual. AI systems can autonomously make deci-
sions, defer decision making to a human expert, or be used by a human decision
maker as an additional opinion. Some AI systems may not require human oversight,
such as models used to improve video compression. Other systems may specifically
requirehumanoversight.
2. Decisionsthatgointothedesign,development,deployment,evaluation,anduse
of AI systems reflect systemic and human cognitive biases. AI actors bring their
cognitive biases, both individual and group, into the process. Biases can stem from
end-user decision-making tasks and be introduced across the AI lifecycle via human
assumptions, expectations, and decisions during design and modeling tasks. These
biases, which are not necessarily always harmful, may be exacerbated by AI system
opacity and the resulting lack of transparency. Systemic biases at the organizational
level can influence how teams are structured and who controls the decision-making
processes throughout the AI lifecycle. These biases can also influence downstream
decisionsbyendusers,decisionmakers,andpolicymakersandmayleadtonegative
impacts.
3. Human-AI interaction results vary. Under certain conditions – for example, in
perceptual-based judgment tasks – the AI part of the human-AI interaction can am-
plify human biases, leading to more biased decisions than the AI or human alone.
When these variations are judiciously taken into account in organizing human-AI
teams, however, they can result in complementarity and improved overall perfor-
mance.
Page40NISTAI100-1 AIRMF1.0
4. Presenting AI system information to humans is complex. Humans perceive and
derivemeaningfromAIsystemoutputandexplanationsindifferentways,reflecting
differentindividualpreferences,traits,andskills.
The GOVERN function provides organizations with the opportunity to clarify and define
the roles and responsibilities for the humans in the Human-AI team configurations and
those who are overseeing the AI system performance. The GOVERN function also creates
mechanisms for organizations to make their decision-making processes more explicit, to
helpcountersystemicbiases.
The MAP function suggests opportunities to define and document processes for operator
andpractitionerproficiencywithAIsystemperformanceandtrustworthinessconcepts,and
to define relevant technical standards and certifications. Implementing MAP function cat-
egories and subcategories may help organizations improve their internal competency for
analyzingcontext,identifyingproceduralandsystemlimitations,exploringandexamining
impacts of AI-based systems in the real world, and evaluating decision-making processes
throughouttheAIlifecycle.
The GOVERN and MAP functionsdescribetheimportanceofinterdisciplinarityanddemo-
graphicallydiverseteamsandutilizingfeedbackfrompotentiallyimpactedindividualsand
communities. AI actors called out in the AI RMF who perform human factors tasks and
activities can assist technical teams by anchoring in design and development practices to
userintentionsandrepresentativesofthebroaderAIcommunity,andsocietalvalues. These
actors further help to incorporate context-specific norms and values in system design and
evaluateenduserexperiences–inconjunctionwithAIsystems.
AI risk management approaches for human-AI configurations will be augmented by on-
going research and evaluation. For example, the degree to which humans are empowered
andincentivizedtochallengeAIsystemoutputrequiresfurtherstudies. Dataaboutthefre-
quency and rationale with which humans overrule AI system output in deployed systems
maybeusefultocollectandanalyze.
Page41NISTAI100-1 AIRMF1.0
Appendix D:
Attributes of the AI RMF
NIST described several key attributes of the AI RMF when work on the Framework first
began. These attributes have remained intact and were used to guide the AI RMF’s devel-
opment. Theyareprovidedhereasareference.
TheAIRMFstrivesto:
1. Berisk-based,resource-efficient,pro-innovation,andvoluntary.
2. Be consensus-driven and developed and regularly updated through an open, trans-
parent process. All stakeholders should have the opportunity to contribute to the AI
RMF’sdevelopment.
3. Use clear and plain language that is understandable by a broad audience, including
senior executives, government officials, non-governmental organization leadership,
and those who are not AI professionals – while still of sufficient technical depth to
be useful to practitioners. The AI RMF should allow for communication of AI risks
across an organization, between organizations, with customers, and to the public at
large.
4. Provide common language and understanding to manage AI risks. The AI RMF
should offer taxonomy, terminology, definitions, metrics, and characterizations for
AIrisk.
5. Be easily usable and fit well with other aspects of risk management. Use of the
Framework should be intuitive and readily adaptable as part of an organization’s
broader risk management strategy and processes. It should be consistent or aligned
withotherapproachestomanagingAIrisks.
6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI
RMF should be universally applicable to any AI technology and to context-specific
usecases.
7. Beoutcome-focusedandnon-prescriptive. TheFrameworkshouldprovidea catalog
ofoutcomesandapproachesratherthanprescribeone-size-fits-allrequirements.
8. Takeadvantageofandfostergreaterawarenessofexistingstandards,guidelines,best
practices, methodologies, and tools for managing AI risks – as well as illustrate the
needforadditional,improvedresources.
9. Be law- and regulation-agnostic. The Framework should support organizations’
abilities to operate under applicable domestic and international legal or regulatory
regimes.
10. Bealivingdocument. TheAIRMFshouldbereadilyupdatedastechnology,under-
standing, and approaches to AI trustworthiness and uses of AI change and as stake-
holders learn from implementing AI risk management generally and this framework
inparticular.
Page42Thispublicationisavailablefreeofchargefrom:
https://doi.org/10.6028/NIST.AI.100-1