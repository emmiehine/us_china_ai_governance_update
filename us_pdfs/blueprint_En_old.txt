BLUEPRINT FOR AN
AI B
ILL OF
R
IGHTS
MAKING AUTOMATED
SYSTEMS WORK FOR
THE AMERICAN PEOPLE
OCTOBER 2022About this Document
The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was
published by the White House Office of Science and Technology Policy in October 2022. This framework was
released one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered
world.” Its release follows a year of public engagement to inform this initiative. The framework is available
online at: https://www.whitehouse.gov/ostp/ai-bill-of-rights
About the Office of Science and Technology Policy
The Office of Science and Technology Policy (OSTP) was established by the National Science and Technology
Policy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office
of the President with advice on the scientific, engineering, and technological aspects of the economy, national
security, health, foreign relations, the environment, and the technological recovery and use of resources, among
other topics. OSTP leads interagency science and technology policy coordination efforts, assists the Office of
Management and Budget (OMB) with an annual review and analysis of Federal research and development in
budgets, and serves as a source of scientific and technological analysis and judgment for the President with
respect to major policies, plans, and programs of the Federal Government.
Legal Disclaimer
The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper
published by the White House Office of Science and Technology Policy. It is intended to support the
development of policies and practices that protect civil rights and promote democratic values in the building,
deployment, and governance of automated systems.
The Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy. It
does not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or
international instrument. It does not constitute binding guidance for the public or Federal agencies and
therefore does not require compliance with the principles described herein. It also is not determinative of what
the U.S. government’s position will be in any international negotiation. Adoption of these principles may not
meet the requirements of existing statutes, regulations, policies, or international instruments, or the
requirements of the Federal agencies that enforce them. These principles are not intended to, and do not,
prohibit or limit any lawful activity of a government agency, including law enforcement, national security, or
intelligence activities.
The appropriate application of the principles set forth in this white paper depends significantly on the
context in which automated systems are being utilized. In some circumstances, application of these principles
in whole or in part may not be appropriate given the intended use of automated systems to achieve government
agency missions. Future sector-specific guidance will likely be necessary and important for guiding the use of
automated systems in certain settings such as AI systems used as part of school building security or automated
health diagnostic systems.
The Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of
equities, for example, between the protection of sensitive law enforcement information and the principle of
notice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and
other law enforcement equities. Even in contexts where these principles may not apply in whole or in part,
federal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well as
existing policies and safeguards that govern automated systems, including, for example, Executive Order 13960,
Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020).
This white paper recognizes that national security (which includes certain law enforcement and
homeland security activities) and defense activities are of increased sensitivity and interest to our nation’s
adversaries and are often subject to special requirements, such as those governing classified information and
other protected data. Such activities require alternative, compatible safeguards through existing policies that
govern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and
Responsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and
Framework. The implementation of these policies to national security and defense activities can be informed by
the Blueprint for an AI Bill of Rights where feasible.
The Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or
defense, substantive or procedural, enforceable at law or in equity by any party against the United States, its
departments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a
waiver of sovereign immunity.
Copyright Information
This document is a work of the United States Government and is in the public domain (see 17 U.S.C. §105).
2SECTION TITLE
F
OREWORD
Among the great challenges posed to democracy today is the use of technology, data, and automated systems in
ways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities and
prevent our access to critical resources or services. These problems are well documented. In America and around
the world, systems supposed to help with patient care have proven unsafe, ineffective, or biased. Algorithms used
in hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed
new harmful bias and discrimination. Unchecked social media data collection has been used to threaten people’s
opportunities, undermine their privacy, or pervasively track their activity—often without their knowledge or
consent.
These outcomes are deeply harmful—but they are not inevitable. Automated systems have brought about extraor-
dinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm
paths, to algorithms that can identify diseases in patients. These tools now drive important decisions across
sectors, while data is helping to revolutionize global industries. Fueled by the power of American innovation,
these tools hold the potential to redefine every part of our society and make life better for everyone.
This important progress must not come at the price of civil rights or democratic values, foundational American
principles that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the
President ordered the full Federal government to work to root out inequity, embed fairness in decision-
making processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.1 The
President has spoken forcefully about the urgent challenges posed to democracy today and has regularly called
on people of conscience to act to preserve civil rights—including the right to privacy, which he has called “the
basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this
country.”2
To advance President Biden’s vision, the White House Office of Science and Technology Policy has identified
five principles that should guide the design, use, and deployment of automated systems to protect the American
public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that
protects all people from these threats—and uses technologies in ways that reinforce our highest values.
Responding to the experiences of the American public, and informed by insights from researchers,
technologists, advocates, journalists, and policymakers, this framework is accompanied by a technical
companion—a handbook for anyone seeking to incorporate these protections into policy and practice, including
detailed steps toward actualizing these principles in the technological design process. These principles help
provide guidance whenever automated systems can meaningfully impact the public’s rights, opportunities,
or access to critical needs.
3A F
BOUT THIS RAMEWORK
The Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the
design, use, and deployment of automated systems to protect the rights of the American public in the age of
artificial intel-ligence. Developed through extensive consultation with the American public, these principles are
a blueprint for building and deploying automated systems that are aligned with democratic values and protect
civil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five
principles, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives
concrete steps that can be taken by many kinds of organizations—from governments at all levels to companies of
all sizes—to uphold these values. Experts from across the private sector, governments, and international
consortia have published principles and frameworks to guide the responsible use of automated systems; this
framework provides a national values statement and toolkit that is sector-agnostic to inform building these
protections into policy, practice, or the technological design process. Where existing law or policy—such as
sector-specific privacy laws and oversight requirements—do not already provide guidance, the Blueprint for an
AI Bill of Rights should be used to inform policy decisions.
L A P
ISTENING TO THE MERICAN UBLIC
The White House Office of Science and Technology Policy has led a year-long process to seek and distill input
from people across the country—from impacted communities and industry stakeholders to technology develop-
ers and other experts across fields and sectors, as well as policymakers throughout the Federal government—on
the issue of algorithmic and data-driven harms and potential remedies. Through panel discussions, public listen-
ing sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicized
email address, people throughout the United States, public servants across Federal agencies, and members of the
international community spoke up about both the promises and potential harms of these technologies, and
played a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these
discussions include that AI has transformative potential to improve Americans’ lives, and that preventing the
harms of these technologies is both necessary and achievable. The Appendix includes a full list of public engage-
ments.
4SECTION TITLE
B AI B R
LUEPRINT FOR AN ILL OF IGHTS
S E S
AFE AND FFECTIVE YSTEMS
You should be protected from unsafe or ineffective systems. Automated systems should be
developed with consultation from diverse communities, stakeholders, and domain experts to identify
concerns, risks, and potential impacts of the system. Systems should undergo pre-deployment testing, risk
identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on
their intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence to
domain-specific standards. Outcomes of these protective measures should include the possibility of not
deploying the system or removing a system from use. Automated systems should not be designed with an intent
or reasonably foreseeable possibility of endangering your safety or the safety of your community. They should
be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or
impacts of automated systems. You should be protected from inappropriate or irrelevant data use in the
design, development, and deployment of automated systems, and from the compounded harm of its reuse.
Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of
steps taken to mitigate potential harms, should be performed and the results made public whenever possible.
A D P
LGORITHMIC ISCRIMINATION ROTECTIONS
You should not face discrimination by algorithms and systems should be used and designed in
an equitable way. Algorithmic discrimination occurs when au tomated systems contribute to unjustified
different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including
pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual
orientation), religion, age, national origin, disability, veteran status, genetic information, or any other
classification protected by law. Depending on the specific circumstances, such algorithmic discrimination
may violate legal protections. Designers, developers, and deployers of automated systems should take
proactive and continuous measures to protect individuals and communities from algorithmic
discrimination and to use and design systems in an equitable way. This protection should include proactive
equity assessments as part of the system design, use of representative data and protection against proxies
for demographic features, ensuring accessibility for people with disabilities in design and development,
pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent
evaluation and plain language reporting in the form of an algorithmic impact assessment, including
disparity testing results and mitigation information, should be performed and made public whenever
possible to confirm these protections.
5SECTION TITLE
D P
ATA RIVACY
You should be protected from abusive data practices via built-in protections and you
should have agency over how data about you is used. You should be protected from violations of
privacy through design choices that ensure such protections are included by default, including ensuring that
data collection conforms to reasonable expectations and that only data strictly necessary for the specific
context is collected. Designers, developers, and deployers of automated systems should seek your permission
and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate
ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be
used. Systems should not employ user experience and design decisions that obfuscate user choice or burden
users with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases
where it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable
in plain language, and give you agency over data collection and the specific context of use; current hard-to-
understand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and
restrictions for data and inferences related to sensitive domains, including health, work, education, criminal
justice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data and
related inferences should only be used for necessary functions, and you should be protected by ethical review
and use prohibitions. You and your communities should be free from unchecked surveillance; surveillance
technologies should be subject to heightened oversight that includes at least pre-deployment assessment of their
potential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring
should not be used in education, work, housing, or in other contexts where the use of such surveillance
technologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access to
reporting that confirms your data decisions have been respected and provides an assessment of the
potential impact of surveillance technologies on your rights, opportunities, or access.
N E
OTICE AND XPLANATION
You should know that an automated system is being used and understand how and why it
contributes to outcomes that impact you. Designers, developers, and deployers of automated systems
should provide generally accessible plain language documentation including clear descriptions of the overall
system functioning and the role automation plays, notice that such systems are in use, the individual or organiza-
tion responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice
should be kept up-to-date and people impacted by the system should be notified of significant use case or key
functionality changes. You should know how and why an outcome impacting you was determined by an
automated system, including when the automated system is not the sole input determining the outcome.
Automated systems should provide explanations that are technically valid, meaningful and useful to you and to
any operators or others who need to understand the system, and calibrated to the level of risk based on the
context. Reporting that includes summary information about these automated systems in plain language and
assessments of the clarity and quality of the notice and explanations should be made public whenever possible.
6SECTION TITLE
H A , C , F
UMAN LTERNATIVES ONSIDERATION AND ALLBACK
You should be able to opt out, where appropriate, and have access to a person who can quickly
consider and remedy problems you encounter. You should be able to opt out from automated systems in
favor of a human alternative, where appropriate. Appropriateness should be determined based on reasonable
expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from
especially harmful impacts. In some cases, a human or other alternative may be required by law. You should have
access to timely human consideration and remedy by a fallback and escalation process if an automated system
fails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and
fallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and
should not impose an unreasonable burden on the public. Automated systems with an intended use within sensi-
tive domains, including, but not limited to, criminal justice, employment, education, and health, should additional-
ly be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting
with the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes
a description of these human governance processes and assessment of their timeliness, accessibility, outcomes,
and effectiveness should be made public whenever possible.
Definitions for key terms in The Blueprint for an AI Bill of Rights can be found in Applying the Blueprint for an AI Bill of Rights.
Accompanying analysis and tools for actualizing each principle can be found in the Technical Companion.
7SECTION TITLE
Applying The Blueprint for an AI Bill of Rights
While many of the concerns addressed in this framework derive from the use of AI, the technical
capabilities and specific definitions of such systems change with the speed of innovation, and the potential
harms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a two-
part test to determine what systems are in scope. This framework applies to (1) automated systems that (2)
have the potential to meaningfully impact the American public’s rights, opportunities, or access to
critical resources or services. These rights, opportunities, and access to critical resources of services should
be enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in
our lives.
This framework describes protections that should be applied with respect to all automated systems that
have the potential to meaningfully impact individuals' or communities' exercise of:
R , O , A
IGHTS PPORTUNITIES OR CCESS
Civil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimi-
nation, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both
public and private sector contexts;
Equal opportunities, including equitable access to education, housing, credit, employment, and other
programs; or,
Access to critical resources or services, such as healthcare, financial services, safety, social services,
non-deceptive information about goods and services, and government benefits.
A list of examples of automated systems for which these principles should be considered is provided in the
Appendix. The Technical Companion, which follows, offers supportive guidance for any person or entity that
creates, deploys, or oversees automated systems.
Considered together, the five principles and associated practices of the Blueprint for an AI Bill of
Rights form an overlapping set of backstops against potential harms. This purposefully overlapping
framework, when taken as a whole, forms a blueprint to help protect the public from harm.
The measures taken to realize the vision set forward in this framework should be proportionate
with the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and
access.
R E L P
ELATIONSHIP TO XISTING AW AND OLICY
The Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is
protected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes princi-
ples that can help ensure these protections. Some of these protections are already required by the U.S. Constitu-
tion or implemented under existing U.S. laws. For example, government surveillance, and data search and
seizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for
human review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws
protect the American people against discrimination.
8SECTION TITLE
Applying The Blueprint for an AI Bill of Rights
R E L P
ELATIONSHIP TO XISTING AW AND OLICY
There are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-spe-
cific privacy and security protections. Ensuring some of the additional protections proposed in this framework
would require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions to
the principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law,
conform to the practicalities of a specific use case, or balance competing public interests. In particular, law
enforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties,
and privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in
this framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in
moving principles into practice.
The expectations given in the Technical Companion are meant to serve as a blueprint for the development of
additional technical standards and practices that should be tailored for particular sectors and contexts. While
existing laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail
those laws beyond providing them as examples, where appropriate, of existing protective measures. This
framework instead shares a broad, forward-leaning vision of recommended principles for automated system
development and use to inform private and public involvement with these systems where they have the poten-
tial to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze or
take a position on legislative and regulatory proposals in municipal, state, and federal government, or those in
other countries.
We have seen modest progress in recent years, with some state and local governments responding to these prob-
lems with legislation, and some courts extending longstanding statutory protections to new and emerging tech-
nologies. There are companies working to incorporate additional protections in their design and use of auto-
mated systems, and researchers developing innovative guardrails. Advocates, researchers, and government
organizations have proposed principles for the ethical use of AI and other automated systems. These include
the Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial
Intelligence, which includes principles for responsible stewardship of trustworthy AI and which the United
States adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the
Federal Government, which sets out principles that govern the federal government’s use of AI. The Blueprint
for an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985
on Advancing Racial Equity and Support for Underserved Communities Through the Federal Government.
These principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report
of an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers,
and the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these core
principles for managing information about individuals have been incorporated into data privacy laws and
policies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are
particularly relevant to automated systems, without articulating a specific set of FIPPs or scoping
applicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties,
ethics, or risk management. The Technical Companion builds on this prior work to provide practical next
steps to move these principles into practice and promote common approaches that allow technological
innovation to flourish while protecting people from harm.
9Applying The Blueprint for an AI Bill of Rights
D
EFINITIONS
ALGORITHMIC DISCRIMINATION: “Algorithmic discrimination” occurs when automated systems
contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity,
sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual
orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifica-
tion protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate
legal protections. Throughout this framework the term “algorithmic discrimination” takes this meaning (and
not a technical understanding of discrimination as distinguishing between items).
AUTOMATED SYSTEM: An "automated system" is any system, software, or process that uses computation as
whole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect
data or observations, or otherwise interact with individuals and/or communities. Automated systems
include, but are not limited to, systems derived from machine learning, statistics, or other data processing
or artificial intelligence techniques, and exclude passive computing infrastructure. “Passive computing
infrastructure” is any intermediary technology that does not influence or determine the outcome of decision,
make or aid in decisions, inform policy implementation, or collect data or observations, including web
hosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout this
framework, automated systems that are considered in scope are only those that have the potential to
meaningfully impact individuals’ or communi-ties’ rights, opportunities, or access.
COMMUNITIES: “Communities” include: neighborhoods; social network connections (both online and
offline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organi-
zational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AI
and other data-driven automated systems most directly collect data on, make inferences about, and may cause
harm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of com-
munities. Accordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights.
United States law and policy have long employed approaches for protecting the rights of individuals, but exist-
ing frameworks have sometimes struggled to provide protections when effects manifest most clearly at a com-
munity level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automated
systems should be evaluated, protected against, and redressed at both the individual and community levels.
EQUITY: “Equity” means the consistent and systematic fair, just, and impartial treatment of all individuals.
Systemic, fair, and just treatment must take into account the status of individuals who belong to underserved
communities that have been denied such treatment, such as Black, Latino, and Indigenous and Native American
persons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities;
women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+)
persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely
affected by persistent poverty or inequality.
RIGHTS, OPPORTUNITIES, OR ACCESS: “Rights, opportunities, or access” is used to indicate the scoping
of this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech,
voting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of
privacy and other freedoms in both public and private sector contexts; equal opportunities, including equitable
access to education, housing, credit, employment, and other programs; or, access to critical resources or
services, such as healthcare, financial services, safety, social services, non-deceptive information about goods
and services, and government benefits.
10Applying The Blueprint for an AI Bill of Rights
SENSITIVE DATA: Data and metadata are sensitive if they pertain to an individual in a sensitive domain
(defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a
sensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric
data, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship
history and legal status such as custody and divorce information, and home, work, or school environmental
data); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful
harm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about
those who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes,
but is not limited to, numerical, text, image, audio, or video data.
SENSITIVE DOMAINS: “Sensitive domains” are those in which activities being conducted can cause material
harms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liber-
ties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections
or where such enhanced protections are reasonably expected by the public include, but are not limited to,
health, family planning and care, employment, education, criminal justice, and personal finance. In the context
of this framework, such domains are considered sensitive whether or not the specifics of a system context
would necessitate coverage under existing law, and domains and data that are considered sensitive are under-
stood to change over time based on societal norms and context.
SURVEILLANCE TECHNOLOGY: “Surveillance technology” refers to products or services marketed for
or that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/or
retain data, identifying information, or communications concerning individuals or groups. This framework
limits its focus to both government and commercial use of surveillance technologies when juxtaposed with
real-time or subsequent automated analysis and when such systems have a potential for meaningful impact
on individuals’ or communities’ rights, opportunities, or access.
UNDERSERVED COMMUNITIES: The term “underserved communities” refers to communities that have
been systematically denied a full opportunity to participate in aspects of economic, social, and civic life, as
exemplified by the list in the preceding definition of “equity.”
11FROM
PRINCIPLES
TO PRACTICE
A T C
ECHINCAL OMPANION TO
Blueprint for an
THE
AI B R
ILL OF IGHTS
12T C
ABLE OF ONTENTS
F ROM P RINCIPLES TO P RACTICE: A T ECHNICAL C OMPANION TO THE B LUEPRINT 12
AI B R
FORAN ILLOF IGHTS
U SING T HIS T ECHNICAL C OMPANION 14
S AFE AND E FFECTIVE S YSTEMS 15
A LGORITHMIC D ISCRIMINATION P ROTECTIONS 23
D ATA P RIVACY 30
N OTICE AND E XPLANATION 40
H UMAN A LTERNATIVES, C ONSIDERATION, AND F ALLBACK 46
A PPENDIX 53
E XAMPLES OF A UTOMATED S YSTEMS 53
L ISTENING TO THE A MERICAN P EOPLE 55
E NDNOTES 63
13U
SING THIS TECHNICAL COMPANION
The Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design,
use, and deployment of automated systems to protect the rights of the American public in the age of artificial
intelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights and
provides examples and concrete steps for communities, industry, governments, and others to take in order to
build these protections into policy, practice, or the technological design process.
Taken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help
guard the American public against many of the potential and actual harms identified by researchers, technolo-
gists, advocates, journalists, policymakers, and communities in the United States and around the world. This
technical companion is intended to be used as a reference by people across many circumstances – anyone
impacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to
govern the use of an automated system.
Each principle is accompanied by three supplemental sections:
1
W IMPORTANT:
HY THIS PRINCIPLE IS
This section provides a brief summary of the problems that the principle seeks to address and protect against, including
illustrative examples.
2
W SYSTEMS:
HAT SHOULD BE EXPECTED OF AUTOMATED
• The expectations for automated systems are meant to serve as a blueprint for the development of additional technical
standards and practices that should be tailored for particular sectors and contexts.
• This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. The
expectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoing
monitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offer
concrete directions for how those changes can be made.
• Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can
be provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should
be made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, law
enforcement, or national security considerations may prevent public release. Where public reports are not possible, the
information should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard-
ing individuals’ rights. These reporting expectations are important for transparency, so the American people can have
confidence that their rights, opportunities, and access as well as their expectations about technologies are respected.
3
H PRACTICE:
OW THESE PRINCIPLES CAN MOVE INTO
This section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices.
It describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access.
The examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help
provide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing these
processes require the cooperation of and collaboration among industry, civil society, researchers, policymakers,
technologists, and the public.
14SAFE AND EFFECTIVE SYSTEMS
You should be protected from unsafe or ineffective sys-
tems. Automated systems should be developed with consultation
from diverse communities, stakeholders, and domain experts to iden-
tify concerns, risks, and potential impacts of the system. Systems
should undergo pre-deployment testing, risk identification and miti-
gation, and ongoing monitoring that demonstrate they are safe and
effective based on their intended use, mitigation of unsafe outcomes
including those beyond the intended use, and adherence to do-
main-specific standards. Outcomes of these protective measures
should include the possibility of not deploying the system or remov-
ing a system from use. Automated systems should not be designed
with an intent or reasonably foreseeable possibility of endangering
your safety or the safety of your community. They should be designed
to proactively protect you from harms stemming from unintended,
yet foreseeable, uses or impacts of automated systems. You should be
protected from inappropriate or irrelevant data use in the design, de-
velopment, and deployment of automated systems, and from the
compounded harm of its reuse. Independent evaluation and report-
ing that confirms that the system is safe and effective, including re-
porting of steps taken to mitigate potential harms, should be per-
formed and the results made public whenever possible.
15SAFE AND EFFECTIVE
SYSTEMS
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
While technologies are being deployed to solve problems across a wide array of issues, our reliance on technology can
also lead to its use in situations where it has not yet been proven to work—either at all or within an acceptable range
of error. In other cases, technologies do not work as intended or as promised, causing substantial and unjustified harm.
Automated systems sometimes rely on data from other systems, including historical data, allowing irrelevant informa-
tion from past decisions to infect decision-making in unrelated situations. In some cases, technologies are purposeful-
ly designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intended
or unintended uses lead to unintended harms.
Many of the harms resulting from these technologies are preventable, and actions are already being taken to protect
the public. Some companies have put in place safeguards that have prevented harm from occurring by ensuring that
key development decisions are vetted by an ethics review; others have identified and mitigated harms found through
pre-deployment testing and ongoing monitoring processes. Governments at all levels have existing public consulta-
tion processes that may be applied when considering the use of new automated systems, and existing product develop-
ment and testing practices already protect the American public from many potential harms.
Still, these kinds of practices are deployed too rarely and unevenly. Expanded, proactive protections could build on
these existing practices, increase confidence in the use of automated systems, and protect the American public. Inno-
vators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protections
from unsafe outcomes. All can benefit from assurances that automated systems will be designed, tested, and consis-
tently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harm-
ful outcomes.
• A proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was imple-
mented at hundreds of hospitals around the country. An independent study showed that the model predictions
underperformed relative to the designer’s claims while also causing ‘alert fatigue’ by falsely alerting
likelihood of sepsis.6
• On social media, Black people who quote and criticize racist messages have had their own speech silenced when
a platform’s automated moderation system failed to distinguish this “counter speech” (or other critique
and journalism) from the original hateful messages to which such speech responded.7
• A device originally developed to help people track and find lost items has been used as a tool by stalkers to track
victims’ locations in violation of their privacy and safety. The device manufacturer took steps after release to
protect people from unwanted tracking by alerting people on their phones when a device is found to be moving
with them over time and also by having the device make an occasional noise, but not all phones are able
to receive the notification and the devices remain a safety concern due to their misuse.8
• An algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit,
even if those neighborhoods were not the ones with the highest crime rates. These incorrect crime predictions
were the result of a feedback loop generated from the reuse of data from previous arrests and algorithm
predictions.9
16SAFE AND EFFECTIVE
SYSTEMS
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
• AI-enabled “nudification” technology that creates images where people appear to be nude—including apps that
enable non-technical users to create or alter images of individuals without their consent—has proliferated at an
alarming rate. Such technology is becoming a common form of image-based abuse that disproportionately
impacts women. As these tools become more sophisticated, they are producing altered images that are increasing-
ly realistic and are difficult for both humans and AI to detect as inauthentic. Regardless of authenticity, the expe-
rience of harm to victims of non-consensual intimate images can be devastatingly real—affecting their personal
and professional lives, and impacting their mental and physical health.10
• A company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its driv-
ers, but the system incorrectly penalized drivers when other cars cut them off or when other events beyond
their control took place on the road. As a result, drivers were incorrectly ineligible to receive a bonus.11
17SAFE AND EFFECTIVE
SYSTEMS
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
In order to ensure that an automated system is safe and effective, it should include safeguards to protect the
public from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the task
at hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness of
the system. These expectations are explained below.
Protect the public from harm in a proactive and ongoing manner
Consultation. The public should be consulted in the design, implementation, deployment, acquisition, and
maintenance phases of automated system development, with emphasis on early-stage consultation before a
system is introduced or a large change implemented. This consultation should directly engage diverse impact-
ed communities to consider concerns and risks that may be unique to those communities, or disproportionate-
ly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold-
ers may differ depending on the specific automated system and development phase, but should include
subject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as
civil rights, civil liberties, and privacy experts. For private sector applications, consultations before product
launch may need to be confidential. Government applications, particularly law enforcement applications or
applications that raise national security considerations, may require confidential or limited engagement based
on system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultation
should be documented, and the automated system developers were proposing to create, use, or deploy should
be reconsidered based on this feedback.
Testing. Systems should undergo extensive testing before deployment. This testing should follow
domain-specific best practices, when available, for ensuring the technology will work in its real-world
context. Such testing should take into account both the specific technology used and the roles of any human
operators or reviewers who impact system outcomes or effectiveness; testing should include both automated
systems testing and human-led (manual) testing. Testing conditions should mirror as closely as possible the
conditions in which the system will be deployed, and new testing may be required for each deployment to
account for material differences in conditions from one deployment to another. Following testing, system
performance should be compared with the in-place, potentially human-driven, status quo procedures, with
existing human performance considered as a performance baseline for the algorithm to meet pre-deployment,
and as a lifecycle minimum performance standard. Decision possibilities resulting from performance testing
should include the possibility of not deploying the system.
Risk identification and mitigation. Before deployment, and in a proactive and ongoing manner, poten-
tial risks of the automated system should be identified and mitigated. Identified risks should focus on the
potential for meaningful impact on people’s rights, opportunities, or access and include those to impacted
communities that may not be direct users of the automated system, risks resulting from purposeful misuse of
the system, and other concerns identified via the consultation process. Assessment and, where possible, mea-
surement of the impact of risks should be included and balanced such that high impact risks receive attention
and mitigation proportionate with those impacts. Automated systems with the intended purpose of violating
the safety of others should not be developed or used; systems with such safety violations as identified unin-
tended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessi-
tate rollback or significant modification to a launched automated system.
18SAFE AND EFFECTIVE
SYSTEMS
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Ongoing monitoring. Automated systems should have ongoing monitoring procedures, including recalibra-
tion procedures, in place to ensure that their performance does not fall below an acceptable level over time,
based on changing real-world conditions or deployment contexts, post-deployment modification, or unexpect-
ed conditions. This ongoing monitoring should include continuous evaluation of performance metrics and
harm assessments, updates of any systems, and retraining of any machine learning models as necessary, as well
as ensuring that fallback mechanisms are in place to allow reversion to a previously working system. Monitor-
ing should take into account the performance of both technical system components (the algorithm as well as
any hardware components, data inputs, etc.) and human operators. It should include mechanisms for testing
the actual accuracy of any predictions or recommendations generated by a system, not just a human operator’s
determination of their accuracy. Ongoing monitoring procedures should include manual, human-led monitor-
ing as a check in the event there are shortcomings in automated monitoring systems. These monitoring proce-
dures should be in place for the lifespan of the deployed automated system.
Clear organizational oversight. Entities responsible for the development or use of automated systems
should lay out clear governance structures and procedures. This includes clearly-stated governance proce-
dures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoing
assessment and mitigation. Organizational stakeholders including those with oversight of the business process
or operation being automated, as well as other organizational divisions that may be affected due to the use of
the system, should be involved in establishing governance procedures. Responsibility should rest high enough
in the organization that decisions about resources, mitigation, incident response, and potential rollback can be
made promptly, with sufficient weight given to risk mitigation objectives against competing concerns. Those
holding this responsibility should be made aware of any use cases with the potential for meaningful impact on
people’s rights, opportunities, or access as determined based on risk identification procedures. In some cases,
it may be appropriate for an independent ethics review to be conducted before deployment.
Avoid inappropriate, low-quality, or irrelevant data use and the compounded harm of its
reuse
Relevant and high-quality data. Data used as part of any automated system’s creation, evaluation, or
deployment should be relevant, of high quality, and tailored to the task at hand. Relevancy should be
established based on research-backed demonstration of the causal influence of the data to the specific use case
or justified more generally based on a reasonable expectation of usefulness in the domain and/or for the
system design or ongoing development. Relevance of data should not be established solely by appealing to
its historical connection to the outcome. High quality and tailored data should be representative of the task at
hand and errors from data entry or other sources should be measured and limited. Any data used as the target
of a prediction process should receive particular attention to the quality and validity of the predicted outcome
or label to ensure the goal of the automated system is appropriately identified and measured. Additionally,
justification should be documented for each data attribute and source to explain why it is appropriate to use
that data to inform the results of the automated system and why such use will not violate any applicable laws.
In cases of high-dimensional and/or derived attributes, such justifications can be provided as overall
descriptions of the attribute generation process and appropriateness.
19SAFE AND EFFECTIVE
SYSTEMS
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Derived data sources tracked and reviewed carefully. Data that is derived from other data through
the use of algorithms, such as data derived or inferred from prior model outputs, should be identified and
tracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk
inputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be care-
fully validated against the risk of collateral consequences.
Data reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result
in the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi-
cating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in
some cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure
safety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal
matters or private sector use) should only occur where use of such data is legally authorized and, after examina-
tion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason-
able measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to
identify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for
replacing individual-level sensitive data.
Demonstrate the safety and effectiveness of the system
Independent evaluation. Automated systems should be designed to allow for independent evaluation (e.g.,
via application programming interfaces). Independent evaluators, such as researchers, journalists, ethics
review boards, inspectors general, and third-party auditors, should be given access to the system and samples
of associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual
property law), in order to perform such evaluations. Mechanisms should be included to ensure that system
access for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to
provide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot
be revoked without reasonable and verified justification.
Reporting.12 Entities responsible for the development or use of automated systems should provide
regularly-updated reports that include: an overview of the system, including how it is embedded in the
organization’s business processes or other activities, system goals, any human-run procedures that form a
part of the system, and specific performance expectations; a description of any data used to train machine
learning models or for other purposes, including how data sources were processed and interpreted, a
summary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the
results of public consultation such as concerns raised and any decisions made due to these concerns; risk
identification and management assessments and any steps taken to mitigate potential harms; the results of
performance testing including, but not limited to, accuracy, differential demographic impact, resulting
error rates (overall and per demographic group), and comparisons to previously deployed systems;
ongoing monitoring procedures and regular performance testing reports, including monitoring frequency,
results, and actions taken; and the procedures for and results from independent evaluations. Reporting
should be provided in a plain language and machine-readable manner.
20SAFE AND EFFECTIVE
SYSTEMS
H
OW THESE PRINCIPLES CAN MOVE INTO PRACTICE
Real-life examples of how these principles can become reality, through laws, policies, and practical
technical and sociotechnical approaches to protecting rights, opportunities, and access.
Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the
Federal Government requires that certain federal agencies adhere to nine principles when
designing, developing, acquiring, or using AI for purposes other than national security or
defense. These principles—while taking into account the sensitive law enforcement and other contexts in which
the federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and
respectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d)
safe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar-
ent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order.
Affected agencies across the federal government have released AI use case inventories13 and are implementing
plans to bring those AI systems into compliance with the Executive Order or retire them.
The law and policy landscape for motor vehicles shows that strong safety regulations—and
measures to address harms when they occur—can enhance innovation in the context of com-
plex technologies. Cars, like automated digital systems, comprise a complex collection of components.
The National Highway Traffic Safety Administration,14 through its rigorous standards and independent
evaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to
innovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate
requirements on drivers, such as slowing down near schools or playgrounds.16
From large companies to start-ups, industry is providing innovative solutions that allow
organizations to mitigate risks to the safety and efficacy of AI systems, both before
deployment and through monitoring over time.17 These innovative solutions include risk
assessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing
monitoring, documentation procedures specific to model assessments, and many other strategies that aim to
mitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety
and effectiveness concerns.
The Office of Management and Budget (OMB) has called for an expansion of opportunities
for meaningful stakeholder engagement in the design of programs and services. OMB also
points to numerous examples of effective and proactive stakeholder engagement, including the Community-
Based Participatory Research Program developed by the National Institutes of Health and the participatory
technology assessments developed by the National Oceanic and Atmospheric Administration.18
The National Institute of Standards and Technology (NIST) is developing a risk
management framework to better manage risks posed to individuals, organizations, and
society by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for
voluntary use to help incorporate trustworthiness considerations into the design, development, use, and
evaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-
driven, open, transparent, and collaborative process that includes workshops and other opportunities to provide
input. The NIST framework aims to foster the development of innovative approaches to address
characteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy,
robustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of
harmful uses. The NIST framework will consider and encompass principles such as
transparency, accountability, and fairness during pre-design, design and development, deployment, use,
and testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23.
21SAFE AND EFFECTIVE
SYSTEMS
H
OW THESE PRINCIPLES CAN MOVE INTO PRACTICE
Real-life examples of how these principles can become reality, through laws, policies, and practical
technical and sociotechnical approaches to protecting rights, opportunities, and access.
Some U.S government agencies have developed specific frameworks for ethical use of AI
systems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina-
tion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the
ethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence
Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national
security and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles
of Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to
develop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implement
these principles.22
The National Science Foundation (NSF) funds extensive research to help foster the
development of automated systems that adhere to and advance their safety, security and
effectiveness. Multiple NSF programs support research that directly addresses many of these principles:
the National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable
AI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe
autonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25
program supports research on cybersecurity and privacy enhancing technologies in automated systems; the
Formal Methods in the Field26 program supports research on rigorous formal verification and analysis of
automated systems and machine learning, and the Designing Accountable Software Systems27 program supports
research on rigorous and reproducible methodologies for developing software systems with legal and regulatory
compliance in mind.
Some state legislatures have placed strong transparency and validity requirements on
the use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a
cause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any
pretrial risk assessment, before use in the state, first be "shown to be free of bias against any class of
individuals protected from discrimination by state or federal law", that any locality using a pretrial risk
assessment must first formally validate the claim of its being free of bias, that "all documents, records, and
information used to build or validate the risk assessment shall be open to public inspection," and that assertions
of trade secrets cannot be used "to quash discovery in a criminal matter by a party to a criminal case."
22A D Protections
LGORITHMIC ISCRIMINATION
You should not face discrimination by algorithms
and systems should be used and designed in an
equitable way. Algorithmic discrimination occurs when
automated systems contribute to unjustified different treatment or
impacts disfavoring people based on their race, color, ethnicity,
sex (including pregnancy, childbirth, and related medical
conditions, gender identity, intersex status, and sexual
orientation), religion, age, national origin, disability, veteran status,
genetic infor-mation, or any other classification protected by law.
Depending on the specific circumstances, such algorithmic
discrimination may violate legal protections. Designers, developers,
and deployers of automated systems should take proactive and
continuous measures to protect individuals and communities
from algorithmic discrimination and to use and design systems in
an equitable way. This protection should include proactive equity
assessments as part of the system design, use of representative data
and protection against proxies for demographic features, ensuring
accessibility for people with disabilities in design and development,
pre-deployment and ongoing disparity testing and mitigation, and
clear organizational oversight. Independent evaluation and plain
language reporting in the form of an algorithmic impact assessment,
including disparity testing results and mitigation information,
should be performed and made public whenever possible to confirm
these protections.
23Algorithmic
Discrimination
Protections
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
There is extensive evidence showing that automated systems can produce inequitable outcomes and amplify
existing inequity.30 Data that fails to account for existing systemic biases in American society can result in a range of
consequences. For example, facial recognition technology that can contribute to wrongful and discriminatory
arrests,31 hiring algorithms that inform discriminatory decisions, and healthcare algorithms that discount
the severity of certain diseases in Black Americans. Instances of discriminatory practices built into and
resulting from AI and other automated systems exist across many industries, areas, and contexts. While automated
systems have the capacity to drive extraordinary advances and innovations, algorithmic discrimination
protections should be built into their design, deployment, and ongoing use.
Many companies, non-profits, and federal government agencies are already taking steps to ensure the public
is protected from algorithmic discrimination. Some companies have instituted bias testing as part of their product
quality assessment and launch procedures, and in some cases this testing has led products to be changed or not
launched, preventing harm to the public. Federal government agencies have been developing standards and guidance
for the use of automated systems in order to help prevent bias. Non-profits and companies have developed best
practices for audits and impact assessments to help identify potential algorithmic discrimination and provide
transparency to the public in the mitigation of such biases.
But there is much more work to do to protect the public from algorithmic discrimination to use and design
automated systems in an equitable way. The guardrails protecting the public from discrimination in their daily
lives should include their digital lives and impacts—basic safeguards against abuse, bias, and discrimination to
ensure that all people are treated fairly when automated systems are used. This includes all dimensions of their
lives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal
justice system. Ensuring equity should also go beyond existing guardrails to consider the holistic impact that
automated systems make on underserved communities and to institute proactive protections that support these
communities.
• An automated system using nontraditional factors such as educational attainment and employment history as
part of its loan underwriting and pricing model was found to be much more likely to charge an applicant who
attended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loan
than an applicant who did not attend an HBCU. This was found to be true even when controlling for
other credit-related factors.32
• A hiring tool that learned the features of a company's employees (predominantly men) rejected women appli-
cants for spurious and discriminatory reasons; resumes with the word “women’s,” such as “women’s
chess club captain,” were penalized in the candidate ranking.33
• A predictive model marketed as being able to predict whether students are likely to drop out of school was
used by more than 500 universities across the country. The model was found to use race directly as a predictor,
and also shown to have large disparities by race; Black students were as many as four times as likely as their
otherwise similar white peers to be deemed at high risk of dropping out. These risk scores are used by advisors
to guide students towards or away from majors, and some worry that they are being used to guide
Black students away from math and science subjects.34
• A risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showed
evidence of disparity in prediction. The tool overpredicts the risk of recidivism for some groups of color on the
general recidivism tools, and underpredicts the risk of recidivism for some groups of color on some of the
violent recidivism tools. The Department of Justice is working to reduce these disparities and has
publicly released a report detailing its review of the tool.35
24Algorithmic
Discrimination
Protections
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
• An automated sentiment analyzer, a tool often used by technology platforms to determine whether a state-
ment posted online expresses a positive or negative sentiment, was found to be biased against Jews and gay
people. For example, the analyzer marked the statement “I’m a Jew” as representing a negative sentiment,
while “I’m a Christian” was identified as expressing a positive sentiment.36 This could lead to the
preemptive blocking of social media comments such as: “I’m gay.” A related company with this bias concern
has made their data public to encourage researchers to help address the issue37 and has released reports
identifying and measuring this problem as well as detailing attempts to address it.38
• Searches for “Black girls,” “Asian girls,” or “Latina girls” return predominantly39 sexualized content, rather
than role models, toys, or activities.40 Some search engines have been working to reduce the prevalence of
these results, but the problem remains.41
• Advertisement delivery systems that predict who is most likely to click on a job advertisement end up deliv-
ering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermar-
ket cashier ads to women and jobs with taxi companies to primarily Black people.42
• Body scanners, used by TSA at airport checkpoints, require the operator to select a “male” or “female”
scanning setting based on the passenger’s sex, but the setting is chosen based on the operator’s perception of
the passenger’s gender identity. These scanners are more likely to flag transgender travelers as requiring
extra screening done by a person. Transgender travelers have described degrading experiences associated
with these extra screenings.43 TSA has recently announced plans to implement a gender-neutral algorithm44
while simultaneously enhancing the security effectiveness capabilities of the existing technology.
• The National Disabled Law Students Association expressed concerns that individuals with disabilities were
more likely to be flagged as potentially suspicious by remote proctoring AI systems because of their disabili-
ty-specific access needs such as needing longer breaks or using screen readers or dictation software.45
• An algorithm designed to identify patients with high needs for healthcare systematically assigned lower
scores (indicating that they were not as high need) to Black patients than to those of white patients, even
when those patients had similar numbers of chronic conditions and other markers of health.46 In addition,
healthcare clinical algorithms that are used by physicians to guide clinical decisions may include
sociodemographic variables that adjust or “correct” the algorithm’s output on the basis of a patient’s race or
ethnicity, which can lead to race-based health inequities.47
25Algorithmic
Discrimination
Protections
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Any automated system should be tested to help ensure it is free from algorithmic discrimination before it can be
sold or used. Protection against algorithmic discrimination should include designing to ensure equity, broadly
construed. Some algorithmic discrimination is already prohibited under existing anti-discrimination law. The
expectations set out below describe proactive technical and policy steps that can be taken to not only
reinforce those legal protections but extend beyond them to ensure equity for underserved communities48
even in circumstances where a specific legal protection may not be clearly established. These protections
should be instituted throughout the design, development, and deployment process and are described below
roughly in the order in which they would be instituted.
Protect the public from algorithmic discrimination in a proactive and ongoing manner
Proactive assessment of equity in design. Those responsible for the development, use, or oversight of
automated systems should conduct proactive equity assessments in the design phase of the technology
research and development or during its acquisition to review potential input data, associated historical
context, accessibility for people with disabilities, and societal goals to identify potential discrimination and
effects on equity resulting from the introduction of the technology. The assessed groups should be as inclusive
as possible of the underserved communities mentioned in the equity definition: Black, Latino, and Indigenous
and Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of
religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and inter-
sex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons
otherwise adversely affected by persistent poverty or inequality. Assessment could include both qualitative
and quantitative evaluations of the system. This equity assessment should also be considered a core part of the
goals of the consultation conducted as part of the safety and efficacy review.
Representative and robust data. Any data used as part of system development or assessment should be
representative of local communities based on the planned deployment setting and should be reviewed for bias
based on the historical and societal context of the data. Such data should be sufficiently robust to identify and
help to mitigate biases and potential harms.
Guarding against proxies. Directly using demographic information in the design, development, or
deployment of an automated system (for purposes other than evaluating a system for discrimination or using
a system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should be
avoided. In many cases, attributes that are highly correlated with demographic features, known as proxies, can
contribute to algorithmic discrimination. In cases where use of the demographic features themselves would
lead to illegal algorithmic discrimination, reliance on such proxies in decision-making (such as that facilitated
by an algorithm) may also be prohibited by law. Proactive testing should be performed to identify proxies by
testing for correlation between demographic information and attributes in any data used as part of system
design, development, or use. If a proxy is identified, designers, developers, and deployers should remove the
proxy; if needed, it may be possible to identify alternative attributes that can be used instead. At a minimum,
organizations should ensure a proxy feature is not given undue weight and should monitor the system closely
for any resulting algorithmic discrimination.
26Algorithmic
Discrimination
Protections
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Ensuring accessibility during design, development, and deployment. Systems should be
designed, developed, and deployed by organizations in ways that ensure accessibility to people with disabili-
ties. This should include consideration of a wide variety of disabilities, adherence to relevant accessibility
standards, and user experience research both before and after deployment to identify and address any accessi-
bility barriers to the use or effectiveness of the automated system.
Disparity assessment. Automated systems should be tested using a broad set of measures to assess wheth-
er the system components, both in pre-deployment testing and in-context deployment, produce disparities.
The demographics of the assessed groups should be as inclusive as possible of race, color, ethnicity, sex
(including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual
orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifi-
cation protected by law. The broad set of measures assessed should include demographic performance mea-
sures, overall and subgroup parity assessment, and calibration. Demographic data collected for disparity
assessment should be separated from data used for the automated system and privacy protections should be
instituted; in some cases it may make sense to perform such assessment using a data sample. For every
instance where the deployed automated system leads to different treatment or impacts disfavoring the identi-
fied groups, the entity governing, implementing, or using the system should document the disparity and a
justification for any continued use of the system.
Disparity mitigation. When a disparity assessment identifies a disparity against an assessed group, it may
be appropriate to take steps to mitigate or eliminate the disparity. In some cases, mitigation or elimination of
the disparity may be required by law. Disparities that have the potential to lead to algorithmic
discrimination, cause meaningful harm, or violate equity49 goals should be mitigated. When designing and
evaluating an automated system, steps should be taken to evaluate multiple models and select the one that
has the least adverse impact, modify data input choices, or otherwise identify a system with fewer
disparities. If adequate mitigation of the disparity is not possible, then the use of the automated system
should be reconsidered. One of the considerations in whether to use the system should be the validity of any
target measure; unobservable targets may result in the inappropriate use of proxies. Meeting these
standards may require instituting mitigation procedures and other protective measures to address
algorithmic discrimination, avoid meaningful harm, and achieve equity goals.
Ongoing monitoring and mitigation. Automated systems should be regularly monitored to assess algo-
rithmic discrimination that might arise from unforeseen interactions of the system with inequities not
accounted for during the pre-deployment testing, changes to the system after deployment, or changes to the
context of use or associated data. Monitoring and disparity assessment should be performed by the entity
deploying or using the automated system to examine whether the system has led to algorithmic discrimina-
tion when deployed. This assessment should be performed regularly and whenever a pattern of unusual
results is occurring. It can be performed using a variety of approaches, taking into account whether and how
demographic information of impacted people is available, for example via testing with a sample of users or via
qualitative user experience research. Riskier and higher-impact systems should be monitored and assessed
more frequently. Outcomes of this assessment should include additional disparity mitigation, if needed, or
fallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, and
prior mechanisms provide better adherence to equity standards.
27Algorithmic
Discrimination
Protections
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Demonstrate that the system protects against algorithmic discrimination
Independent evaluation. As described in the section on Safe and Effective Systems, entities should allow
independent evaluation of potential algorithmic discrimination caused by automated systems they use or
oversee. In the case of public sector uses, these independent evaluations should be made public unless law
enforcement or national security restrictions prevent doing so. Care should be taken to balance individual
privacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and
controls allow access to such data without compromising privacy.
Reporting. Entities responsible for the development or use of automated systems should provide
reporting of an appropriately designed algorithmic impact assessment,50 with clear specification of who
performs the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in
response to the assessment. This algorithmic impact assessment should include at least: the results of any
consultation, design stage equity assessments (potentially including qualitative analysis), accessibility
designs and testing, disparity testing, document any remaining disparities, and detail any mitigation
implementation and assessments. This algorithmic impact assessment should be made public whenever
possible. Reporting should be provided in a clear and machine-readable manner using plain language to
allow for more straightforward public accountability.
28Algorithmic
Discrimination
Protections
H
OW THESE PRINCIPLES CAN MOVE INTO PRACTICE
Real-life examples of how these principles can become reality, through laws, policies, and practical
technical and sociotechnical approaches to protecting rights, opportunities, and access.
The federal government is working to combat discrimination in mortgage lending. The Depart-
ment of Justice has launched a nationwide initiative to combat redlining, which includes reviewing how
lenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.51
This initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial
Protection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation
Equity includes a commitment from the agencies that oversee mortgage lending to include a
nondiscrimination standard in the proposed rules for Automated Valuation Models.52
The Equal Employment Opportunity Commission and the Department of Justice have clearly
laid out how employers’ use of AI and other automated systems can result in
discrimination against job applicants and employees with disabilities.53 The documents explain
how employers’ use of software that relies on algorithmic decision-making may violate existing requirements
under Title I of the Americans with Disabilities Act (“ADA”). This technical assistance also provides practical
tips to employers on how to comply with the ADA, and to job applicants and employees who think that their
rights may have been violated.
Disparity assessments identified harms to Black patients' healthcare access. A widely
used healthcare algorithm relied on the cost of each patient’s past medical care to predict future medical needs,
recommending early interventions for the patients deemed most at risk. This process discriminated
against Black patients, who generally have less access to medical care and therefore have generated less cost
than white patients with similar illness and need. A landmark study documented this pattern and proposed
practical ways that were shown to reduce this bias, such as focusing specifically on active chronic health
conditions or avoidable future costs related to emergency visits and hospitalization.54
Large employers have developed best practices to scrutinize the data and models used
for hiring. An industry initiative has developed Algorithmic Bias Safeguards for the Workforce, a structured
questionnaire that businesses can use proactively when procuring software to evaluate workers. It covers
specific technical questions such as the training data used, model training process, biases identified, and
mitigation steps employed.55
Standards organizations have developed guidelines to incorporate accessibility criteria
into technology design processes. The most prevalent in the United States is the Access Board’s Section
508 regulations,56 which are the technical standards for federal information communication technology (software,
hardware, and web). Other standards include those issued by the International Organization for
Standardization,57 and the World Wide Web Consortium Web Content Accessibility Guidelines,58 a globally
recognized voluntary consensus standard for web content and other information and communications
technology.
NIST has released Special Publication 1270, Towards a Standard for Identifying and Managing Bias
in Artificial Intelligence.59 The special publication: describes the stakes and challenges of bias in artificial
intelligence and provides examples of how and why it can chip away at public trust; identifies three categories
of bias in AI – systemic, statistical, and human – and describes how and where they contribute to harms; and
describes three broad challenges for mitigating bias – datasets, testing and evaluation, and human factors – and
introduces preliminary guidance for addressing them. Throughout, the special publication takes a socio-
technical perspective to identifying and managing AI bias.
29D P
ATA RIVACY
You should be protected from abusive data practices via built-in
protections and you should have agency over how data about
you is used. You should be protected from violations of privacy through
design choices that ensure such protections are included by default, including
ensuring that data collection conforms to reasonable expectations and that
only data strictly necessary for the specific context is collected. Designers, de-
velopers, and deployers of automated systems should seek your permission
and respect your decisions regarding collection, use, access, transfer, and de-
letion of your data in appropriate ways and to the greatest extent possible;
where not possible, alternative privacy by design safeguards should be used.
Systems should not employ user experience and design decisions that obfus-
cate user choice or burden users with defaults that are privacy invasive. Con-
sent should only be used to justify collection of data in cases where it can be
appropriately and meaningfully given. Any consent requests should be brief,
be understandable in plain language, and give you agency over data collection
and the specific context of use; current hard-to-understand no-
tice-and-choice practices for broad uses of data should be changed. Enhanced
protections and restrictions for data and inferences related to sensitive do-
mains, including health, work, education, criminal justice, and finance, and
for data pertaining to youth should put you first. In sensitive domains, your
data and related inferences should only be used for necessary functions, and
you should be protected by ethical review and use prohibitions. You and your
communities should be free from unchecked surveillance; surveillance tech-
nologies should be subject to heightened oversight that includes at least
pre-deployment assessment of their potential harms and scope limits to pro-
tect privacy and civil liberties. Continuous surveillance and monitoring
should not be used in education, work, housing, or in other contexts where the
use of such surveillance technologies is likely to limit rights, opportunities, or
access. Whenever possible, you should have access to reporting that confirms
your data decisions have been respected and provides an assessment of the
potential impact of surveillance technologies on your rights, opportunities, or
access.
30DATA PRIVACY
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
Data privacy is a foundational and cross-cutting principle required for achieving all others in this framework. Surveil-
lance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries,
with more and more companies tracking the behavior of the American public, building individual profiles based on
this data, and using this granular-level information as input into automated systems that further track, profile, and
impact the American public. Government agencies, particularly law enforcement agencies, also use and help develop
a variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input
into other automated systems that directly impact people’s lives. Federal law has not grown to address the expanding
scale of private data collection, or of the ability of governments at all levels to access that data and leverage the means
of private collection.
Meanwhile, members of the American public are often unable to access their personal data or make critical decisions
about its collection and use. Data brokers frequently collect consumer data from numerous sources without
consumers’ permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to
make decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillance
technologies has increased in schools and workplaces, and, when coupled with consequential management and
evaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, and
a reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated by
data brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive,
breeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; and
threatening our democratic process.63 The American public should be protected from these growing risks.
Increasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer
privacy into their products by design and by default, including by minimizing the data they collect, communicating
collection and use clearly, and improving security practices. Federal government surveillance and other collection and
use of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention
in some cases. Many states have also enacted consumer data privacy protection regimes to address some of these
harms.
However, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory
framework governing the rights of the public when it comes to personal data. While a patchwork of laws exists to
guide the collection and use of personal data in specific contexts, including health, employment, education, and credit,
it can be unclear how these laws apply in other contexts and in an increasingly automated society. Additional protec-
tions would assure the American public that the automated systems they use are not monitoring their activities,
collecting information on their lives, or otherwise surveilling them without context-specific consent or legal authori-
ty.
31DATA PRIVACY
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
• An insurer might collect data from a person's social media presence as part of deciding what life
insurance rates they should be offered.64
• A data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of
thousands of people to potential identity theft. 65
• A local public housing authority installed a facial recognition system at the entrance to housing complexes to
assist law enforcement with identifying individuals viewed via camera when police reports are filed, leading
the community, both those living in the housing complex and not, to have videos of them sent to the local
police department and made available for scanning by its facial recognition software.66
• Companies use surveillance software to track employee discussions about union activity and use the
resulting data to surveil individual employees and surreptitiously intervene in discussions.67
32DATA PRIVACY
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Traditional terms of service—the block of text that the public is accustomed to clicking through when using a web-
site or digital app—are not an adequate mechanism for protecting privacy. The American public should be protect-
ed via built-in privacy protections, data minimization, use and collection limitations, and transparency, in addition
to being entitled to clear mechanisms to control access to and use of their data—including their metadata—in a
proactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal data
should meet these expectations.
Protect privacy by design and by default
Privacy by design and by default. Automated systems should be designed and built with privacy protect-
ed by default. Privacy risks should be assessed throughout the development life cycle, including privacy risks
from reidentification, and appropriate technical and policy mitigation measures should be implemented. This
includes potential harms to those who are not users of the automated system, but who may be harmed by
inferred data, purposeful privacy violations, or community surveillance or other community harms. Data
collection should be minimized and clearly communicated to the people whose data is collected. Data should
only be collected or used for the purposes of training or testing machine learning models if such collection and
use is legal and consistent with the expectations of the people whose data is collected. User experience
research should be conducted to confirm that people understand what data is being collected about them and
how it will be used, and that this collection matches their expectations and desires.
Data collection and use-case scope limits. Data collection should be limited in scope, with specific,
narrow identified goals, to avoid "mission creep." Anticipated data collection should be determined to be
strictly necessary to the identified goals and should be minimized as much as possible. Data collected based on
these identified goals and for a specific context should not be used in a different context without assessing for
new privacy risks and implementing appropriate mitigation measures, which may include express consent.
Clear timelines for data retention should be established, with data deleted as soon as possible in accordance
with legal or policy-based limitations. Determined data retention timelines should be documented and justi-
fied.
Risk identification and mitigation. Entities that collect, use, share, or store sensitive data should
attempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropri-
ately to identified risks. Appropriate responses include determining not to process data when the privacy risks
outweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do not
include sharing or transferring the privacy risks to users via notice or consent requests where users could not
reasonably be expected to understand the risks without further support.
Privacy-preserving security. Entities creating, using, or governing automated systems should follow
privacy and security best practices designed to ensure data and metadata do not leak beyond the specific
consented use case. Best practices could include using privacy-enhancing cryptography or other types of
privacy-enhancing technologies or fine-grained permissions and access control mechanisms, along with
conventional system security protocols.
33DATA PRIVACY
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Protect the public from unchecked surveillance
Heightened oversight of surveillance. Surveillance or monitoring systems should be subject to
heightened oversight that includes at a minimum assessment of potential harms during design (before deploy-
ment) and in an ongoing manner, to ensure that the American public’s rights, opportunities, and access are
protected. This assessment should be done before deployment and should give special attention to ensure
there is not algorithmic discrimination, especially based on community membership, when deployed in a
specific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as the
system is in use.
Limited and proportionate surveillance. Surveillance should be avoided unless it is strictly necessary
to achieve a legitimate purpose and it is proportionate to the need. Designers, developers, and deployers of
surveillance systems should use the least invasive means of monitoring available and restrict monitoring to the
minimum number of subjects possible. To the greatest extent possible consistent with law enforcement and
national security needs, individuals subject to monitoring should be provided with clear and specific notice
before it occurs and be informed about how the data gathered through surveillance will be used.
Scope limits on surveillance to protect rights and democratic values. Civil liberties and civil
rights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated
system. Surveillance systems should not be used to monitor the exercise of democratic rights, such as voting,
privacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liber-
ties. Information about or algorithmically-determined assumptions related to identity should be carefully
limited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such iden-
tity-related information includes group characteristics or affiliations, geographic designations, location-based
and association-based inferences, social networks, and biometrics. Continuous surveillance and monitoring
systems should not be used in physical or digital workplaces (regardless of employment status), public educa-
tional institutions, and public accommodations. Continuous surveillance and monitoring systems should not
be used in a way that has the effect of limiting access to critical resources or services or suppressing the exer-
cise of rights, even where the organization is not under a particular duty to protect those rights.
Provide the public with mechanisms for appropriate and meaningful consent, access, and
control over their data
Use-specific consent. Consent practices should not allow for abusive surveillance practices. Where data
collectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specif-
ic time durations, and for use by specific entities. Consent should not extend if any of these conditions change;
consent should be re-acquired before using data if the use case changes, a time limit elapses, or data is trans-
ferred to another entity (including being shared or sold). Consent requested should be limited in scope and
should not request consent beyond what is required. Refusal to provide consent should be allowed, without
adverse effects, to the greatest extent possible based on the needs of the use case.
Brief and direct consent requests. When seeking consent from users short, plain language consent
requests should be used so that users understand for what use contexts, time span, and entities they are
providing data and metadata consent. User experience research should be performed to ensure these consent
requests meet performance standards for readability and comprehension. This includes ensuring that consent
requests are accessible to users with disabilities and are available in the language(s) and reading level appro-
priate for the audience. User experience design choices that intentionally obfuscate or manipulate user
choice (i.e., “dark patterns”) should be not be used.
34DATA PRIVACY
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Data access and correction. People whose data is collected, used, shared, or stored by automated
systems should be able to access data and metadata about themselves, know who has access to this data, and
be able to correct it if necessary. Entities should receive consent before sharing data with other entities and
should keep records of what data is shared and with whom.
Consent withdrawal and data deletion. Entities should allow (to the extent legally permissible) with-
drawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of
their data from any systems (e.g., machine learning models) derived from that data.68
Automated system support. Entities designing, developing, and deploying automated systems should
establish and maintain the capabilities that will allow individuals to use their own automated systems to help
them make consent, access, and control decisions in a complex data ecosystem. Capabilities include machine
readable data, standardized data formats, metadata or tags for expressing data processing permissions and
preferences and data provenance and lineage, context of use and access-specific tags, and training models for
assessing privacy risk.
Demonstrate that data privacy and user control are protected
Independent evaluation. As described in the section on Safe and Effective Systems, entities should allow
independent evaluation of the claims made regarding data policies. These independent evaluations should be
made public whenever possible. Care will need to be taken to balance individual privacy with evaluation data
access needs.
Reporting. When members of the public wish to know what data about them is being used in a system, the
entity responsible for the development of the system should respond quickly with a report on the data it has
collected or stored about them. Such a report should be machine-readable, understandable by most users, and
include, to the greatest extent allowable under law, any data and metadata about them or collected from them,
when and how their data and metadata were collected, the specific ways that data or metadata are being used,
who has access to their data and metadata, and what time limitations apply to these data. In cases where a user
login is not available, identity verification may need to be performed before providing such a report to ensure
user privacy. Additionally, summary reporting should be proactively made public with general information
about how peoples’ data and metadata is used, accessed, and stored. Summary reporting should include the
results of any surveillance pre-deployment assessment, including disparity assessment in the real-world
deployment context, the specific identified goals of any data collection, and the assessment done to ensure
only the minimum required data is collected. It should also include documentation about the scope limit
assessments, including data retention timelines and associated justification, and an assessment of the
impact of surveillance or data collection on rights, opportunities, and access. Where possible, this
assessment of the impact of surveillance should be done by an independent party. Reporting should be
provided in a clear and machine-readable manner.
35DATA PRIVACY
E P D R S
XTRA ROTECTIONS FOR ATA ELATED TO ENSITIVE
D
OMAINS
Some domains, including health, employment, education, criminal justice, and personal finance, have long been
singled out as sensitive domains deserving of enhanced data protections. This is due to the intimate nature of these
domains as well as the inability of individuals to opt out of these domains in any meaningful way, and the
historical discrimination that has often accompanied data knowledge.69 Domains understood by the public to be
sensitive also change over time, including because of technological developments. Tracking and monitoring
technologies, personal tracking devices, and our extensive data footprints are used and misused more than ever
before; as such, the protections afforded by current legal guidelines may be inadequate. The American public
deserves assurances that data related to such sensitive domains is protected and used appropriately and only in
narrowly defined contexts with clear benefits to the individual and/or society.
To this end, automated systems that collect, use, share, or store data related to these sensitive domains should meet
additional expectations. Data and metadata are sensitive if they pertain to an individual in a sensitive domain (defined
below); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or
sensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data,
geolocation data, data related to interaction with the criminal justice system, relationship history and legal status such
as custody and divorce information, and home, work, or school environmental data); or have the reasonable potential
to be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm
due to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even
if not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video
data. “Sensitive domains” are those in which activities being conducted can cause material harms, including signifi-
cant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains
that have historically been singled out as deserving of enhanced data protections or where such enhanced protections
are reasonably expected by the public include, but are not limited to, health, family planning and care, employment,
education, criminal justice, and personal finance. In the context of this framework, such domains are considered
sensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains
and data that are considered sensitive are understood to change over time based on societal norms and context.
36DATA PRIVACY
E P D R S
XTRA ROTECTIONS FOR ATA ELATED TO ENSITIVE
D
OMAINS
• Continuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleep
apnea, and send usage data to a patient’s insurance company, which may subsequently deny coverage for the
device based on usage data. Patients were not aware that the data would be used in this way or monitored
by anyone other than their doctor.70
• A department store company used predictive analytics applied to collected consumer data to determine that a
teenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to her
house, revealing to her father that she was pregnant.71
• School audio surveillance systems monitor student conversations to detect potential "stress indicators" as
a warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an
exam using biometric markers.73 These systems have the potential to limit student freedom to express a range
of emotions at school and may inappropriately flag students with disabilities who need accommodations or
use screen readers or dictation software as cheating.74
• Location data, acquired from a data broker, can be used to identify people who visit abortion clinics.75
• Companies collect student data such as demographic information, free or reduced lunch status, whether
they've used drugs, or whether they've expressed interest in LGBTQI+ groups, and then use that data to
forecast student success.76 Parents and education experts have expressed concern about collection of such
sensitive data without express parental consent, the lack of transparency in how such data is being used, and
the potential for resulting discriminatory impacts.
• Many employers transfer employee data to third party job verification services. This information is then used
by potential future employers, banks, or landlords. In one case, a former employee alleged that a
company supplied false data about her job title which resulted in a job offer being revoked.77
37DATA PRIVACY
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
In addition to the privacy expectations above for general non-sensitive data, any system collecting, using, shar-
ing, or storing sensitive data should meet the expectations below. Depending on the technological use case and
based on an ethical assessment, consent for sensitive data may need to be acquired from a guardian and/or child.
Provide enhanced protections for data related to sensitive domains
Necessary functions only. Sensitive data should only be used for functions strictly necessary for that
domain or for functions that are required for administrative reasons (e.g., school attendance records), unless
consent is acquired, if appropriate, and the additional expectations in this section are met. Consent for non-
necessary functions should be optional, i.e., should not be required, incentivized, or coerced in order to
receive opportunities or access to services. In cases where data is provided to an entity (e.g., health insurance
company) in order to facilitate payment for such a need, that data should only be used for that purpose.
Ethical review and use prohibitions. Any use of sensitive data or decision process based in part on sensi-
tive data that might limit rights, opportunities, or access, whether the decision is automated or not, should go
through a thorough ethical review and monitoring, both in advance and by periodic review (e.g., via an indepen-
dent ethics committee or similarly robust process). In some cases, this ethical review may determine that data
should not be used or shared for specific uses even with consent. Some novel uses of automated systems in this
context, where the algorithm is dynamically developing and where the science behind the use case is not well
established, may also count as human subject experimentation, and require special review under organizational
compliance bodies applying medical, scientific, and academic human subject experimentation ethics rules and
governance procedures.
Data quality. In sensitive domains, entities should be especially careful to maintain the quality of data to
avoid adverse consequences arising from decision-making based on flawed or inaccurate data. Such care is
necessary in a fragmented, complex data ecosystem and for datasets that have limited access such as for fraud
prevention and law enforcement. It should be not left solely to individuals to carry the burden of reviewing and
correcting data. Entities should conduct regular, independent audits and take prompt corrective measures to
maintain accurate, timely, and complete data.
Limit access to sensitive data and derived data. Sensitive data and derived data should not be sold,
shared, or made public as part of data brokerage or other agreements. Sensitive data includes data that can be
used to infer sensitive information; even systems that are not directly marketed as sensitive domain technologies
are expected to keep sensitive data private. Access to such data should be limited based on necessity and based
on a principle of local control, such that those individuals closest to the data subject have more access while
those who are less proximate do not (e.g., a teacher has access to their students’ daily progress data while a
superintendent does not).
Reporting. In addition to the reporting on data privacy (as listed above for non-sensitive data), entities devel-
oping technologies related to a sensitive domain and those collecting, using, storing, or sharing sensitive data
should, whenever appropriate, regularly provide public reports describing: any data security lapses or breaches
that resulted in sensitive data leaks; the number, type, and outcomes of ethical pre-reviews undertaken; a
description of any data sold, shared, or made public, and how that data was assessed to determine it did not pres-
ent a sensitive data risk; and ongoing risk identification and management procedures, and any mitigation added
based on these procedures. Reporting should be provided in a clear and machine-readable manner.
38DATA PRIVACY
H
OW THESE PRINCIPLES CAN MOVE INTO PRACTICE
Real-life examples of how these principles can become reality, through laws, policies, and practical
technical and sociotechnical approaches to protecting rights, opportunities, and access.
The Privacy Act of 1974 requires privacy protections for personal information in federal
records systems, including limits on data retention, and also provides individuals a general
right to access and correct their data. Among other things, the Privacy Act limits the storage of individual
information in federal systems of records, illustrating the principle of limiting the scope of data retention. Under
the Privacy Act, federal agencies may only retain data about an individual that is “relevant and necessary” to
accomplish an agency’s statutory purpose or to comply with an Executive Order of the President. The law allows
for individuals to be able to access any of their individual information stored in a federal system of records, if not
included under one of the systems of records exempted pursuant to the Privacy Act. In these cases, federal agen-
cies must provide a method for an individual to determine if their personal information is stored in a particular
system of records, and must provide procedures for an individual to contest the contents of a record about them.
Further, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does not
comply with the Privacy Act’s requirements. Among other things, a court may order a federal agency to amend or
correct an individual’s information in its records or award monetary damages if an inaccurate, irrelevant, untimely,
or incomplete record results in an adverse determination about an individual’s “qualifications, character, rights, …
opportunities…, or benefits.”
NIST’s Privacy Framework provides a comprehensive, detailed and actionable approach for
organizations to manage privacy risks. The NIST Framework gives organizations ways to identify and
communicate their privacy risks and goals to support ethical decision-making in system, product, and service
design or deployment, as well as the measures they are taking to demonstrate compliance with applicable laws
or regulations. It has been voluntarily adopted by organizations across many different sectors around the world.78
A school board’s attempt to surveil public school students—undertaken without
adequate community input—sparked a state-wide biometrics moratorium.79 Reacting to a plan in
the city of Lockport, New York, the state’s legislature banned the use of facial recognition systems and other
“biometric identifying technology” in schools until July 1, 2022.80 The law additionally requires that a report on
the privacy, civil rights, and civil liberties implications of the use of such technologies be issued before
biometric identification technologies can be used in New York schools.
Federal law requires employers, and any consultants they may retain, to report the costs
of surveilling employees in the context of a labor dispute, providing a transparency
mechanism to help protect worker organizing. Employers engaging in workplace surveillance "where
an object there-of, directly or indirectly, is […] to obtain information concerning the activities of employees or a
labor organization in connection with a labor dispute" must report expenditures relating to this surveillance to
the Department of Labor Office of Labor-Management Standards, and consultants who employers retain for
these purposes must also file reports regarding their activities.81
Privacy choices on smartphones show that when technologies are well designed, privacy
and data agency can be meaningful and not overwhelming. These choices—such as contextual, timely
alerts about location tracking—are brief, direct, and use-specific. Many of the expectations listed here for
privacy by design and use-specific consent mirror those distributed to developers as best practices when
developing for smart phone devices,82 such as being transparent about how user data will be used, asking for app
permissions during their use so that the use-context will be clear to users, and ensuring that the app will still
work if users deny (or later revoke) some permissions.
39N E
OTICE AND XPLANATION
You should know that an automated system is being used,
and understand how and why it contributes to outcomes
that impact you. Designers, developers, and deployers of automat-
ed systems should provide generally accessible plain language docu-
mentation including clear descriptions of the overall system func-
tioning and the role automation plays, notice that such systems are in
use, the individual or organization responsible for the system, and ex-
planations of outcomes that are clear, timely, and accessible. Such
notice should be kept up-to-date and people impacted by the system
should be notified of significant use case or key functionality chang-
es. You should know how and why an outcome impacting you was de-
termined by an automated system, including when the automated
system is not the sole input determining the outcome. Automated
systems should provide explanations that are technically valid,
meaningful and useful to you and to any operators or others who
need to understand the system, and calibrated to the level of risk
based on the context. Reporting that includes summary information
about these automated systems in plain language and assessments of
the clarity and quality of the notice and explanations should be made
public whenever possible.
40NOTICE &
EXPLANATION
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
Automated systems now determine opportunities, from employment to credit, and directly shape the American
public’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives. But this
expansive impact is not always visible. An applicant might not know whether a person rejected their resume or a
hiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny-
ing their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contesting
decisions, people are often denied the knowledge they need to address the impact of automated systems on their lives.
Notice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable-
ness of a recommendation before enacting it.
In order to guard against potential harms, the American public needs to know if an automated system is being used.
Clear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework. Like-
wise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to a
particular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore,
unaccountable, whether by design or by omission. These factors can make explanations both more challenging and
more important, and should not be used as a pretext to avoid explaining important decisions to the people impacted
by those choices. In the context of automated systems, clear and valid explanations should be recognized as a baseline
requirement.
Providing notice has long been a standard practice, and in many cases is a legal requirement, when, for example,
making a video recording of someone (outside of a law enforcement or national security context). In some cases, such
as credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate the
process of explaining such systems are under active research and improvement and such explanations can take many
forms. Innovative companies and researchers are rising to the challenge and creating and deploying explanatory
systems that can help the public better understand decisions that impact them.
While notice and explanation requirements are already in place in some sectors or situations, the American public
deserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights,
opportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in the
validity and reasonable use of automated systems.
• A lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home
health-care assistance couldn't determine why, especially since the decision went against historical access
practices. In a court hearing, the lawyer learned from a witness that the state in which the older client
lived had recently adopted a new algorithm to determine el igibility.83 The lack of a timely explanation made it
harder to understand and contest the decision.
• A formal child welfare investigation is opened against a parent based on an algorithm and without the parent
ever being notified that data was being collected and used as part of an algorithmic child maltreatment
risk assessment.84 The lack of notice or an explanation makes it harder for those performing child
maltreatment assessments to validate the risk assessment and denies parents knowledge that could help them
contest a decision.
41NOTICE &
EXPLANATION
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
• A predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of
gun violence (based on automated analysis of social ties to gang members, criminal histories, previous experi-
ences of gun violence, and other factors) and led to individuals being placed on a watch list with no
explanation or public transparency regarding how the system came to its conclusions.85 Both police and
the public deserve to understand why and how such a system is making these determinations.
• A system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entry
errors and other system flaws. These flaws were only revealed when an explanation of the system
was demanded and produced.86 The lack of an explanation made it harder for errors to be corrected in a
timely manner.
42NOTICE &
EXPLANATION
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
An automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and
explanations as to how and why a decision was made or an action was taken by the system. These expectations are
explained below.
Provide clear, timely, understandable, and accessible notice of use and explanations
Generally accessible plain language documentation. The entity responsible for using the automated
system should ensure that documentation describing the overall system (including any human components) is
public and easy to find. The documentation should describe, in plain language, how the system works and how
any automated component is used to determine an action or decision. It should also include expectations about
reporting described throughout this framework, such as the algorithmic impact assessments described as
part of Algorithmic Discrimination Protections.
Accountable. Notices should clearly identify the entity responsible for designing each component of the
system and the entity using it.
Timely and up-to-date. Users should receive notice of the use of automated systems in advance of using or
while being impacted by the technology. An explanation should be available with the decision itself, or soon
thereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use case
or key functionality changes.
Brief and clear. Notices and explanations should be assessed, such as by research on users’ experiences,
including user testing, to ensure that the people using or impacted by the automated system are able to easily
find notices and explanations, read them quickly, and understand and act on them. This includes ensuring that
notices and explanations are accessible to users with disabilities and are available in the language(s) and read-
ing level appropriate for the audience. Notices and explanations may need to be available in multiple forms,
(e.g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the
American public.
Provide explanations as to how and why a decision was made or an action was taken by an
automated system
Tailored to the purpose. Explanations should be tailored to the specific purpose for which the user is
expected to use the explanation, and should clearly state that purpose. An informational explanation might
differ from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the
context of a dispute or contestation process. For the purposes of this framework, 'explanation' should be
construed broadly. An explanation need not be a plain-language statement about causality but could consist of
any mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the
stated purpose. Tailoring should be assessed (e.g., via user experience research).
Tailored to the target of the explanation. Explanations should be targeted to specific audiences and
clearly state that audience. An explanation provided to the subject of a decision might differ from one provided
to an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience
research).
43NOTICE &
EXPLANATION
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Tailored to the level of risk. An assessment should be done to determine the level of risk of the auto-
mated system. In settings where the consequences are high as determined by a risk assessment, or extensive
oversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should
be built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully
transparent models should be used), rather than as an after-the-decision interpretation. In other settings, the
extent of explanation provided should be tailored to the risk level.
Valid. The explanation provided by a system should accurately reflect the factors and the influences that led
to a particular decision, and should be meaningful for the particular customization based on purpose, target,
and level of risk. While approximation and simplification may be necessary for the system to succeed based on
the explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns
related to revealing decision-making information, such simplifications should be done in a scientifically
supportable way. Where appropriate based on the explanatory system, error ranges for the explanation should
be calculated and included in the explanation, with the choice of presentation of such information balanced
with usability and overall interface complexity concerns.
Demonstrate protections for notice and explanation
Reporting. Summary reporting should document the determinations made based on the above consider-
ations, including: the responsible entities for accountability purposes; the goal and use cases for the system,
identified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of
the explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment
of how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of
risk. Individualized profile information should be made readily available to the greatest extent possible that
includes explanations for any system impacts or inferences. Reporting should be provided in a clear plain
language and machine-readable manner.
44NOTICE &
EXPLANATION
H
OW THESE PRINCIPLES CAN MOVE INTO PRACTICE
Real-life examples of how these principles can become reality, through laws, policies, and practical
technical and sociotechnical approaches to protecting rights, opportunities, and access.
People in Illinois are given written notice by the private sector if their biometric informa-
tion is used. The Biometric Information Privacy Act enacted by the state contains a number of provisions
concerning the use of individual biometric data and identifiers. Included among them is a provision that no private
entity may "collect, capture, purchase, receive through trade, or otherwise obtain" such information about an
individual, unless written notice is provided to that individual or their legally appointed representative. 87
Major technology companies are piloting new ways to communicate with the public about
their automated technologies. For example, a collection of non-profit organizations and companies have
worked together to develop a framework that defines operational approaches to transparency for machine
learning systems.88 This framework, and others like it,89 inform the public about the use of these tools, going
beyond simple notice to include reporting elements such as safety evaluations, disparity assessments, and
explanations of how the systems work.
Lenders are required by federal law to notify consumers about certain decisions made about
them. Both the Fair Credit Reporting Act and the Equal Credit Opportunity Act require in certain circumstances
that consumers who are denied credit receive "adverse action" notices. Anyone who relies on the information in a
credit report to deny a consumer credit must, under the Fair Credit Reporting Act, provide an "adverse action"
notice to the consumer, which includes "notice of the reasons a creditor took adverse action on the application
or on an existing credit account."90 In addition, under the risk-based pricing rule,91 lenders must either inform
borrowers of their credit score, or else tell consumers when "they are getting worse terms because of
information in their credit report." The CFPB has also asserted that "[t]he law gives every applicant the right to
a specific explanation if their application for credit was denied, and that right is not diminished simply because
a company uses a complex algorithm that it doesn't understand."92 Such explanations illustrate a shared value
that certain decisions need to be explained.
A California law requires that warehouse employees are provided with notice and explana-
tion about quotas, potentially facilitated by automated systems, that apply to them. Warehous-
ing employers in California that use quota systems (often facilitated by algorithmic monitoring systems) are
required to provide employees with a written description of each quota that applies to the employee, including
“quantified number of tasks to be performed or materials to be produced or handled, within the defined
time period, and any potential adverse employment action that could result from failure to meet the quota.”93
Across the federal government, agencies are conducting and supporting research on explain-
able AI systems. The NIST is conducting fundamental research on the explainability of AI systems. A multidis-
ciplinary team of researchers aims to develop measurement methods and best practices to support the
implementation of core tenets of explainable AI.94 The Defense Advanced Research Projects Agency has a
program on Explainable Artificial Intelligence that aims to create a suite of machine learning techniques that
produce more explainable models, while maintaining a high level of learning performance (prediction
accuracy), and enable human users to understand, appropriately trust, and effectively manage the emerging
generation of artificially intelligent partners.95 The National Science Foundation’s program on Fairness in
Artificial Intelligence also includes a specific interest in research foundations for explainable AI.96
45H A , C , F
UMAN LTERNATIVES ONSIDERATION AND ALLBACK
You should be able to opt out, where appropriate, and
have access to a person who can quickly consider and
remedy problems you encounter. You should be able to opt
out from automated systems in favor of a human alternative, where
appropriate. Appropriateness should be determined based on rea-
sonable expectations in a given context and with a focus on ensuring
broad accessibility and protecting the public from especially harm-
ful impacts. In some cases, a human or other alternative may be re-
quired by law. You should have access to timely human consider-
ation and remedy by a fallback and escalation process if an automat-
ed system fails, it produces an error, or you would like to appeal or
contest its impacts on you. Human consideration and fallback
should be accessible, equitable, effective, maintained, accompanied
by appropriate operator training, and should not impose an unrea-
sonable burden on the public. Automated systems with an intended
use within sensitive domains, including, but not limited to, criminal
justice, employment, education, and health, should additionally be
tailored to the purpose, provide meaningful access for oversight,
include training for any people interacting with the system, and in-
corporate human consideration for adverse or high-risk decisions.
Reporting that includes a description of these human governance
processes and assessment of their timeliness, accessibility, out-
comes, and effectiveness should be made public whenever possible.
46HUMAN ALTERNATIVES,
CONSIDERATION, AND
FALLBACK
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
There are many reasons people may prefer not to use an automated system: the system can be flawed and can lead to
unintended outcomes; it may reinforce bias or be inaccessible; it may simply be inconvenient or unavailable; or it may
replace a paper or manual process to which people had grown accustomed. Yet members of the public are often
presented with no alternative, or are forced to endure a cumbersome process to reach a human decision-maker once
they decide they no longer want to deal exclusively with the automated system or be impacted by its results. As a result
of this lack of human reconsideration, many receive delayed access, or lose access, to rights, opportunities, benefits,
and critical services. The American public deserves the assurance that, when rights, opportunities, or access are
meaningfully at stake and there is a reasonable expectation of an alternative to an automated system, they can conve-
niently opt out of an automated system and will not be disadvantaged for that choice. In some cases, such a human or
other alternative may be required by law, for example it could be required as “reasonable accommodations” for people
with disabilities.
In addition to being able to opt out and use a human alternative, the American public deserves a human fallback
system in the event that an automated system fails or causes harm. No matter how rigorously an automated system is
tested, there will always be situations for which the system fails. The American public deserves protection via human
review against these outlying or unexpected scenarios. In the case of time-critical systems, the public should not have
to wait—immediate human consideration and fallback should be available. In many time-critical systems, such a
remedy is already immediately available, such as a building manager who can open a door in the case an automated
card access system fails.
In the criminal justice system, employment, education, healthcare, and other sensitive domains, automated systems
are used for many purposes, from pre-trial risk assessments and parole decisions to technologies that help doctors
diagnose disease. Absent appropriate safeguards, these technologies can lead to unfair, inaccurate, or dangerous
outcomes. These sensitive domains require extra protections. It is critically important that there is extensive human
oversight in such settings.
These critical protections have been adopted in some scenarios. Where automated systems have been introduced to
provide the public access to government benefits, existing human paper and phone-based processes are generally still
in place, providing an important alternative to ensure access. Companies that have introduced automated call centers
often retain the option of dialing zero to reach an operator. When automated identity controls are in place to board an
airplane or enter the country, there is a person supervising the systems who can be turned to for help or to appeal a
misidentification.
The American people deserve the reassurance that such procedures are in place to protect their rights, opportunities,
and access. People make mistakes, and a human alternative or fallback mechanism will not always have the right
answer, but they serve as an important check on the power and validity of automated systems.
• An automated signature matching system is used as part of the voting process in many parts of the country to
determine whether the signature on a mail-in ballot matches the signature on file. These signature matching
systems are less likely to work correctly for some voters, including voters with mental or physical
disabilities, voters with shorter or hyphenated names, and voters who have changed their name.97 A human
curing process,98 which helps voters to confirm their signatures and correct other voting mistakes, is
important to ensure all votes are counted,99 and it is already standard practice in much of the country for
both an election official and the voter to have the opportunity to review and correct any such issues.100
47HUMAN ALTERNATIVES,
CONSIDERATION, AND
FALLBACK
W
HY THIS PRINCIPLE IS IMPORTANT
This section provides a brief summary of the problems which the principle seeks to address and protect
against, including illustrative examples.
• An unemployment benefits system in Colorado required, as a condition of accessing benefits, that applicants
have a smartphone in order to verify their identity. No alternative human option was readily available,
which denied many people access to benefits.101
• A fraud detection system for unemployment insurance distribution incorrectly flagged entries as fraudulent,
leading to people with slight discrepancies or complexities in their files having their wages withheld and tax
returns seized without any chance to explain themselves or receive a review by a person.102
• A patient was wrongly denied access to pain medication when the hospital’s software confused her medica-
tion history with that of her dog’s. Even after she tracked down an explanation for the problem, doctors
were afraid to override the system, and she was forced to go without pain relief due to the system’s error.103
• A large corporation automated performance evaluation and other HR functions, leading to workers being
fired by an automated system without the possibility of human review, appeal or other form of recourse.104
48HUMAN ALTERNATIVES,
CONSIDERATION, AND
FALLBACK
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
An automated system should provide demonstrably effective mechanisms to opt out in favor of a human alterna-
tive, where appropriate, as well as timely human consideration and remedy by a fallback system, with additional
human oversight and safeguards for systems used in sensitive domains, and with training and assessment for any
human-based portions of the system to ensure effectiveness.
Provide a mechanism to conveniently opt out from automated systems in favor of a human
alternative, where appropriate
Brief, clear, accessible notice and instructions. Those impacted by an automated system should be
given a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out.
Instructions should be provided in an accessible form and should be easily findable by those impacted by the
automated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g.,
via user experience research).
Human alternatives provided when appropriate. In many scenarios, there is a reasonable expectation
of human involvement in attaining rights, opportunities, or access. When automated systems make up part of
the attainment process, alternative timely human-driven processes should be provided. The use of a human
alternative should be triggered by an opt-out process.
Timely and not burdensome human alternative. Opting out should be timely and not unreasonably
burdensome in both the process of requesting to opt-out and the human-driven alternative provided.
Provide timely human consideration and remedy by a fallback and escalation system in the
event that an automated system fails, produces error, or you would like to appeal or con-
test its impacts on you
Proportionate. The availability of human consideration and fallback, along with associated training and
safeguards against human bias, should be proportionate to the potential of the automated system to meaning-
fully impact rights, opportunities, or access. Automated systems that have greater control over outcomes,
provide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to
meaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and over-
sight of human consideration and fallback mechanisms.
Accessible. Mechanisms for human consideration and fallback, whether in-person, on paper, by phone, or
otherwise provided, should be easy to find and use. These mechanisms should be tested to ensure that users
who have trouble with the automated system are able to use human consideration and fallback, with the under-
standing that it may be these users who are most likely to need the human assistance. Similarly, it should be
tested to ensure that users with disabilities are able to find and use human consideration and fallback and also
request reasonable accommodations or modifications.
Convenient. Mechanisms for human consideration and fallback should not be unreasonably burdensome as
compared to the automated system’s equivalent.
49HUMAN ALTERNATIVES,
CONSIDERATION, AND
FALLBACK
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Equitable. Consideration should be given to ensuring outcomes of the fallback and escalation system are
equitable when compared to those of the automated system and such that the fallback and escalation
system provides equitable access to underserved communities.105
Timely. Human consideration and fallback are only useful if they are conducted and concluded in a
timely manner. The determination of what is timely should be made relative to the specific automated
system, and the review system should be staffed and regularly assessed to ensure it is providing timely
consideration and fallback. In time-critical systems, this mechanism should be immediately available or,
where possible, available before the harm occurs. Time-critical systems include, but are not limited to,
voting-related systems, automated building access and other access systems, systems that form a critical
component of healthcare, and systems that have the ability to withhold wages or otherwise cause
immediate financial penalties.
Effective. The organizational structure surrounding processes for consideration and fallback should
be designed so that if the human decision-maker charged with reassessing a decision determines that it
should be overruled, the new decision will be effectively enacted. This includes ensuring that the new
decision is entered into the automated system throughout its components, any previous repercussions from
the old decision are also overturned, and safeguards are put in place to help ensure that future decisions do
not result in the same errors.
Maintained. The human consideration and fallback process and any associated automated processes
should be maintained and supported as long as the relevant automated system continues to be in use.
Institute training, assessment, and oversight to combat automation bias and ensure any
human-based components of a system are effective.
Training and assessment. Anyone administering, interacting with, or interpreting the outputs of an auto-
mated system should receive training in that system, including how to properly interpret outputs of a system
in light of its intended purpose and in how to mitigate the effects of automation bias. The training should reoc-
cur regularly to ensure it is up to date with the system and to ensure the system is used appropriately. Assess-
ment should be ongoing to ensure that the use of the system with human involvement provides for appropri-
ate results, i.e., that the involvement of people does not invalidate the system's assessment as safe and effective
or lead to algorithmic discrimination.
Oversight. Human-based systems have the potential for bias, including automation bias, as well as other
concerns that may limit their effectiveness. The results of assessments of the efficacy and potential bias of
such human-based systems should be overseen by governance structures that have the potential to update the
operation of the human-based system in order to mitigate these effects.
50HUMAN ALTERNATIVES,
CONSIDERATION, AND
FALLBACK
W
HAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS
The expectations for automated systems are meant to serve as a blueprint for the development of additional
technical standards and practices that are tailored for particular sectors and contexts.
Implement additional human oversight and safeguards for automated systems related to
sensitive domains
Automated systems used within sensitive domains, including criminal justice, employment, education, and
health, should meet the expectations laid out throughout this framework, especially avoiding capricious,
inappropriate, and discriminatory impacts of these technologies. Additionally, automated systems used within
sensitive domains should meet these expectations:
Narrowly scoped data and inferences. Human oversight should ensure that automated systems in
sensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attri-
bute as relevant to the specific use case. Data included should be carefully limited to avoid algorithmic
discrimination resulting from, e.g., use of community characteristics, social network analysis, or group-based
inferences.
Tailored to the situation. Human oversight should ensure that automated systems in sensitive domains
are tailored to the specific use case and real-world deployment scenario, and evaluation testing should show
that the system is safe and effective for that specific situation. Validation testing performed based on one loca-
tion or use case should not be assumed to transfer to another.
Human consideration before any high-risk decision. Automated systems, where they are used in
sensitive domains, may play a role in directly providing information or otherwise providing positive outcomes
to impacted people. However, automated systems should not be allowed to directly intervene in high-risk
situations, such as sentencing decisions or medical care, without human consideration.
Meaningful access to examine the system. Designers, developers, and deployers of automated
systems should consider limited waivers of confidentiality (including those related to trade secrets) where
necessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating mea-
sures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. This
includes (potentially private and protected) meaningful access to source code, documentation, and related
data during any associated legal discovery, subject to effective confidentiality or court orders. Such meaning-
ful access should include (but is not limited to) adhering to the principle on Notice and Explanation using the
highest level of risk so the system is designed with built-in explanations; such systems should use fully-trans-
parent models where the model itself can be understood by people needing to directly examine it.
Demonstrate access to human alternatives, consideration, and fallback
Reporting. Reporting should include an assessment of timeliness and the extent of additional burden for
human alternatives, aggregate statistics about who chooses the human alternative, along with the results of
the assessment about brevity, clarity, and accessibility of notice and opt-out instructions. Reporting on the
accessibility, timeliness, and effectiveness of human consideration and fallback should be made public at regu-
lar intervals for as long as the system is in use. This should include aggregated information about the number
and type of requests for consideration, fallback employed, and any repeated requests; the timeliness of the
handling of these requests, including mean wait times for different types of requests as well as maximum wait
times; and information about the procedures used to address requests for consideration along with the results
of the evaluation of their accessibility. For systems used in sensitive domains, reporting should include infor-
mation about training and governance procedures for these technologies. Reporting should also include docu-
mentation of goals and assessment of meeting those goals, consideration of data included, and documentation
of the governance of reasonable access to the technology. Reporting should be provided in a clear and
51
machine-readable manner.HUMAN ALTERNATIVES,
CONSIDERATION, AND
FALLBACK
H
OW THESE PRINCIPLES CAN MOVE INTO PRACTICE
Real-life examples of how these principles can become reality, through laws, policies, and practical
technical and sociotechnical approaches to protecting rights, opportunities, and access.
Healthcare “navigators” help people find their way through online signup forms to choose
and obtain healthcare. A Navigator is “an individual or organization that's trained and able to help
consumers, small businesses, and their employees as they look for health coverage options through the
Marketplace (a government web site), including completing eligibility and enrollment forms.”106 For
the 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could
“train and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensive
health coverage.”107
The customer service industry has successfully integrated automated services such as
chat-bots and AI-driven call response systems with escalation to a human support
team.108 Many businesses now use partially automated customer service platforms that help answer customer
questions and compile common problems for human agents to review. These integrated human-AI
systems allow companies to provide faster customer care while maintaining human agents to answer
calls or otherwise respond to complicated requests. Using both AI and human agents is viewed as key to
successful customer service.109
Ballot curing laws in at least 24 states require a fallback system that allows voters to
correct their ballot and have it counted in the case that a voter signature matching
algorithm incorrectly flags their ballot as invalid or there is another issue with their
ballot, and review by an election official does not rectify the problem. Some federal
courts have found that such cure procedures are constitutionally required.110 Ballot
curing processes vary among states, and include direct phone calls, emails, or mail contact by election
officials.111 Voters are asked to provide alternative information or a new signature to verify the validity of their
ballot.
52A
PPENDIX
Examples of Automated Systems
The below examples are meant to illustrate the breadth of automated systems that, insofar as they have the
potential to meaningfully impact rights, opportunities, or access to critical resources or services, should
be covered by the Blueprint for an AI Bill of Rights. These examples should not be construed to limit that
scope, which includes automated systems that may not yet exist, but which fall under these criteria.
Examples of automated systems for which the Blueprint for an AI Bill of Rights should be considered include
those that have the potential to meaningfully impact:
• Civil rights, civil liberties, or privacy, including but not limited to:
Speech-related systems such as automated content moderation tools;
Surveillance and criminal justice system algorithms such as risk assessments, predictive
policing, automated license plate readers, real-time facial recognition systems (especially
those used in public places or during protected activities like peaceful protests), social media
monitoring, and ankle monitoring devices;
Voting-related systems such as signature matching tools;
Systems with a potential privacy impact such as smart home systems and associated data,
systems that use or collect health-related data, systems that use or collect education-related
data, criminal justice system data, ad-targeting systems, and systems that perform big data
analytics in order to build profiles or infer personal information about individuals; and
Any system that has the meaningful potential to lead to algorithmic discrimination.
• Equal opportunities, including but not limited to:
Education-related systems such as algorithms that purport to detect student cheating or
plagiarism, admissions algorithms, online or virtual reality student monitoring systems,
projections of student progress or outcomes, algorithms that determine access to resources or
rograms, and surveillance of classes (whether online or in-person);
Housing-related systems such as tenant screening algorithms, automated valuation systems that
estimate the value of homes used in mortgage underwriting or home insurance, and automated
valuations from online aggregator websites; and
Employment-related systems such as workplace algorithms that inform all aspects of the terms
and conditions of employment including, but not limited to, pay or promotion, hiring or termina-
tion algorithms, virtual or augmented reality workplace training programs, and electronic work
place surveillance and management systems.
• Access to critical resources and services, including but not limited to:
Health and health insurance technologies such as medical AI systems and devices, AI-assisted
diagnostic tools, algorithms or predictive models used to support clinical decision making, medical
or insurance health risk assessments, drug addiction risk assessments and associated access alg
-orithms, wearable technologies, wellness apps, insurance care allocation algorithms, and health
insurance cost and underwriting algorithms;
Financial system algorithms such as loan allocation algorithms, financial system access determi-
nation algorithms, credit scoring systems, insurance algorithms including risk assessments, auto
-mated interest rate determinations, and financial algorithms that apply penalties (e.g., that can
garnish wages or withhold tax returns);
53A
PPENDIX
Systems that impact the safety of communities such as automated traffic control systems, elec
-ctrical grid controls, smart city technologies, and industrial emissions and environmental
impact control algorithms; and
Systems related to access to benefits or services or assignment of penalties such as systems that
support decision-makers who adjudicate benefits such as collating or analyzing information or
matching records, systems which similarly assist in the adjudication of administrative or criminal
penalties, fraud detection algorithms, services or benefits access control algorithms, biometric
systems used as access control, and systems which make benefits or services related decisions on a
fully or partially autonomous basis (such as a determination to revoke benefits).
54SECTION TITLE
A
PPENDIX
Listening to the American People
The White House Office of Science and Technology Policy (OSTP) led a yearlong process to seek and distill
input from people across the country – from impacted communities to industry stakeholders to
technology developers to other experts across fields and sectors, as well as policymakers across the Federal
government – on the issue of algorithmic and data-driven harms and potential remedies. Through panel
discussions, public listening sessions, private meetings, a formal request for information, and input to a
publicly accessible and widely-publicized email address, people across the United States spoke up about
both the promises and potential harms of these technologies, and played a central role in shaping the
Blueprint for an AI Bill of Rights.
Panel Discussions to Inform the Blueprint for An AI Bill of Rights
OSTP co-hosted a series of six panel discussions in collaboration with the Center for American Progress,
the Joint Center for Political and Economic Studies, New America, the German Marshall Fund, the Electronic
Privacy Information Center, and the Mozilla Foundation. The purpose of these convenings – recordings of
which are publicly available online112 – was to bring together a variety of experts, practitioners, advocates
and federal government officials to offer insights and analysis on the risks, harms, benefits, and
policy opportunities of automated systems. Each panel discussion was organized around a wide-ranging
theme, exploring current challenges and concerns and considering what an automated society that
respects democratic values should look like. These discussions focused on the topics of consumer
rights and protections, the criminal justice system, equal opportunities and civil justice, artificial
intelligence and democratic values, social welfare and development, and the healthcare system.
Summaries of Panel Discussions:
Panel 1: Consumer Rights and Protections. This event explored the opportunities and challenges for
individual consumers and communities in the context of a growing ecosystem of AI-enabled consumer
products, advanced platforms and services, “Internet of Things” (IoT) devices, and smart city products and
services.
Welcome:
• Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and
Technology Policy
• Karen Kornbluh, Senior Fellow and Director of the Digital Innovation and Democracy Initiative, German
Marshall Fund
Moderator:
Devin E. Willis, Attorney, Division of Privacy and Identity Protection, Bureau of Consumer Protection, Federal
Trade Commission
Panelists:
• Tamika L. Butler, Principal, Tamika L. Butler Consulting
• Jennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio
State University
• Carl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet
• Surya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup
• Mariah Montgomery, National Campaign Director, Partnership for Working Families
55A
PPENDIX
Panelists discussed the benefits of AI-enabled systems and their potential to build better and more
innovative infrastructure. They individually noted that while AI technologies may be new, the process of
technological diffusion is not, and that it was critical to have thoughtful and responsible development and
integration of technology within communities. Some panelists suggested that the integration of technology
could benefit from examining how technological diffusion has worked in the realm of urban planning:
lessons learned from successes and failures there include the importance of balancing ownership rights, use
rights, and community health, safety and welfare, as well ensuring better representation of all voices,
especially those traditionally marginalized by technological advances. Some panelists also raised the issue of
power structures – providing examples of how strong transparency requirements in smart city projects
helped to reshape power and give more voice to those lacking the financial or political power to effect change.
In discussion of technical and governance interventions that that are needed to protect against the harms
of these technologies, various panelists emphasized the need for transparency, data collection, and
flexible and reactive policy development, analogous to how software is continuously updated and deployed.
Some panelists pointed out that companies need clear guidelines to have a consistent environment for
innovation, with principles and guardrails being the key to fostering respon sible innovation.
Panel 2: The Criminal Justice System. This event explored current and emergent uses of technology in
the criminal justice system and considered how they advance or undermine public safety, justice, and
democratic values.
Welcome:
• Suresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science
and Technology Policy
• Ben Winters, Counsel, Electronic Privacy Information Center
Moderator: Chiraag Bains, Deputy Assistant to the President on Racial Justice & Equity
Panelists:
• Sean Malinowski, Director of Policing Innovation and Reform, University of Chicago Crime Lab
• Kristian Lum, Researcher
• Jumana Musa, Director, Fourth Amendment Center, National Association of Criminal Defense Lawyers
• Stanley Andrisse, Executive Director, From Prison Cells to PHD; Assistant Professor, Howard University
College of Medicine
• Myaisha Hayes, Campaign Strategies Director, MediaJustice
Panelists discussed uses of technology within the criminal justice system, including the use of predictive
policing, pretrial risk assessments, automated license plate readers, and prison communication tools. The
discussion emphasized that communities deserve safety, and strategies need to be identified that lead to safety;
such strategies might include data-driven approaches, but the focus on safety should be primary, and
technology may or may not be part of an effective set of mechanisms to achieve safety. Various panelists raised
concerns about the validity of these systems, the tendency of adverse or irrelevant data to lead to a replication of
unjust outcomes, and the confirmation bias and tendency of people to defer to potentially inaccurate automated
systems. Throughout, many of the panelists individually emphasized that the impact of these systems on
individuals and communities is potentially severe: the systems lack individualization and work against the
belief that people can change for the better, system use can lead to the loss of jobs and custody of children, and
surveillance can lead to chilling effects for communities and sends negative signals to community members
about how they're viewed.
In discussion of technical and governance interventions that that are needed to protect against the harms of
these technologies, various panelists emphasized that transparency is important but is not enough to achieve
accountability. Some panelists discussed their individual views on additional system needs for validity, and
agreed upon the importance of advisory boards and compensated community input early in the design process
(before the technology is built and instituted). Various panelists also emphasized the importance of regulation
that includes limits to the type and cost of such technologies.
56A
PPENDIX
Panel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of
technology that impact equity of opportunity in employment, education, and housing.
Welcome:
• Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and
Technology Policy
• Dominique Harrison, Director for Technology Policy, The Joint Center for Political and Economic
Studies
Moderator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor
Panelists:
• Christo Wilson, Associate Professor of Computer Science, Northeastern University
• Frida Polli, CEO, Pymetrics
• Karen Levy, Assistant Professor, Department of Information Science, Cornell University
• Natasha Duarte, Project Director, Upturn
• Elana Zeide, Assistant Professor, University of Nebraska College of Law
• Fabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community
Advocate and Floor Captain, Atlantic Plaza Towers Tenants Association
The individual panelists described the ways in which AI systems and other technologies are increasingly being
used to limit access to equal opportunities in education, housing, and employment. Education-related
concerning uses included the increased use of remote proctoring systems, student location and facial
recognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses
including automated tenant background screening and facial recognition-based controls to enter or exit
housing complexes. Employment-related concerning uses included discrimination in automated hiring
screening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key
concern, pointing out that students should be able to reinvent themselves and require privacy of their student
records and education-related data in order to do so. The overarching concerns of surveillance in these
domains included concerns about the chilling effects of surveillance on student expression, inappropriate
control of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work
and life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists
pointed out ways that data from one situation was misapplied in another in a way that limited people's
opportunities, for example data from criminal justice settings or previous evictions being used to block further
access to housing. Throughout, various panelists emphasized that these technologies are being used to shift the
burden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in
ways that diminish and encroach on equality of opportunity; assessment of these technologies should include
whether they are genuinely helpful in solving an identified problem.
In discussion of technical and governance interventions that that are needed to protect against the harms of
these technologies, panelists individually described the importance of: receiving community input into the
design and use of technologies, public reporting on crucial elements of these systems, better notice and consent
procedures that ensure privacy based on context and use case, ability to opt-out of using these systems and
receive a fallback to a human process, providing explanations of decisions and how these systems work, the
need for governance including training in using these systems, ensuring the technological use cases are
genuinely related to the goal task and are locally validated to work, and the need for institution and protection
of third party audits to ensure systems continue to be accountable and valid.
57A
PPENDIX
Panel 4: Artificial Intelligence and Democratic Values. This event examined challenges and opportunities in
the design of technology that can help support a democratic vision for AI. It included discussion of the
technical aspects of designing non-discriminatory technology, explainable AI, human-computer
interaction with an emphasis on community participation, and privacy-aware design.
Welcome:
• Sorelle Friedler, Assistant Director for Data and Democracy, White House Office of Science and
Technology Policy
• J. Bob Alotta, Vice President for Global Programs, Mozilla Foundation
• Navrina Singh, Board Member, Mozilla Foundation
Moderator: Kathy Pham Evans, Deputy Chief Technology Officer for Product and Engineering, U.S
Federal Trade Commission.
Panelists:
• Liz O’Sullivan, CEO, Parity AI
• Timnit Gebru, Independent Scholar
• Jennifer Wortman Vaughan, Senior Principal Researcher, Microsoft Research, New York City
• Pamela Wisniewski, Associate Professor of Computer Science, University of Central Florida; Director,
Socio-technical Interaction Research (STIR) Lab
• Seny Kamara, Associate Professor of Computer Science, Brown University
Each panelist individually emphasized the risks of using AI in high-stakes settings, including the potential for
biased data and discriminatory outcomes, opaque decision-making processes, and lack of public trust and
understanding of the algorithmic systems. The interventions and key needs various panelists put forward as
necessary to the future design of critical AI systems included ongoing transparency, value sensitive and
participatory design, explanations designed for relevant stakeholders, and public consultation. Various
panelists emphasized the importance of placing trust in people, not technologies, and in engaging with
impacted communities to understand the potential harms of technologies and build protection by design into
future systems.
Panel 5: Social Welfare and Development. This event explored current and emerging uses of technology to
implement or improve social welfare systems, social development programs, and other systems that can impact
life chances.
Welcome:
• Suresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science
and Technology Policy
• Anne-Marie Slaughter, CEO, New America
Moderator: Michele Evermore, Deputy Director for Policy, Office of Unemployment Insurance
Modernization, Office of the Secretary, Department of Labor
Panelists:
• Blake Hall, CEO and Founder, ID.Me
• Karrie Karahalios, Professor of Computer Science, University of Illinois, Urbana-Champaign
• Christiaan van Veen, Director of Digital Welfare State and Human Rights Project, NYU School of Law's
Center for Human Rights and Global Justice
58A
PPENDIX
• Julia Simon-Mishel, Supervising Attorney, Philadelphia Legal Assistance
• Dr. Zachary Mahafza, Research & Data Analyst, Southern Poverty Law Center
• J. Khadijah Abdurahman, Tech Impact Network Research Fellow, AI Now Institute, UCLA C2I1, and
UWA Law School
Panelists sep arately described the increasing scope of technology use in providing for social welfare, including
in fraud detection, digital ID systems, and other methods focused on improving efficiency and reducing cost.
However, various panelists individually cautioned that these systems may reduce burden for government
agencies by increasing the burden and agency of people using and interacting with these technologies.
Additionally, these systems can produce feedback loops and compounded harm, collecting data from
communities and using it to reinforce inequality. Various panelists suggested that these harms could be
mitigated by ensuring community input at the beginning of the design process, providing ways to opt out of
these systems and use associated human-driven mechanisms instead, ensuring timeliness of benefit payments,
and providing clear notice about the use of these systems and clear explanations of how and what the
technologies are doing. Some panelists suggested that technology should be used to help people receive
benefits, e.g., by pushing benefits to those in need and ensuring automated decision-making systems are only
used to provide a positive outcome; technology shouldn't be used to take supports away from people who need
them.
Panel 6: The Healthcare System. This event explored current and emerging uses of technology in the
healthcare system and consumer products related to health.
Welcome:
• Alondra Nelson, Deputy Director for Science and Society, White House Office of Science and Technology
Policy
• Patrick Gaspard, President and CEO, Center for American Progress
Moderator: Micky Tripathi, National Coordinator for Health Information Technology, U.S Department of
Health and Human Services.
Panelists:
• Mark Schneider, Health Innovation Advisor, ChristianaCare
• Ziad Obermeyer, Blue Cross of California Distinguished Associate Professor of Policy and Management,
University of California, Berkeley School of Public Health
• Dorothy Roberts, George A. Weiss University Professor of Law and Sociology and the Raymond Pace and
Sadie Tanner Mossell Alexander Professor of Civil Rights, University of Pennsylvania
• David Jones, A. Bernard Ackerman Professor of the Culture of Medicine, Harvard University
• Jamila Michener, Associate Professor of Government, Cornell University; Co-Director, Cornell Center for
Health Equity
Panelists discussed the impact of new technologies on health disparities; healthcare access, delivery, and
outcomes; and areas ripe for research and policymaking. Panelists discussed the increasing importance of tech-
nology as both a vehicle to deliver healthcare and a tool to enhance the quality of care. On the issue of
delivery, various panelists pointed to a number of concerns including access to and expense of broadband
service, the privacy concerns associated with telehealth systems, the expense associated with health
monitoring devices, and how this can exacerbate equity issues. On the issue of technology enhanced care,
some panelists spoke extensively about the way in which racial biases and the use of race in medicine
perpetuate harms and embed prior discrimination, and the importance of ensuring that the technologies used
in medical care were accountable to the relevant stakeholders. Various panelists emphasized the importance
of having the voices of those subjected to these technologies be heard.
59A
PPENDIX
Summaries of Additional Engagements:
• OSTP created an email address (ai-equity@ostp.eop.gov) to solicit comments from the public on the use of
artificial intelligence and other data-driven technologies in their lives.
• OSTP issued a Request For Information (RFI) on the use and governance of biometric technologies.113 The
purpose of this RFI was to understand the extent and variety of biometric technologies in past, current, or
planned use; the domains in which these technologies are being used; the entities making use of them; current
principles, practices, or policies governing their use; and the stakeholders that are, or may be, impacted by their
use or regulation. The 130 responses to this RFI are available in full online114 and were submitted by the below
listed organizations and individuals:
Accenture Cisco Systems Google
Access Now City of Portland Smart City PDX Health Information Technology
ACT | The App Association Program Research and Development
AHIP CLEAR Interagency Working Group
AIethicist.org Clearview AI HireVue
Airlines for America Cognoa HR Policy Association
Alliance for Automotive Innovation Color of Change ID.me
Amelia Winger-Bearskin Common Sense Media Identity and Data Sciences
American Civil Liberties Union Computing Community Consortium Laboratory at Science Applications
American Civil Liberties Union of at Computing Research Association International Corporation
Massachusetts Connected Health Initiative Information Technology and
American Medical Association Consumer Technology Association Innovation Foundation
ARTICLE19 Courtney Radsch Information Technology Industry
Attorneys General of the District of Coworker Council
Columbia, Illinois, Maryland, Cyber Farm Labs Innocence Project
Michigan, Minnesota, New York, Data & Society Research Institute Institute for Human-Centered
North Carolina, Oregon, Vermont, Data for Black Lives Artificial Intelligence at Stanford
and Washington Data to Actionable Knowledge Lab University
Avanade at Harvard University Integrated Justice Information
Aware Deloitte Systems Institute
Barbara Evans Dev Technology Group International Association of Chiefs
Better Identity Coalition Digital Therapeutics Alliance of Police
Bipartisan Policy Center Digital Welfare State & Human International Biometrics + Identity
Brandon L. Garrett and Cynthia Rights Project and Center for Association
Rudin Human Rights and Global Justice at International Business Machines
Brian Krupp New York University School of Corporation
Brooklyn Defender Services Law, and Temple University International Committee of the Red
BSA | The Software Alliance Institute for Law, Innovation & Cross
Carnegie Mellon University Technology Inventionphysics
Center for Democracy & Dignari iProov
Technology Douglas Goddard Jacob Boudreau
Center for New Democratic Edgar Dworsky Jennifer K. Wagner, Dan Berger,
Processes Electronic Frontier Foundation Margaret Hu, and Sara Katsanis
Center for Research and Education Electronic Privacy Information Jonathan Barry-Blocker
on Accessible Technology and Center, Center for Digital Joseph Turow
Experiences at University of Democracy, and Consumer Joy Buolamwini
Washington, Devva Kasnitz, L Jean Federation of America Joy Mack
Camp, Jonathan Lazar, Harry FaceTec Karen Bureau
Hochheiser Fight for the Future Lamont Gholston
Center on Privacy & Technology at Ganesh Mani Lawyers’ Committee for Civil
Georgetown Law Georgia Tech Research Institute Rights Under Law
60A
PPENDIX
Lisa Feldman Barrett Security Industry Association
Madeline Owens Sheila Dean
Marsha Tudor Software & Information Industry
Microsoft Corporation Association
MITRE Corporation Stephanie Dinkins and the Future
National Association for the Histories Studio at Stony Brook
Advancement of Colored People University
Legal Defense and Educational TechNet
Fund The Alliance for Media Arts and
National Association of Criminal Culture, MIT Open Documentary
Defense Lawyers Lab and Co-Creation Studio, and
National Center for Missing & Immerse
Exploited Children The International Brotherhood of
National Fair Housing Alliance Teamsters
National Immigration Law Center The Leadership Conference on
NEC Corporation of America Civil and Human Rights
New America’s Open Technology Thorn
Institute U.S. Chamber of Commerce’s
New York Civil Liberties Union Technology Engagement Center
No Name Provided Uber Technologies
Notre Dame Technology Ethics University of Pittsburgh
Center Undergraduate Student
Office of the Ohio Public Defender Collaborative
Onfido Upturn
Oosto US Technology Policy Committee
Orissa Rose of the Association of Computing
Palantir Machinery
Pangiam Virginia Puccio
Parity Technologies Visar Berisha and Julie Liss
Patrick A. Stewart, Jeffrey K. XR Association
Mullins, and Thomas J. Greitens XR Safety Initiative
Pel Abbott
Philadelphia Unemployment
Project
Project On Government Oversight
Recording Industry Association of
America
Robert Wilkens
Ron Hedges
Science, Technology, and Public
Policy Program at University of
Michigan Ann Arbor
• As an additional effort to reach out to stakeholders regarding the RFI, OSTP conducted two listening sessions
for members of the public. The listening sessions together drew upwards of 300 participants. The Science and
Technology Policy Institute produced a synopsis of both the RFI submissions and the feedback at the listening
sessions.115
61A
PPENDIX
•OSTP conducted meetings with a variety of stakeholders in the private sector and civil society. Some of these
meetings were specifically focused on providing ideas related to the development of the Blueprint for an AI
Bill of Rights while others provided useful general context on the positive use cases, potential harms, and/or
oversight possibilities for these technologies. Participants in these conversations from the private sector and
civil society included:
Adobe Movement Alliance Project
American Civil Liberties Union The National Association of
(ACLU) Criminal Defense Lawyers
The Aspen Commission on O’Neil Risk Consulting &
Information Disorder Algorithmic Auditing
The Awood Center The Partnership on AI
The Australian Human Rights Pinterest
Commission The Plaintext Group
Biometrics Institute pymetrics
The Brookings Institute SAP
BSA | The Software Alliance The Security Industry Association
Cantellus Group Software and Information Industry
Center for American Progress Association (SIIA)
Center for Democracy and Special Competitive Studies Project
Technology Thorn
Center on Privacy and Technology United for Respect
at Georgetown Law University of California at Berkeley
Christiana Care Citris Policy Lab
Color of Change University of California at Berkeley
Coworker Labor Center
Data Robot Unfinished/Project Liberty
Data Trust Alliance Upturn
Data and Society Research Institute US Chamber of Commerce
Deepmind US Chamber of Commerce
EdSAFE AI Alliance Technology Engagement Center
Electronic Privacy Information A.I. Working Group
Center (EPIC) Vibrent Health
Encode Justice Warehouse Worker Resource
Equal AI Center
Google Waymap
Hitachi's AI Policy Committee
The Innocence Project
Institute of Electrical and
Electronics Engineers (IEEE)
Intuit
Lawyers Committee for Civil Rights
Under Law
Legal Aid Society
The Leadership Conference on
Civil and Human Rights
Meta
Microsoft
The MIT AI Policy Forum
62E
NDNOTES
1.The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the
Federal Government. https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive
order-advancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/
2. The White House. Remarks by President Biden on the Supreme Court Decision to Overturn Roe v. Wade. Jun.
24, 2022. https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/06/24/remarks-by-president-
biden-on-the-supreme-court-decision-to-overturn-roe-v-wade/
3. The White House. Join the Effort to Create A Bill of Rights for an Automated Society. Nov. 10, 2021. https://
www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-rights-for-an-
automated-society/
4. U.S. Dept. of Health, Educ. & Welfare, Report of the Sec’y’s Advisory Comm. on Automated Pers. Data Sys.,
Records, Computers, and the Rights of Citizens (July 1973). https://www.justice.gov/opcl/docs/rec-com-
rights.pdf.
5. See, e.g., Office of Mgmt. & Budget, Exec. Office of the President, Circular A-130, Managing Information as a
Strategic Resource, app. II § 3 (July 28, 2016); Org. of Econ. Co-Operation & Dev., Revision of the
Recommendation of the Council Concerning Guidelines Governing the Protection of Privacy and Transborder
Flows of Personal Data, Annex Part Two (June 20, 2013). https://one.oecd.org/document/C(2013)79/en/pdf.
6. Andrew Wong et al. External validation of a widely implemented proprietary sepsis prediction model in
hospitalized patients. JAMA Intern Med. 2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626
7. Jessica Guynn. Facebook while black: Users call it getting 'Zucked,' say talking about racism is censored as hate
speech. USA Today. Apr. 24, 2019. https://www.usatoday.com/story/news/2019/04/24/facebook-while-black-
zucked-users-say-they-get-blocked-racism-discussion/2859593002/
8. See, e.g., Michael Levitt. AirTags are being used to track people and cars. Here's what is being done about it.
NPR. Feb. 18, 2022. https://www.npr.org/2022/02/18/1080944193/apple-airtags-theft-stalking-privacy-tech;
Samantha Cole. Police Records Show Women Are Being Stalked With Apple AirTags Across the Country.
Motherboard. Apr. 6, 2022. https://www.vice.com/en/article/y3vj3y/apple-airtags-police-reports-stalking-
harassment
9. Kristian Lum and William Isaac. To Predict and Serve? Significance. Vol. 13, No. 5, p. 14-19. Oct. 7, 2016.
https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x; Aaron Sankin, Dhruv Mehrotra,
Surya Mattu, and Annie Gilbertson. Crime Prediction Software Promised to Be Free of Biases. New Data Shows
It Perpetuates Them. The Markup and Gizmodo. Dec. 2, 2021. https://themarkup.org/prediction-
bias/2021/12/02/crime-prediction-software-promised-to-be-free-of-biases-new-data-shows-it-perpetuates-
them
10. Samantha Cole. This Horrifying App Undresses a Photo of Any Woman With a Single Click. Motherboard.
June 26, 2019. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman
11. Lauren Kaori Gurley. Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make.
Motherboard. Sep. 20, 2021. https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-
drivers-for-mistakes-they-didnt-make
63E
NDNOTES
12. Expectations about reporting are intended for the entity developing or using the automated system. The
resulting reports can be provided to the public, regulators, auditors, industry standards groups, or others
engaged in independent review, and should be made public as much as possible consistent with law,
regulation, and policy, and noting that intellectual property or law enforcement considerations may prevent
public release. These reporting expectations are important for transparency, so the American people can
have confidence that their rights, opportunities, and access as well as their expectations around
technologies are respected.
13. National Artificial Intelligence Initiative Office. Agency Inventories of AI Use Cases. Accessed Sept. 8,
2022. https://www.ai.gov/ai-use-case-inventories/
14. National Highway Traffic Safety Administration. https://www.nhtsa.gov/
15. See, e.g., Charles Pruitt. People Doing What They Do Best: The Professional Engineers and NHTSA. Public
Administration Review. Vol. 39, No. 4. Jul.-Aug., 1979. https://www.jstor.org/stable/976213?seq=1
16. The US Department of Transportation has publicly described the health and other benefits of these
“traffic calming” measures. See, e.g.: U.S. Department of Transportation. Traffic Calming to Slow Vehicle
Speeds. Accessed Apr. 17, 2022. https://www.transportation.gov/mission/health/Traffic-Calming-to-Slow-
Vehicle-Speeds
17. Karen Hao. Worried about your firm’s AI ethics? These startups are here to help.
A growing ecosystem of “responsible AI” ventures promise to help organizations monitor and fix their AI
models. MIT Technology Review. Jan 15., 2021.
https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top Progressive
Companies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021. https://
www.analyticsinsight.net/top-progressive-companies-building-ethical-ai-to-look-out-for-
in-2021/ https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top
Progressive Companies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021.
18. Office of Management and Budget. Study to Identify Methods to Assess Equity: Report to the President.
Aug. 2021. https://www.whitehouse.gov/wp-content/uploads/2021/08/OMB-Report-on-E013985-
Implementation_508-Compliant-Secure-v1.1.pdf
19. National Institute of Standards and Technology. AI Risk Management Framework. Accessed May 23,
2022. https://www.nist.gov/itl/ai-risk-management-framework
20. U.S. Department of Energy. U.S. Department of Energy Establishes Artificial Intelligence Advancement
Council. U.S. Department of Energy Artificial Intelligence and Technology Office. April 18, 2022. https://
www.energy.gov/ai/articles/us-department-energy-establishes-artificial-intelligence-advancement-council
21. Department of Defense. U.S Department of Defense Responsible Artificial Intelligence Strategy and
Implementation Pathway. Jun. 2022. https://media.defense.gov/2022/Jun/22/2003022604/-1/-1/0/
Department-of-Defense-Responsible-Artificial-Intelligence-Strategy-and-Implementation-
Pathway.PDF
22. Director of National Intelligence. Principles of Artificial Intelligence Ethics for the Intelligence
Community. https://www.dni.gov/index.php/features/2763-principles-of-artificial-intelligence-ethics-for-
the-intelligence-community
64E
NDNOTES
23. National Science Foundation. National Artificial Intelligence Research Institutes. Accessed Sept. 12,
2022. https://beta.nsf.gov/funding/opportunities/national-artificial-intelligence-research-institutes
24. National Science Foundation. Cyber-Physical Systems. Accessed Sept. 12, 2022. https://beta.nsf.gov/
funding/opportunities/cyber-physical-systems-cps
25. National Science Foundation. Secure and Trustworthy Cyberspace. Accessed Sept. 12, 2022. https://
beta.nsf.gov/funding/opportunities/secure-and-trustworthy-cyberspace-satc
26. National Science Foundation. Formal Methods in the Field. Accessed Sept. 12, 2022. https://
beta.nsf.gov/funding/opportunities/formal-methods-field-fmitf
27. National Science Foundation. Designing Accountable Software Systems. Accessed Sept. 12, 2022.
https://beta.nsf.gov/funding/opportunities/designing-accountable-software-systems-dass
28. The Leadership Conference Education Fund. The Use Of Pretrial “Risk Assessment” Instruments: A
Shared Statement Of Civil Rights Concerns. Jul. 30, 2018. http://civilrightsdocs.info/pdf/criminal-justice/
Pretrial-Risk-Assessment-Short.pdf; https://civilrights.org/edfund/pretrial-risk-assessments/
29. Idaho Legislature. House Bill 118. Jul. 1, 2019. https://legislature.idaho.gov/sessioninfo/2019/
legislation/H0118/
30. See, e.g., Executive Office of the President. Big Data: A Report on Algorithmic Systems, Opportunity, and
Civil Rights. May, 2016. https://obamawhitehouse.archives.gov/sites/default/files/microsites/
ostp/2016_0504_data_discrimination.pdf; Cathy O’Neil. Weapons of Math Destruction. Penguin Books.
2017. https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction; Ruha Benjamin. Race After
Technology: Abolitionist Tools for the New Jim Code. Polity. 2019. https://www.ruhabenjamin.com/race-
after-technology
31.See, e.g., Kashmir Hill. Another Arrest, and Jail Time, Due to a Bad Facial Recognition Match: A New
Jersey man was accused of shoplifting and trying to hit an officer with a car. He is the third known Black man
to be wrongfully arrested based on face recognition. New York Times. Dec. 29, 2020, updated Jan. 6, 2021.
https://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html; Khari
Johnson. How Wrongful Arrests Based on AI Derailed 3 Men's Lives. Wired. Mar. 7, 2022. https://
www.wired.com/story/wrongful-arrests-ai-derailed-3-mens-lives/
32. Student Borrower Protection Center. Educational Redlining. Student Borrower Protection Center
Report. Feb. 2020. https://protectborrowers.org/wp-content/uploads/2020/02/Education-Redlining-
Report.pdf
33. Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Oct.
10, 2018. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-
secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G
34. Todd Feathers. Major Universities Are Using Race as a “High Impact Predictor” of Student Success:
Students, professors, and education experts worry that that’s pushing Black students in particular out of math
and science. The Markup. Mar. 2, 2021. https://themarkup.org/machine-learning/2021/03/02/major-
universities-are-using-race-as-a-high-impact-predictor-of-student-success
65E
NDNOTES
35. Carrie Johnson. Flaws plague a tool meant to help low-risk federal prisoners win early release. NPR.
Jan. 26, 2022. https://www.npr.org/2022/01/26/1075509175/flaws-plague-a-tool-meant-to-help-low-
risk-federal-prisoners-win-early-release.; Carrie Johnson. Justice Department works to curb racial bias
in deciding who's released from prison. NPR. Apr. 19, 2022. https://
www.npr.org/2022/04/19/1093538706/justice-department-works-to-curb-racial-bias-in-deciding-
whos-released-from-pris; National Institute of Justice. 2021 Review and Revalidation of the First Step Act
Risk Assessment Tool. National Institute of Justice NCJ 303859. Dec., 2021. https://www.ojp.gov/
pdffiles1/nij/303859.pdf
36. Andrew Thompson. Google’s Sentiment Analyzer Thinks Being Gay Is Bad. Vice. Oct. 25, 2017. https://
www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias
37. Kaggle. Jigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of
conversations. 2019. https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification
38. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and
Mitigating Unintended Bias in Text Classification. Proceedings of AAAI/ACM Conference on AI, Ethics,
and Society. Feb. 2-3, 2018. https://dl.acm.org/doi/pdf/10.1145/3278721.3278729
39. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,
2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina-
teenager-2022-03-30/
40. Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.
Feb. 2018. https://nyupress.org/9781479837243/algorithms-of-oppression/
41. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,
2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina-
teenager-2022-03-30/
42. Miranda Bogen. All the Ways Hiring Algorithms Can Introduce Bias. Harvard Business Review. May
6, 2019. https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias
43. Arli Christian. Four Ways the TSA Is Making Flying Easier for Transgender People. American Civil
Liberties Union. Apr. 5, 2022. https://www.aclu.org/news/lgbtq-rights/four-ways-the-tsa-is-making-
flying-easier-for-transgender-people
44. U.S. Transportation Security Administration. Transgender/ Non Binary / Gender Nonconforming
Passengers. TSA. Accessed Apr. 21, 2022. https://www.tsa.gov/transgender-passengers
45. See, e.g., National Disabled Law Students Association. Report on Concerns Regarding Online
Administration of Bar Exams. Jul. 29, 2020. https://ndlsa.org/wp-content/uploads/2020/08/
NDLSA_Online-Exam-Concerns-Report1.pdf; Lydia X. Z. Brown. How Automated Test Proctoring
Software Discriminates Against Disabled Students. Center for Democracy and Technology. Nov. 16, 2020.
https://cdt.org/insights/how-automated-test-proctoring-software-discriminates-against-disabled-
students/
46. Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of
populations, 366 Science (2019), https://www.science.org/doi/10.1126/science.aax2342.
66E
NDNOTES
47. Darshali A. Vyas et al., Hidden in Plain Sight – Reconsidering the Use of Race Correction in Clinical
Algorithms, 383 N. Engl. J. Med.874, 876-78 (Aug. 27, 2020), https://www.nejm.org/doi/full/10.1056/
NEJMms2004740.
48. The definitions of 'equity' and 'underserved communities' can be found in the Definitions section of
this framework as well as in Section 2 of The Executive Order On Advancing Racial Equity and Support
for Underserved Communities Through the Federal Government. https://www.whitehouse.gov/
briefing-room/presidential-actions/2021/01/20/executive-order-advancing-racial-equity-and-support-
for-underserved-communities-through-the-federal-government/
49. Id.
50. Various organizations have offered proposals for how such assessments might be designed. See, e.g.,
Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf.
Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. Data & Society
Research Institute Report. June 29, 2021. https://datasociety.net/library/assembling-accountability-
algorithmic-impact-assessment-for-the-public-interest/; Nicol Turner Lee, Paul Resnick, and Genie
Barton. Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms.
Brookings Report. May 22, 2019.
https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-
policies-to-reduce-consumer-harms/; Andrew D. Selbst. An Institutional View Of Algorithmic Impact
Assessments. Harvard Journal of Law & Technology. June 15, 2021. https://ssrn.com/abstract=3867634;
Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker. Algorithmic Impact
Assessments: A Practical Framework for Public Agency Accountability. AI Now Institute Report. April
2018. https://ainowinstitute.org/aiareport2018.pdf
51. Department of Justice. Justice Department Announces New Initiative to Combat Redlining. Oct. 22,
2021. https://www.justice.gov/opa/pr/justice-department-announces-new-initiative-combat-redlining
52. PAVE Interagency Task Force on Property Appraisal and Valuation Equity. Action Plan to Advance
Property Appraisal and Valuation Equity: Closing the Racial Wealth Gap by Addressing Mis-valuations for
Families and Communities of Color. March 2022. https://pave.hud.gov/sites/pave.hud.gov/files/
documents/PAVEActionPlan.pdf
53. U.S. Equal Employment Opportunity Commission. The Americans with Disabilities Act and the Use of
Software, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees. EEOC-
NVTA-2022-2. May 12, 2022. https://www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use-
software-algorithms-and-artificial-intelligence; U.S. Department of Justice. Algorithms, Artificial
Intelligence, and Disability Discrimination in Hiring. May 12, 2022. https://beta.ada.gov/resources/ai-
guidance/
54. Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in
an algorithm used to manage the health of populations. Science. Vol. 366, No. 6464. Oct. 25, 2019. https://
www.science.org/doi/10.1126/science.aax2342
55. Data & Trust Alliance. Algorithmic Bias Safeguards for Workforce: Overview. Jan. 2022. https://
dataandtrustalliance.org/Algorithmic_Bias_Safeguards_for_Workforce_Overview.pdf
56. Section 508.gov. IT Accessibility Laws and Policies. Access Board. https://www.section508.gov/
manage/laws-and-policies/
67E
NDNOTES
57. ISO Technical Management Board. ISO/IEC Guide 71:2014. Guide for addressing accessibility in
standards. International Standards Organization. 2021. https://www.iso.org/standard/57385.html
58. World Wide Web Consortium. Web Content Accessibility Guidelines (WCAG) 2.0. Dec. 11, 2008.
https://www.w3.org/TR/WCAG20/
59. Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, and Andrew Bert. NIST Special
Publication 1270: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. The
National Institute of Standards and Technology. March, 2022. https://nvlpubs.nist.gov/nistpubs/
SpecialPublications/NIST.SP.1270.pdf
60. See, e.g., the 2014 Federal Trade Commission report “Data Brokers A Call for Transparency and
Accountability”. https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-
accountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf
61. See, e.g., Nir Kshetri. School surveillance of students via laptops may do more harm than good. The
Conversation. Jan. 21, 2022.
https://theconversation.com/school-surveillance-of-students-via-laptops-may-do-more-harm-than-
good-170983; Matt Scherer. Warning: Bossware May be Hazardous to Your Health. Center for Democracy
& Technology Report.
https://cdt.org/wp-content/uploads/2021/07/2021-07-29-Warning-Bossware-May-Be-Hazardous-To-
Your-Health-Final.pdf; Human Impact Partners and WWRC. The Public Health Crisis Hidden in Amazon
Warehouses. HIP and WWRC report. Jan. 2021.
https://humanimpact.org/wp-content/uploads/2021/01/The-Public-Health-Crisis-Hidden-In-Amazon-
Warehouses-HIP-WWRC-01-21.pdf; Drew Harwell. Contract lawyers face a growing invasion of
surveillance programs that monitor their work. The Washington Post. Nov. 11, 2021. https://
www.washingtonpost.com/technology/2021/11/11/lawyer-facial-recognition-monitoring/;
Virginia Doellgast and Sean O'Brady. Making Call Center Jobs Better: The Relationship between
Management Practices and Worker Stress. A Report for the CWA. June 2020. https://
hdl.handle.net/1813/74307
62. See, e.g., Federal Trade Commission. Data Brokers: A Call for Transparency and Accountability. May
2014.
https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability-
report-federal-trade-commission-may-2014/140527databrokerreport.pdf; Cathy O’Neil.
Weapons of Math Destruction. Penguin Books. 2017.
https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction
63. See, e.g., Rachel Levinson-Waldman, Harsha Pandurnga, and Faiza Patel. Social Media Surveillance by
the U.S. Government. Brennan Center for Justice. Jan. 7, 2022.
https://www.brennancenter.org/our-work/research-reports/social-media-surveillance-us-government;
Shoshana Zuboff. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of
Power. Public Affairs. 2019.
64. Angela Chen. Why the Future of Life Insurance May Depend on Your Online Presence. The Verge. Feb.
7, 2019.
https://www.theverge.com/2019/2/7/18211890/social-media-life-insurance-new-york-algorithms-big-
data-discrimination-online-records
6865. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info
Appears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://
www.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles-
in-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server. WIRED,
Nov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/
66.Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash. New York Times.
Sept. 24, 2019.
https://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html
67. Jo Constantz. ‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust
Unions. Newsweek. Dec. 13, 2021.
https://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust-
unions-1658603
68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum
(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and
against Weight Watchers and their subsidiary Kurbo
(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)
69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109
(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g), Children's Online
Privacy Protection Act of 1998, 15 U.S.C. 6501–6505, and Confidential Information Protection and
Statistical Efficiency Act (CIPSEA) (116 Stat. 2899)
70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.
21, 2018.
https://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true
71. Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.
https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html
72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology
Schools are Using to Monitor Students. ProPublica. Jun. 25, 2019.
https://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology-
schools-are-using-to-monitor-students/
73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are
fighting back. Washington Post. Nov. 12, 2020.
https://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/
74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government
Technology. May 24, 2022.
https://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;
Lydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability
Discrimination In New Surveillance Technologies: How new surveillance technologies in education,
policing, health care, and the workplace disproportionately harm disabled people. Center for Democracy
and Technology Report. May 24, 2022.
https://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how-
new-surveillance-technologies-in-education-policing-health-care-and-the-workplace-
disproportionately-harm-disabled-people/
69E
NDNOTES
75. See., e.g., Sam Sabin. Digital surveillance in a post-Roe world. Politico. May 5, 2022. https://
www.politico.com/newsletters/digital-future-daily/2022/05/05/digital-surveillance-in-a-post-roe-
world-00030459; Federal Trade Commission. FTC Sues Kochava for Selling Data that Tracks People at
Reproductive Health Clinics, Places of Worship, and Other Sensitive Locations. Aug. 29, 2022. https://
www.ftc.gov/news-events/news/press-releases/2022/08/ftc-sues-kochava-selling-data-tracks-people-
reproductive-health-clinics-places-worship-other
76. Todd Feathers. This Private Equity Firm Is Amassing Companies That Collect Data on America’s
Children. The Markup. Jan. 11, 2022.
https://themarkup.org/machine-learning/2022/01/11/this-private-equity-firm-is-amassing-companies-
that-collect-data-on-americas-children
77. Reed Albergotti. Every employee who leaves Apple becomes an ‘associate’: In job databases used by
employers to verify resume information, every former Apple employee’s title gets erased and replaced with
a generic title. The Washington Post. Feb. 10, 2022.
https://www.washingtonpost.com/technology/2022/02/10/apple-associate/
78. National Institute of Standards and Technology. Privacy Framework Perspectives and Success
Stories. Accessed May 2, 2022.
https://www.nist.gov/privacy-framework/getting-started-0/perspectives-and-success-stories
79. ACLU of New York. What You Need to Know About New York’s Temporary Ban on Facial
Recognition in Schools. Accessed May 2, 2022.
https://www.nyclu.org/en/publications/what-you-need-know-about-new-yorks-temporary-ban-facial-
recognition-schools
80. New York State Assembly. Amendment to Education Law. Enacted Dec. 22, 2020.
https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=S05140&term=2019&Summary=Y&Text=Y
81. U.S Department of Labor. Labor-Management Reporting and Disclosure Act of 1959, As Amended.
https://www.dol.gov/agencies/olms/laws/labor-management-reporting-and-disclosure-act (Section
203). See also: U.S Department of Labor. Form LM-10. OLMS Fact Sheet, Accessed May 2, 2022. https://
www.dol.gov/sites/dolgov/files/OLMS/regs/compliance/LM-10_factsheet.pdf
82. See, e.g., Apple. Protecting the User’s Privacy. Accessed May 2, 2022.
https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy; Google Developers.
Design for Safety: Android is secure by default and private by design. Accessed May 3, 2022.
https://developer.android.com/design-for-safety
83. Karen Hao. The coming war on the hidden algorithms that trap people in poverty. MIT Tech Review.
Dec. 4, 2020.
https://www.technologyreview.com/2020/12/04/1013068/algorithms-create-a-poverty-trap-lawyers-
fight-back/
84. Anjana Samant, Aaron Horowitz, Kath Xu, and Sophie Beiers. Family Surveillance by Algorithm.
ACLU. Accessed May 2, 2022.
https://www.aclu.org/fact-sheet/family-surveillance-algorithm
70E
NDNOTES
85. Mick Dumke and Frank Main. A look inside the watch list Chicago police fought to keep secret. The
Chicago Sun Times. May 18, 2017.
https://chicago.suntimes.com/2017/5/18/18386116/a-look-inside-the-watch-list-chicago-police-fought-
to-keep-secret
86. Jay Stanley. Pitfalls of Artificial Intelligence Decisionmaking Highlighted In Idaho ACLU Case.
ACLU. Jun. 2, 2017.
https://www.aclu.org/blog/privacy-technology/pitfalls-artificial-intelligence-decisionmaking-
highlighted-idaho-aclu-case
87. Illinois General Assembly. Biometric Information Privacy Act. Effective Oct. 3, 2008.
https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57
88. Partnership on AI. ABOUT ML Reference Document. Accessed May 2, 2022.
https://partnershiponai.org/paper/about-ml-reference-document/1/
89. See, e.g., the model cards framework: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker
Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.
Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and
Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220–229. https://
dl.acm.org/doi/10.1145/3287560.3287596
90. Sarah Ammermann. Adverse Action Notice Requirements Under the ECOA and the FCRA. Consumer
Compliance Outlook. Second Quarter 2013.
https://consumercomplianceoutlook.org/2013/second-quarter/adverse-action-notice-requirements-
under-ecoa-fcra/
91. Federal Trade Commission. Using Consumer Reports for Credit Decisions: What to Know About
Adverse Action and Risk-Based Pricing Notices. Accessed May 2, 2022.
https://www.ftc.gov/business-guidance/resources/using-consumer-reports-credit-decisions-what-
know-about-adverse-action-risk-based-pricing-notices#risk
92. Consumer Financial Protection Bureau. CFPB Acts to Protect the Public from Black-Box Credit
Models Using Complex Algorithms. May 26, 2022.
https://www.consumerfinance.gov/about-us/newsroom/cfpb-acts-to-protect-the-public-from-black-
box-credit-models-using-complex-algorithms/
93. Anthony Zaller. California Passes Law Regulating Quotas In Warehouses – What Employers Need to
Know About AB 701. Zaller Law Group California Employment Law Report. Sept. 24, 2021.
https://www.californiaemploymentlawreport.com/2021/09/california-passes-law-regulating-quotas-
in-warehouses-what-employers-need-to-know-about-ab-701/
94. National Institute of Standards and Technology. AI Fundamental Research – Explainability.
Accessed Jun. 4, 2022.
https://www.nist.gov/artificial-intelligence/ai-fundamental-research-explainability
95. DARPA. Explainable Artificial Intelligence (XAI). Accessed July 20, 2022.
https://www.darpa.mil/program/explainable-artificial-intelligence
71E
NDNOTES
96. National Science Foundation. NSF Program on Fairness in Artificial Intelligence in Collaboration
with Amazon (FAI). Accessed July 20, 2022.
https://www.nsf.gov/pubs/2021/nsf21585/nsf21585.htm
97. Kyle Wiggers. Automatic signature verification software threatens to disenfranchise U.S. voters.
VentureBeat. Oct. 25, 2020.
https://venturebeat.com/2020/10/25/automatic-signature-verification-software-threatens-to-
disenfranchise-u-s-voters/
98. Ballotpedia. Cure period for absentee and mail-in ballots. Article retrieved Apr 18, 2022.
https://ballotpedia.org/Cure_period_for_absentee_and_mail-in_ballots
99. Larry Buchanan and Alicia Parlapiano. Two of these Mail Ballot Signatures are by the Same Person.
Which Ones? New York Times. Oct. 7, 2020.
https://www.nytimes.com/interactive/2020/10/07/upshot/mail-voting-ballots-signature-
matching.html
100. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020.
https://bipartisanpolicy.org/blog/the-low-down-on-ballot-curing/
101. Andrew Kenney. 'I'm shocked that they need to have a smartphone': System for unemployment
benefits exposes digital divide. USA Today. May 2, 2021.
https://www.usatoday.com/story/tech/news/2021/05/02/unemployment-benefits-system-leaving-
people-behind/4915248001/
102. Allie Gross. UIA lawsuit shows how the state criminalizes the unemployed. Detroit Metro-Times.
Sep. 18, 2015.
https://www.metrotimes.com/news/uia-lawsuit-shows-how-the-state-criminalizes-the-
unemployed-2369412
103. Maia Szalavitz. The Pain Was Unbearable. So Why Did Doctors Turn Her Away? Wired. Aug. 11,
2021. https://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/
104. Spencer Soper. Fired by Bot at Amazon: "It's You Against the Machine". Bloomberg, Jun. 28, 2021.
https://www.bloomberg.com/news/features/2021-06-28/fired-by-bot-amazon-turns-to-machine-
managers-and-workers-are-losing-out
105. Definitions of ‘equity’ and ‘underserved communities’ can be found in the Definitions section of
this document as well as in Executive Order on Advancing Racial Equity and Support for Underserved
Communities Through the Federal Government:
https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive-order-
advancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/
106. HealthCare.gov. Navigator - HealthCare.gov Glossary. Accessed May 2, 2022.
https://www.healthcare.gov/glossary/navigator/
72E
NDNOTES
107. Centers for Medicare & Medicaid Services. Biden-Harris Administration Quadruples the Number
of Health Care Navigators Ahead of HealthCare.gov Open Enrollment Period. Aug. 27, 2021.
https://www.cms.gov/newsroom/press-releases/biden-harris-administration-quadruples-number-
health-care-navigators-ahead-healthcaregov-open
108. See, e.g., McKinsey & Company. The State of Customer Care in 2022. July 8, 2022. https://
www.mckinsey.com/business-functions/operations/our-insights/the-state-of-customer-care-in-2022;
Sara Angeles. Customer Service Solutions for Small Businesses. Business News Daily.
Jun. 29, 2022. https://www.businessnewsdaily.com/7575-customer-service-solutions.html
109. Mike Hughes. Are We Getting The Best Out Of Our Bots? Co-Intelligence Between Robots &
Humans. Forbes. Jul. 14, 2022.
https://www.forbes.com/sites/mikehughes1/2022/07/14/are-we-getting-the-best-out-of-our-bots-co-
intelligence-between-robots--humans/?sh=16a2bd207395
110. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020. https://
bipartisanpolicy.org/blog/the-low-down-on-ballot-curing/; Zahavah Levine and Thea Raymond-
Seidel. Mail Voting Litigation in 2020, Part IV: Verifying Mail Ballots. Oct. 29, 2020.
https://www.lawfareblog.com/mail-voting-litigation-2020-part-iv-verifying-mail-ballots
111. National Conference of State Legislatures. Table 15: States With Signature Cure Processes. Jan. 18,
2022.
https://www.ncsl.org/research/elections-and-campaigns/vopp-table-15-states-that-permit-voters-to-
correct-signature-discrepancies.aspx
112. White House Office of Science and Technology Policy. Join the Effort to Create A Bill of Rights for
an Automated Society. Nov. 10, 2021.
https://www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-
rights-for-an-automated-society/
113. White House Office of Science and Technology Policy. Notice of Request for Information (RFI) on
Public and Private Sector Uses of Biometric Technologies. Issued Oct. 8, 2021.
https://www.federalregister.gov/documents/2021/10/08/2021-21975/notice-of-request-for-
information-rfi-on-public-and-private-sector-uses-of-biometric-technologies
114. National Artificial Intelligence Initiative Office. Public Input on Public and Private Sector Uses of
Biometric Technologies. Accessed Apr. 19, 2022.
https://www.ai.gov/86-fr-56300-responses/
115. Thomas D. Olszewski, Lisa M. Van Pay, Javier F. Ortiz, Sarah E. Swiersz, and Laurie A. Dacus.
Synopsis of Responses to OSTP’s Request for Information on the Use and Governance of Biometric
Technologies in the Public and Private Sectors. Science and Technology Policy Institute. Mar. 2022.
https://www.ida.org/-/media/feature/publications/s/sy/synopsis-of-responses-to-request-for-
information-on-the-use-and-governance-of-biometric-technologies/ida-document-d-33070.ashx
73